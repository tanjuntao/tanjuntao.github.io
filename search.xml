<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[PyCharm配置优化]]></title>
    <url>%2F2021%2F01%2F11%2FPyCharm%E9%85%8D%E7%BD%AE%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[PyCharm 优化1. 将代码竖直线调整到 80 个字符的位置良好的 python 代码，每行代码最好不超过 80 个字符，但是 PyCharm 中默认情况下，代码超长的竖直线并不在 80 个字符的位置，我们需要手动修改。 分两步，首先需要确保这条竖直线的代码长度提示功能是正常打开的： 接着需要将每行代码长度修改成 80 字符： 2. 关闭掉恼人的 typo 提示PyCharm 中默认情况下，会对自己不认识的单词加上波浪线提示，该词可能是一个 typo，但是实际编码时，谁会给变量名起全名啊，所以这个功能也需要关闭。 参考下图关闭步骤：只需要将 typo 对应的 checkbox 取消勾选就可以了： 3. 关闭掉恼人的 PEP8 代码格式提示PyCharm 中默认情况下将 PEP8 代码风格提示开启，这就导致了几百行代码写下来，满屏都是下划线….非常影响代码阅读。这里 Python 代码风格建议参考 Google’s Python Style Guid 。 找到 Preference -&gt; Editor -&gt; Inspections -&gt; Python，然后将下面两个 PEP8 对应的 CheckBox 取消勾选： 至此：就可以在 Pycharm 中愉快的编码了。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>配置</tag>
        <tag>Python</tag>
        <tag>PyCharm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[远程使用 jupyter notebook]]></title>
    <url>%2F2020%2F12%2F19%2F%E8%BF%9C%E7%A8%8B%E4%BD%BF%E7%94%A8-jupyter-notebook%2F</url>
    <content type="text"><![CDATA[这篇文章记录下如何远程使用 jupyter notebook。 首先在服务器上，激活自己的 Python 虚拟环境： 1source /raid/workspace/tanjuntao/py35env/bin/activate 如果没有安装 jupyter 这个包的话，需要首先安装： 1pip install jupyter 安装成功后，接着启动 jupyter notebook 这个服务： 1jupyter notebook --no-browser --port=8080 这里面的端口可以任意指定，只要不和系统进程的端口冲突即可。 启动之后，会有一串输出提示信息： 12345678910(py35env) (base) user@1-scw4750:/raid/workspace/tanjuntao$ jupyter notebook --no-browser --port=8080[W 18:48:29.706 NotebookApp] Collisions detected in /home/user/.jupyter/jupyter_notebook_config.py and /home/user/.jupyter/jupyter_notebook_config.json config files. /home/user/.jupyter/jupyter_notebook_config.json has higher priority: &#123; "NotebookApp": &#123; "password": "'sha1:c2a505dd0ed4:b62adc94acff946e3efa595a4a6e1d7b35cd6716' ignored, using 'sha1:2b1858591aaf:43b0123a99a39ee6a38e329a336b48092c9776f8'" &#125; &#125;[I 18:48:29.895 NotebookApp] Serving notebooks from local directory: /raid/workspace/tanjuntao[I 18:48:29.895 NotebookApp] The Jupyter Notebook is running at:[I 18:48:29.895 NotebookApp] http://s1.linkeservers.ml:8080/[I 18:48:29.895 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). 从第 9 行中我们可以知道，我们的服务已经成功启动起来了，地址是：s1.linkeservers.ml，端口是 8080。 如果端口 8080 被其它进程占用的话，那么 jupyter notebook 会自动分配下一个端口，也就是 8081 。 接着，我们在自己本地的浏览器上，直接打开刚才的地址：http://s1.linkeservers.ml:8080/，这时候会弹出一个登录窗口，如果你记得密码，直接登录就好了；如果忘了密码，就需要重新设置。 重设密码也很简单，在服务器上，先 ctrl + c 关闭 jupyter notebook 服务，接着输入： 1jupyter notebook password 这时候会提示输入新密码，输入完之后再输一遍进行确认，jupyter notebook 的登录密码就被重置了。 接着我们重新启动 jupyter notebook 服务，在浏览器中输入刚才重置后的密码，就可以使用远程的 jupyter noteboook 了。 有两点需要注意的： 在浏览器中打开的 jupyter notebook 目录，和在服务器上启动 jupyter notebook 时的目录一致。也就是说，当前在服务器上所处的目录，会在浏览器中以 jupyter notebook 的方式打开； 可以同时在多个端口启动当前的 jupyter notebook : 我们可以在多个终端会话下启动 jupyter notebook，每次使用不同的 --port 参数。这样的话，如果我有多个工程项目，在不同的目录下，我就可以先切换到各个工程所在目录，然后在该目录下去启动 jupyter notebook，然后在浏览器上打开不同的端口，就可以同时编辑不同的工程项目了。 启动完服务之后，如何关闭该端口的服务呢？有两种方式： 直接在服务器终端下 ctrl + c 关闭服务； 在浏览器打开的 jupyter notebook 页面下，点击 quit 按钮即可。 References Jupyter Notebook修改登陆密码]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>python</tag>
        <tag>jupyter</tag>
        <tag>远程连接</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 学习记录：scp 命令的几种用法]]></title>
    <url>%2F2020%2F12%2F19%2FLinux-%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%9Ascp-%E5%91%BD%E4%BB%A4%E7%9A%84%E5%87%A0%E7%A7%8D%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[这篇文章总结下 scp 命令几种常见的使用方法，方便自己查找。 1. scp 命令介绍scp 其实是一个 SSH 客户端工具，全称是 secure copy，其主要用途是做本机和远程服务器之间的文件拷贝，底层使用的是 ssh 协议，因此在跨主机之间拷贝文件时，具有安全性保障。 2. 常见使用方法2.1 从服务器下载单个文件首先在本地切换到想要保存服务器文件的目录，然后执行下列命令: 1scp &lt;username&gt;@&lt;hostname&gt;:/datapool/workspace/tanjuntao/hello.txt . 注意：命令最后有一个 .，表示本机当前目录的意思。 2.2 从服务器下载整个文件夹首先在本地切换到想要保存服务器文件夹的目录，然后执行下列命令: 1scp -r &lt;username&gt;@&lt;hostname&gt;:/datapool/workspace/tanjuntao/TestDir . 唯一的区别是多了一个 -r 参数，表示递归下载，也就是下载一个文件夹。 2.3 上传单个文件到服务器切换到本机需要上传的文件所在的文件夹下，然后执行下列命令： 1scp Test.txt &lt;username&gt;@&lt;hostname&gt;:/datapool/workspace/tanjuntao/TestDir 2.4 上传整个文件夹到服务器同样道理，上传文件夹需要加入 -r 参数 : 1scp -r LocalDir &lt;username&gt;@&lt;hostname&gt;:/datapool/workspace/tanjuntao/TestDir 2.5 一次性上传多个文件有时候需要将本地好几个文件一次性上传到服务器上，而不想多次执行单个上传文件的命令，可以这么做： 1scp a.txt b.txt c.txt &lt;username&gt;@&lt;hostname&gt;:/datapool/workspace/tanjuntao/TestDir 即：将多个文件一个个列出来即可。 2.6 从服务器一次性下载多个文件反过来，有时候也想要从服务器上一次性下载多个文件到本地，这种情况语法稍微特殊一点： 1scp &lt;username&gt;@&lt;hostname&gt;:/datapool/workspace/tanjuntao/TestDir/\&#123;a.txt, b.txt, c.txt\&#125; . 服务器上的多个文件，需要放入到 {} 中，并且以逗号分隔开来，同时 {} 需要用反斜杠进行转义。 2.7 如果 ssh 服务不是使用默认的 22 的端口有时候为了安全性考虑，服务器上的 sshd 服务不是在默认的 22 端口启动的，而是换了一个不容易猜测的端口，比方说 2323，这时候就需要在 scp 命令中引入一个指定端口的参数：-P （大写）参数。 前面的六种情况，都必须加入 -P 参数才能正常执行。以从本地传输多个文件到服务器为例，也就是 2.5 中的例子，现在命令需要修改为： 1scp -P 2323 a.txt, b.txt &lt;username&gt;@&lt;hostname&gt;:/datapool/workspace/tanjuntao/TestDir -P 参数必须紧跟在 scp 的后面。 3. 注意事项scp 命令也是可以利用到本地的 SSH 配置文件 ~/.ssh/config 的，也就是可以用服务器别名来替代 &lt;username&gt;@&lt;hostname&gt;。 比方说，如果 &lt;username&gt;@&lt;hostname&gt; 这台服务器在 config 文件中有个 jump 的别名，那么，上面的 2.7 就可以变成下面这样： 1scp -P 2323 a.txt, b.txt jump:/datapool/workspace/tanjuntao/TestDir 一下子方便了很多。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>scp</tag>
        <tag>linux</tag>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习记录：打包及压缩命令使用方法总结]]></title>
    <url>%2F2020%2F10%2F27%2FLinux%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%9A%E6%89%93%E5%8C%85%E5%8F%8A%E5%8E%8B%E7%BC%A9%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[1. 概览在这篇文章中，我将总结 Linux 系统下面常见的几种压缩工具，包括它们的压缩命令、解压缩命令、特殊参数等，同时，我还将介绍打包命令 tar，以及如何将其与压缩命令同时使用。 Linux 下面的压缩命令主要有下面几个： zip，对应压缩文件格式：.zip gzip，对应压缩文件格式：.gz bzip2，对应压缩文件格式：.bz2 xz，对应压缩文件格式：.xz 打包命令： tar，对应压缩文件格式：.tar 如果和 gzip 一块使用：.tar.gz 或者 .tgz 如果和 bzip2 一块使用：.tar.bz2 如果和 xz 一块使用：.tar.xz 所谓打包，就是将一个文件、或者一堆文件打包成一个单一文件的过程。 这里还需要说明一个术语：压缩率。其计算公式是：（压缩后文件体积） / （压缩前文件体积）。 还有一点是：gzip bzip2 xz 这几个命令都只能对但一个文件做压缩，不能压缩一个目录，压缩目录需要通过 tar 配合这几个压缩命令来处理，或者使用 zip 命令 zip 命令能提供的功能和 tar 非常类似，更详细的使用方法使用 tldr zip &amp; tldr unzip 来查看。 2. 使用方法2.1 gzip 命令gzip 会就地压缩，也就是源文件会被压缩文件替代。 压缩 1gzip &lt;file_name&gt; 执行完上述命令，会在当前目录下生成 file_name.gz 文件。 压缩命令有个比较有用的参数： -v : 可以显示压缩后的压缩率 1gzip -v &lt;file_name&gt; 解压缩 1gzip -d &lt;file_name&gt;.gz 或者 1gunzip &lt;file_name&gt;.gz 会在当前目录下生成 &lt;file_name&gt; ，源压缩文件不在了。 2.2 bzip2 命令默认情况下，bzip2 命令也是就地压缩，压缩后源文件将会被替代。 使用方法和 gzip 基本一样，但是能提供更好的压缩率，当然，付出的代价就是压缩时间更长。 压缩 1bzip2 &lt;file_name&gt; 或者引入 -v 参数查看压缩率： 1bzip2 -v &lt;file_name&gt; 将在当前目录下生成 &lt;file_name&gt;.bz2 文件。 解压缩 1bzip2 -d &lt;file_name&gt;.bz2 或者 1bunzip2 &lt;file_name&gt;.bz2 2.3 xz 命令默认情况下，xz 命令也是就地压缩，原始文件将被覆盖。 使用方法和前面提到的 gzip bzip2 类似，参数也几乎是一样的。 压缩 1xz &lt;file_name&gt; 或者加上 -v 参数查看压缩率： 1xz -v &lt;file_name&gt; 压缩完后，会在当前目录下生成 &lt;file_name&gt;.xz 文件。 这时可以使用 -l 参数，来查看压缩前后的文件对比信息： 1xz -l &lt;file_name&gt;.xz 举个例子如下： 1234tanjuntao@mbp:~/Downloads$ xz -l ppt模板.pptx.xzStrms Blocks Compressed Uncompressed Ratio Check Filename 1 1 124.7 KiB 135.5 KiB 0.920 CRC64 ppt模板.pptx.xztanjuntao@mbp:~/Downloads$ 解压缩 1xz -d &lt;file_name&gt;.xz 注意，在 xz 命令这里，没有 xunz 这种命令哦。 2.4 zip 命令zip 命令和前面的 gzip bzip2 xz 不一样，它压缩后不会覆盖原有的文件。 压缩单个文件 1zip &lt;compresed&gt;.zip &lt;file_name&gt; 或者加入 -v 参数，可以查看压缩率： 1zip -v &lt;compresed&gt;.zip &lt;file_name&gt; 执行上述命令，将会在当前目录下新生成 &lt;compresed&gt;.zip 文件，原有的 &lt;file_name&gt; 将继续保留。 压缩多个文件 1zip &lt;compresed&gt;.zip a.txt b.txt c.txt 会将 a.txt, b.txt, c.txt 同时压缩到 compresed.zip 文件中。 压缩目录： 1zip -r dir.zip &lt;dir_name&gt; 解压缩 1unzip &lt;compresed&gt;.zip 如果这个 .zip 文件是从多个单一文件通过 zip 命令压缩来的，那么执行 unzip 将会倾倒里面所有的文件到当前文件夹； 如果这个 .zip 文件是从某个目录通过 zip -r 压缩来的，那么执行 unzip 将会在当前目录下生成一个 compresed 名字的目录，目录里面的所有文件不会被倾倒在当前目录。 2.5 tar 命令tar 命令的参数可以说是 Linux 上面最复杂且最不容易记忆的几个之一了，完整的 tar 命令的使用方法可以足够写另外一篇文章。在这里，我主要将介绍 tar 命令最核心的参数。 先解释下 tar 的作用：tar 是打包命令，也就是将一系列文件，或者某个文件夹，打包成单一文件。如果配合前面介绍的 3 种压缩命令 gzip bzip2 xz （不包含 zip ），那么其可以完成 打包+压缩 的功能。 tar 执行完后，原有文件或者目录将保留。 下面先介绍 tar 最常用的参数： -c : create 的意思，创建 tar 文件； -t : 查看 tar 文件； -x : 解打包或者解压缩； -z : 用 gzip 来压缩； -j : 用 bzip2 来压缩； -J : 用 xz 来压缩； -f : 后面接需要生成的文件名； -v : verbose 的意思，显示打包压缩或者解打包解压缩过程中的详细信息； -C : 后面接某个目录，将文件加压缩到该特定目录下（默认是解压缩在当前目录） 只打包文件不压缩 打包单个或多个文件： 1tar -cv -f target.tar a.txt b.txt c.txt d.txt 打包单个目录或者多个目录： 1tar -cv -f target.tar dir1 dir2 dir3 同时打包文件和目录： 1tar -cv -f target.tar dir1 dir2 a.txt b.txt c.txt 打包同时用 gzip 压缩 1tar -cvz -f target.tar.gz dir1 dir2 a.txt 打包同时使用 bzip2 压缩 1tar -cvj -f target.tar.bz dir1 dir2 a.txt 打包同时使用 xz 压缩 1tar -cvJ -f target.tar.xz dir1 dir2 a.txt 查看 1tar -tvz -f target.tar.gz 或者 1tar -tvJ -f target.tar.xz 解包和解压缩 1tar -xvz -f target.tar.gz 上述命令将会在当前目录下解压缩，也就是 target.tar.gz 里面的文件会解压缩到当前目录下。 如果要解压缩到指定目录怎么做呢？ 使用 -C 参数： 1tar -xvz -f target.tar.gz -C ~/test 上述命令，会将 target.tar.gz 里面的所有文件解压到 ~/test 目录下。 总结下：tar 命令可以将任意的文件和目录打包成一个单一文件，解压缩时，默认是将 tar ball 里面的所有文件倾倒在当前目录，如果需要倾倒到其它目录，使用 -C 参数。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>zip</tag>
        <tag>gzip</tag>
        <tag>tar</tag>
        <tag>bz2</tag>
        <tag>xz</tag>
        <tag>打包</tag>
        <tag>压缩</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习记录：rsync使用方法总结]]></title>
    <url>%2F2020%2F10%2F26%2FLinux%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%9Arsync%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[这篇文章总结下 rsync 命令的常见使用方法。 1. Introductionrsync，全称是 remote sync，字面意思是做远程同步的，但是 rsync 能做的不只是远程同步，还能做： 文件拷贝； 系统备份； 远程文件传输； 等。rsync 完全能够替代常见的 cp / mv/ scp (secure copy) 等文件拷贝、移动和远程传输等命令。 rsync 最大的的用途是可以做增量备份，即 rsync 在第一次执行备份时，是全量备份（将所有的文件都备份），后面再重新备份时，只会备份哪些修改过的文件。 rsync 命令支持的参数非常多，下面就每一种参数的具体使用情景，做详细的说明。 2. Usagersync 命令的语法如下： 1rsync [options] [source] [target] 其中： options 表示各种可选参数，是下面要详细介绍的； source 表示原文件或者原目录，可以是本地的，也可以是远程的； target 表示目标目录，可以是本地的，也可以是远程的。 下面详细介绍各种参数的使用方法。 2.1 -r 参数Linux 中的 -r 参数基本都是 recursively 的意思，也就是递归，一般是面向目录，例如 cp 命令中的 -r 参数，rm 命令中的 -r 参数。 注意，-r 参数虽然能够递归的对一个目录进行拷贝，或者传输，**但是，其只传递文件内容，不传输文件的一些属性信息，如修改时间，文件所有者，文件权限等。比方说，当你使用 rsync 传递文件从一个地方到另一个地方之后，文件的所有者不再是原来的所有者，而变成了你。（即文件所有者信息不回被传递） 解释了这么多，来看几个使用 rsync 复制文件、传输文件的例子。 拷贝文件到一个本地目标目录 1rsync source.txt ~/target/ 需要强调的是，如果目标目录不存在，rsync 会自动创建该目录。 拷贝目录到一个本地目标目录 假设 test1 目录下面有两个文件 a.txt 和 b.txt，现在想将 test1 目录拷贝到 test2 目录下，形成 test2/test1 这种结构，可以这么做： 1rsync -r test1 test2/ 拷贝完，我们看看 test2 的内容： 12tanjuntao@mbp:~$ ls test2test1 但是，如果我们在 test1 后面加上 /，结果就不一样了： 1rsync -r test1/ test2/ 结果： 1234tanjuntao@mbp:~$ rsync -r test1/ test2/tanjuntao@mbp:~$ cd test2tanjuntao@mbp:~/test2$ lsa.txt b.txt test1 也就是说： 源目录带有 /，将拷贝其子文件和子目录，该目录本身不会出现在目标目录； 源目录不带 /，将拷贝源目录本身。 目标目录带 / 或者不带 / ，均不影响。 2.2 -a 参数前面说了，-r 参数不会将 modified time ，owner，permition 等属性拷贝到目标文件，那如果想要保持这些信息呢？那就用 -a 参数！ 实际上，如果是用 rsync 做文件备份，基本只会用 -a 参数，而不会使用 -r 参数。 -a 参数可以在拷贝单个文件时用，也可以在拷贝目录时用。拷贝目录时，使用 -a 就不需要再用 -r 了。 拷贝单个文件 1rsync -a source.txt ~/target/ 拷贝一个目录 1rsync -a test1 test2/ 上面的拷贝会形成 test2/test1 的目录结构。如果不想拷贝该目录本身，可以在 test1 后面加上 / : 1rsync -a test1/ test2/ 2.3 -v 参数在 Linux 中，-v 在很多情况下，表示 verbose，也就是在命令执行过程中会输出很多信息。在 rsync 命令中，-v 会将拷贝的整个过程打印在屏幕上。 12345678tanjuntao@mbp:~$ rsync -av test1/ test2building file list ... done./a.txtb.txtsent 196 bytes received 70 bytes 532.00 bytes/sectotal size is 0 speedup is 0.00 2.4 -z 参数在 Linux 中，-z 参数一般表示压缩的意思（想想 tar 命令中的 z 参数表示用 gzip 压缩和解压缩）。 使用这个参数，rsync 会首先将需要拷贝或传输的数据压缩，到了 destination，进行解压缩，整个过程中用户不需要手动做压缩和解压，rsync 全帮用户做好了，这样就能减少需要传输的文件大小。 1tanjuntao@mbp:~$ rsync -az test1/ test2/ 2.5 --progress 或者 -P (大写) 参数使用这个参数，可以将文件拷贝或者传输的过程给显示出来，看下面例子的执行结果就知道了： 123456789101112131415161718192021222324tanjuntao@mbp:~$ rsync -a --progress pytorch-cifar test2/building file list ...64 files to considerpytorch-cifar/pytorch-cifar/LICENSE 1065 100% 0.00kB/s 0:00:00 (xfer#1, to-check=62/64)pytorch-cifar/README.md 1324 100% 1.26MB/s 0:00:00 (xfer#2, to-check=61/64)pytorch-cifar/main.py 4518 100% 4.31MB/s 0:00:00 (xfer#3, to-check=60/64)pytorch-cifar/utils.py 3446 100% 3.29MB/s 0:00:00 (xfer#4, to-check=59/64)pytorch-cifar/.git/pytorch-cifar/.git/HEAD 23 100% 11.23kB/s 0:00:00 (xfer#5, to-check=57/64)pytorch-cifar/.git/config 311 100% 151.86kB/s 0:00:00 (xfer#6, to-check=56/64)pytorch-cifar/.git/description 73 100% 35.64kB/s 0:00:00 (xfer#7, to-check=55/64)pytorch-cifar/.git/index 1818 100% 887.70kB/s 0:00:00 (xfer#8, to-check=54/64)pytorch-cifar/.git/packed-refs 321 100% 104.49kB/s 0:00:00 (xfer#9, to-check=53/64)# &lt;后面省略...&gt; 2.6 -e 参数-e 参数后面接 ssh 表示我现在想要在本地和远程服务器之间，通过 ssh 协议来相互传输文件。 将本地文件传递到远程 1rsync -a -e ssh source.txt &lt;username&gt;@&lt;hostname&gt; 将本地目录传递到远程 1rsync -a -e ssh test1/ &lt;username&gt;@&lt;hostname&gt; 同样的，将远程文件或者目录拉取到本地，只需要将 [source] 和 [target] 参数调换一下位置即可。 有时候 ssh 为了安全起见，不是使用默认的 22 端口，改成了， 例如 1213 端口（为了安全起见防止攻击），这时候还想使用 rsync 在本地和远程之间传输文件，也不难。 将本地文件传输到服务器 1rsync -a -e 'ssh -p 1213' source.txt &lt;username&gt;@&lt;hostname&gt; 将服务器目录拉取到本地 1rsync -a -e 'ssh -p 1213' test2 . # 将 test2 目录拉取到当前目录，形成 ./test2/ 的目录结构 2.7 其它rsync 其实还有很多其他的参数： --include : 包含指定文件； --exclude：不包含指定类型文件或目录； --dry-run：模拟 rsync 的执行，看看执行后会发生什么，但实际上 rsync 并没有执行； --delete：在做备份时使用。如果我们的目标目录只想做源目录的一个镜像，即目标目录中不包含源目录中没有的文件，这时候，在使用 rsync 时带上 --delete 参数，就可以将目标目录中，那些不在源目录中的文件删除掉。 3. 增量备份rsync 最强大的功能就是做增量备份了，即第一次执行备份时，是做全量备份，后面再执行 rsync 做备份时，就是做增量备份了。 我们可以使用 crontab 来帮助我们做定时备份。 首先 crontab -e 来新建一个例行性工作： 会自动调用 vim 编辑器打开 crontab 文件，接着在里面按照 crontab 的时间格式，写入一行，例如下面的： 100 00 * * * rsync -avz /home/tanjuntao/Documents /media/tanjuntao/WestData/backups/ 上面的命令，就会每天在 00:00 时刻，将我的 Documents 文件夹备份到外接的硬盘 WestData 的 backups 文件夹中。 4. Referencesrsync 的具体使用细节，可以参考前三篇文章；rsync 用来备份系统，可以参考阮一峰文章中的增量备份一节，和引用列表最后那一篇文章。 【阮一峰】rsync 用法教程 Rsync (Remote Sync): 10 Practical Examples of Rsync Command in Linux How to Sync Files/Directories Using Rsync with Non-standard SSH Port How to Back Up Your Linux System]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>ssh</tag>
        <tag>rsync</tag>
        <tag>备份</tag>
        <tag>同步</tag>
        <tag>增量备份</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pip相关用法]]></title>
    <url>%2F2020%2F09%2F30%2Fpip%E5%B8%B8%E8%A7%81%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[1. pip 常见命令12345678910111213141516171819202122232425262728293031323334# 查看 pip3 版本pip3 --version# 查看帮助信息pip3 --help# 查找软件包pip3 search &lt;package_name&gt;# 查看已经安装的软件包pip3 list# 显示软件包当前版本以及最新版本pip3 list --outdated --format columns# 显示某个软件包的信息pip3 show &lt;package_name&gt;# 导出已经安装的软件包列表pip3 freeze &gt; requirements.txt # txt 文件的命名随意，一般命名为 requierements.txt# 接着想要在另外一台机器上或者另外一个环境下面安装pip3 install -r requirements.txt# 安装软件pip3 install &lt;package_name&gt;# 安装特定版本的软件包pip3 install requests==2.6.0# 升级某个特定软件包pip3 install upgrade &lt;package_name&gt;# 临时更换安装源pip3 install -i &lt;URL&gt; &lt;package_name&gt; # e.g., pip install -i http://mirrors.aliyun.com/pypi/simple/ numpy# 卸载软件包pip3 uninstall &lt;package_name&gt; 完整的文档地址：https://pip.pypa.io/en/stable/ 2. pip 如何指定某个安装源正常情况下，比如需要安装 numpy，用的命令是: 1pip install numpy 如果需要指定某个安装源的话，比方说阿里云的源，可以使用 -i 参数: 1pip install -i http://mirrors.aliyun.com/pypi/simple/ numpy 国内的源还有，具体参考 pip 国内源 豆瓣源：http://pypi.douban.com/simple/ 清华源：https://pypi.tuna.tsinghua.edu.cn/simple/ 科大源：http://pypi.mirrors.ustc.edu.cn/simple/ 阿里源：http://mirrors.aliyun.com/pypi/simple/ 上面的 -i 参数方法只能临时修改安装源，下次如果不跟这个参数，还是会从海外的站点下载安装包。 那有没有一劳永逸的方法呢？当然是有的 Linux 下：编辑 ~/.pip/pip.conf 文件（如果没有该文件，手动创建一个），比方说需要修改成科大源，可以这样： 1234[global]index-url = http://pypi.mirrors.ustc.edu.cn/simple/[install]trusted-host = pypi.mirrors.ustc.edu.cn 上面的 trusted-host 的目的是为了防止在安装包时，提示安装源不受信任，避免不必要的麻烦。 Windows 下：在 C:/Users/tanjuntao/ 目录下，新建一个 pip 目录。接着切换到 pip 目录下，新建一个 pip.ini 文件，填入下面内容即可： 1234[global]index-url = http://pypi.mirrors.ustc.edu.cn/simple/[install]trusted-host = pypi.mirrors.ustc.edu.cn 如果需要换成清华源，只需要将上面的地址改成清华源的地址即可，其它不变。 3. 如何使用某个 python 版本对应的 pip 来安装第三方包这里以安装虚拟环境第三方包 virtualenv 为例，同时介绍如何用指定版本的 python 来安装第三方包，同时如何建立 python 虚拟环境。 首先，Python 中的虚拟环境主要有 venv 和 virtualenv 这两个工具，venv 在 python3 中是自带的一个包，可以直接用来创建虚拟环境；virtualenv 是第三方包，需要单独安装。 首先我们需要安装这个 virtualenv 到某个指定的 python 版本中。 这件事情并不是很好弄。 在服务器上，我们可以使用类似 which python3.5 来查看系统的 python3.5 安装在什么目录，使用这种方法，我们可以在服务器上查看每一种版本的 python 的位置，下面是我们跳板机上的 python3 版本列表： 12345678910user@DataServer:~$ which python3/usr/bin/python3user@DataServer:~$ which python3.5/usr/bin/python3.5user@DataServer:~$ which python3.6/usr/bin/python3.6user@DataServer:~$ which python3.7/usr/bin/python3.7user@DataServer:~$ which python3.8user@DataServer:~$ 可以发现，服务器上的 python3 都是安装在 /usr/bin 这个目录下。python3.5 到 python3.7 都有安装，python3.8 则没有。 现在，我想要将 virtualenv 这个包安装给 python3.7 这个版本的 python，该怎么做呢？ 直觉上，我们可能觉得服务器上既然存在 python3.5 / python3.6 / python3.7 这种，是不是也有 pip3.5 / pip3.6 / pip3.7 呢？遗憾的是，服务器上并没有，这样就不能直接通过制定 pip 的版本来将 virtualenv 安装到该 pip 对应的 python 中了。 经过一番查找，找到了下面的办法： 1python3.7 -m pip install virtualenv 安装好 virtualenv 之后，我们可以首先看看这个命令的位置： 1which virtualenv 接着，我们就可以使用这个安装好的 virtualenv 来新建虚拟环境了。首先切换到自己想要安装的目录，然后执行： 1virtualenv -p /usr/bin/python3.7 py37env 这条命令将会在当前目录下，新建一个 py37env 的目录，这个目录就是我们的虚拟环境。 -p 参数的目的是后面跟上 python 的完整路径，py37env 是环境名，可以任意指定。 references: installing and using virtualenv install a module using pip for a specific python versionhttps://stackoverflow.com/questions/10919569/install-a-module-using-pip-for-specific-python-version/25123329)]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>python</tag>
        <tag>pip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pip相关用法]]></title>
    <url>%2F2020%2F09%2F30%2Fpip%E7%9B%B8%E5%85%B3%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1. pip 常见命令12345678910111213141516171819202122232425262728293031323334# 查看 pip3 版本pip3 --version# 查看帮助信息pip3 --help# 查找软件包pip3 search &lt;package_name&gt;# 查看已经安装的软件包pip3 list# 显示软件包当前版本以及最新版本pip3 list --outdated --format columns# 显示某个软件包的信息pip3 show &lt;package_name&gt;# 导出已经安装的软件包列表pip3 freeze &gt; requirements.txt # txt 文件的命名随意，一般命名为 requierements.txt# 接着想要在另外一台机器上或者另外一个环境下面安装pip3 install -r requirements.txt# 安装软件pip3 install &lt;package_name&gt;# 安装特定版本的软件包pip3 install requests==2.6.0# 升级某个特定软件包pip3 install upgrade &lt;package_name&gt;# 临时更换安装源pip3 install -i &lt;URL&gt; &lt;package_name&gt; # e.g., pip install -i http://mirrors.aliyun.com/pypi/simple/ numpy# 卸载软件包pip3 uninstall &lt;package_name&gt; 完整的文档地址：https://pip.pypa.io/en/stable/ 2. pip 如何指定某个安装源正常情况下，比如需要安装 numpy，用的命令是: 1pip install numpy 如果需要指定某个安装源的话，比方说阿里云的源，可以使用 -i 参数: 1pip install -i http://mirrors.aliyun.com/pypi/simple/ numpy 国内的源还有，具体参考 pip 国内源 豆瓣源：http://pypi.douban.com/simple/ 清华源：https://pypi.tuna.tsinghua.edu.cn/simple/ 科大源：http://pypi.mirrors.ustc.edu.cn/simple/ 阿里源：http://mirrors.aliyun.com/pypi/simple/ 上面的 -i 参数方法只能临时修改安装源，下次如果不跟这个参数，还是会从海外的站点下载安装包。 那有没有一劳永逸的方法呢？当然是有的 Linux 下：编辑 ~/.pip/pip.conf 文件（如果没有该文件，手动创建一个），比方说需要修改成科大源，可以这样： 1234[global]index-url = http://pypi.mirrors.ustc.edu.cn/simple/[install]trusted-host = pypi.mirrors.ustc.edu.cn 上面的 trusted-host 的目的是为了防止在安装包时，提示安装源不受信任，避免不必要的麻烦。 Windows 下：在 C:/Users/tanjuntao/ 目录下，新建一个 pip 目录。接着切换到 pip 目录下，新建一个 pip.ini 文件，填入下面内容即可： 1234[global]index-url = http://pypi.mirrors.ustc.edu.cn/simple/[install]trusted-host = pypi.mirrors.ustc.edu.cn 如果需要换成清华源，只需要将上面的地址改成清华源的地址即可，其它不变。 3. 如何使用某个 python 版本对应的 pip 来安装第三方包这里以安装虚拟环境第三方包 virtualenv 为例，同时介绍如何用指定版本的 python 来安装第三方包，同时如何建立 python 虚拟环境。 首先，Python 中的虚拟环境主要有 venv 和 virtualenv 这两个工具，venv 在 python3 中是自带的一个包，可以直接用来创建虚拟环境；virtualenv 是第三方包，需要单独安装。 首先我们需要安装这个 virtualenv 到某个指定的 python 版本中。 这件事情并不是很好弄。 在服务器上，我们可以使用类似 which python3.5 来查看系统的 python3.5 安装在什么目录，使用这种方法，我们可以在服务器上查看每一种版本的 python 的位置，下面是我们跳板机上的 python3 版本列表： 12345678910user@DataServer:~$ which python3/usr/bin/python3user@DataServer:~$ which python3.5/usr/bin/python3.5user@DataServer:~$ which python3.6/usr/bin/python3.6user@DataServer:~$ which python3.7/usr/bin/python3.7user@DataServer:~$ which python3.8user@DataServer:~$ 可以发现，服务器上的 python3 都是安装在 /usr/bin 这个目录下。python3.5 到 python3.7 都有安装，python3.8 则没有。 现在，我想要将 virtualenv 这个包安装给 python3.7 这个版本的 python，该怎么做呢？ 直觉上，我们可能觉得服务器上既然存在 python3.5 / python3.6 / python3.7 这种，是不是也有 pip3.5 / pip3.6 / pip3.7 呢？遗憾的是，服务器上并没有，这样就不能直接通过制定 pip 的版本来将 virtualenv 安装到该 pip 对应的 python 中了。 经过一番查找，找到了下面的办法： 1python3.7 -m pip install virtualenv 安装好 virtualenv 之后，我们可以首先看看这个命令的位置： 1which virtualenv 接着，我们就可以使用这个安装好的 virtualenv 来新建虚拟环境了。首先切换到自己想要安装的目录，然后执行： 1virtualenv -p /usr/bin/python3.7 py37env 这条命令将会在当前目录下，新建一个 py37env 的目录，这个目录就是我们的虚拟环境。 -p 参数的目的是后面跟上 python 的完整路径，py37env 是环境名，可以任意指定。 references: installing and using virtualenv install a module using pip for a specific python versionhttps://stackoverflow.com/questions/10919569/install-a-module-using-pip-for-specific-python-version/25123329)]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>python</tag>
        <tag>pip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSH远程执行命令]]></title>
    <url>%2F2020%2F09%2F30%2FSSH%E8%BF%9C%E7%A8%8B%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[这篇文章中记录下如何使用 SSH 来远程执行命令。 1. 背景传统的 SSH（Secure SHell）的使用方法一般是：首先利用 SSH 登录到远程主机，然后在远程主机上执行相关命令。 但是有时候我们可能有这种需求：我只是想看下远程主机上的某个文件是否存在，或者查看远程主机的某种工作状态，我们只是想看下这种信息，并不想先登录然后执行命令完再退出这种繁琐的操作。SSH 是支持在本地直接将命令提交给远程主机来执行的，执行完命令后返回用户本地的控制台，而不需要手动退出远程的登录。 2. 使用方法简单使用可以直接在 SSH 登录的命令后加上想要执行的命令，即可让该命令在远程主机上执行，执行完之后立刻返回本地的命令行： 1ssh &lt;user_name&gt;@&lt;host_name&gt; '&lt;your command&gt;' 例如，我想看远程主机上面 HOME 目录有什么文件，那么可以： 12# 注意，如果命令需要带参数，那么整个命令串一定要用单引号括起来ssh user@202.38.79.55 'ls ~' -t 参数有时候我们执行的命令需要和用户进行一定的交互，例如使用 sudo 的命令一定需要用户输入密码，这种情况如果仍然像上面那样来提交命令会出错，我们需要加入 -t 参数： 1ssh -t user@202.38.79.55 'sudo apt update' 1ssh -t user@202.38.79.55 'htop' 只有带上了 -t 参数，这种交互式的命令才会执行成功。 3. References SSH tip: Send commands remotely Run / Execute Command Using SSH]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 常用命令记录]]></title>
    <url>%2F2020%2F08%2F26%2FGit-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[这篇文章系统性的总结下 git 常见命令的使用方法。 1. 背景需求今年以来帮助实验室的博士生做了两篇论文的实验。做 paper 的实验有一个特点：需求会经常变动，经常是这种方法效果不行，那就需要换另外一种方法进行试验。导致的结果就是代码会经常变动，并且这次改动之后以后还有可能需要改动回来，因为一直没有用上 git，所以就显得代码非常混乱。最后非常尴尬的局面就是：今天需要尝试下这个方法能不能 work，需要将整个项目的代码过一遍，保证当前的代码、当前需要执行的程序、当前的参数是和这个需求符合的；明天可能又换了一种方法；后来又需要重新切换会第一天的方法，又得整个把代码给过一遍，将一些注释给重新注释回来，反正就是非常非常麻烦，人工一遍一遍进行代码控制非常累。 但是在实验过程中间又不想去使用新的技术，即使使用 git 是一件非常直觉的事情，知道 deadline 也没有用 git 来管理项目代码…… paper 投出去之后，就觉得将 git 使用熟练真的太太太重要了。 2. 常见命令使用方法创建一个 git 仓库手下，我们可以在 github 上面找到任意一个 repository，在本地 git clone 下来（注意，不是下载 github 上面的 .zip 压缩包），那么这个项目对应的文件夹本身就是一个 git repository。 如果我们从零开始创建 git 仓库，也很简单，先新建一个文件夹，进入文件夹，然后： 1git init 就会在当前文件夹下面生成 .git 文件夹，默认是不可见的。这个 .git 就是用来存储我们对目录文件的所有操作历史的。 git add &amp; git commit这里首先需要解释下 git 的工作原理，如下图所示： 当我们使用 git add xxx.txt 命令时，当前对 xxx.txt 的修改将会被提交到 staging area；接着当我们继续使用 git commit -m &quot;message&quot; 时，这个修改才会被提交到 repository。 add 单个文件： 1git add xxx.txt # xxx.txt 是文件名 add 多个文件 1git add * # or git add . git commit 时必须要带入提交信息（commit message）： 1git commit -m "your message about this commit" git status &amp; git diff要查看当前 repository 的状态： 1git status 如果要查看某个文件前后两次修改的差异： 1git diff xxx.txt git log &amp; git reflog查看我们 commit 的历史，可以： 1git log 因为 git log 输出的信息很多，如果只想要简洁的输出，可以： 1git log --pretty=oneline 假设我们现在有这样一串提交历史： 1a1 -&gt; a2 -&gt; a3 -&gt; a4 -&gt; a5 我们从 a5 回退到 a3，这时候使用 git log 将只能查看到 a1 a2 a3 这 3 次的提交历史，a4 a5 的提交历史是查看不到的。这时候，如果我们改变主意，又想从 a3 回到 a5 该怎么办呢，就可以使用： 1git reflog 可以查看所有的提交历史记录。我们只需要知道我们在 a5 时的 commit message 然后找到对应的 commit id，就可以回退到 a5。 下面具体讲怎么进行版本回退或者版本切换。 git reset和之前一样，假如我们有下面这一串 commit 的历史，我们希望从 a5 回退到 a3。 1a1 -&gt; a2 -&gt; a3 -&gt; a4 -&gt; a5 首先在 a5 我们输入 git log 查看 a3 对应的 commit id，是一段很长的字母数字序列，假设前 4 位是 ab42，并且在所有的 commit id 里面是唯一的（方便我们做简写）。 接着执行： 1git reset --hard ab42 # commit id 不用写全，写明能够标识该 commit id 的前几位即可 我们还有更方便的方法，可以不用知道 commit id : 1git reset --hard HEAD^^ 其中 HEAD^^ 标识我们切换会上上一个版本。如果指向切换为上一个版本，也就是 a4，那可以： 1git reset --hard HEAD^]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>版本控制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 学习记录：history 查看历史命令列表]]></title>
    <url>%2F2020%2F07%2F14%2FLinux-%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%9Ahistory-%E6%9F%A5%E7%9C%8B%E5%8E%86%E5%8F%B2%E5%91%BD%E4%BB%A4%E5%88%97%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[history 命令能够查询到我们之前在 bash 里面输入的指令历史，这些指令一般存放在 HOME 目录下的 .bash_history 文件里，这个文件里面存放多少条指令记录，与环境变量 $HISTSIZE 有关，一般在 Ubuntu 等发行版中，这个值是 1000 。 history 命令的工作方式是这样的：当我们登录到 Linux 主机后，bash 首先会读取 .bash_history 文件，将里面的存放的所有命令历史（一般为 1000 条）读入到内存当中，这样，我们就能输入 history 查看这次登录以前所输入的所有命令。 我们在本次登录下所输入的所有命令，在我们退出之前是不会被写入到 .bash_history 文件的，也就是说，只有我们推出本次登录，这次登录里所输入的所有指令才会被写入到该文件。在我们没退出登录之前，.bash_history 文件的内容是不会变动的。 但是，我们在登录过程中所输入的所有指令，使用 history 命令是能够查看到的，因为这些命令都存在内存当中，而不是从文件中读取到的。 下面举个例子具体说明下。 首先，我们登录到 Linux，查看 .bash_history 最后 5 条记录（对应最近的 5 条指令）： 1cat ~/.bash_history | tail -n 5 输出： 123456tan@tjt1024:/mnt/c/Users/tanjuntao$ cat ~/.bash_history | tail -n 5vim man_db.confecho $HISTSIZWecho $HISTSIZeecho $HISTSIZEexit 接着，我们使用 history 命令来查看最近 5 条记录： 123456tan@tjt1024:/mnt/c/Users/tanjuntao$ history 5 1058 echo $HISTSIZe 1059 echo $HISTSIZE 1060 exit 1061 cat ~/.bash_history | tail -n 5 1062 history 5 可以看到，在 .bash_history 文件的基础上，此次登录后，我们输入了两条新命令： cat ~/.bash_history | tail -n 5 history 5 这两条命令马上就能被 history 捕捉到，因为 history 是从内存中读取指令历史。 我们再看看此时 .bash_history 的内容，如下： 123456tan@tjt1024:/mnt/c/Users/tanjuntao$ cat ~/.bash_history | tail -n 5vim man_db.confecho $HISTSIZWecho $HISTSIZeecho $HISTSIZEexit 还是和之前一样的，说明本次登录后，输入的所有命令都没有被记录到该文件中。 接着我们输入 exit 退出登录，再重新登录，再来查看 .bash_history 的内容，发现变成这样： 123456tan@tjt1024:/mnt/c/Users/tanjuntao$ cat ~/.bash_history | tail -n 5exitcat ~/.bash_history | tail -n 5history 5cat ~/.bash_history | tail -n 5exit 说明上次登录过程中输入的所有指令，已经被写入到 .bash_history 文件中了。 如果我们想让登录过程中输入的所有指令都写入到 .bash_history 中，而不是必须要退出当前登录，我们可以使用 history 命令的 -w 参数： 1history -w 接着上面的例子，我们重新登录后查看了 .bash_history 的最后 5 条记录。接下来，我们输入 history -w，再来查看该文件内容： 123456tan@tjt1024:/mnt/c/Users/tanjuntao$ cat ~/.bash_history | tail -n 5history 5cat ~/.bash_history | tail -n 5exitcat ~/.bash_history | tail -n 5history -w 说明 history- w 将该命令自己，已经该命令前面的所有命令都写入到 .bash_history 中了，但是我们最近一条查看 .bash_history 的 cat ~/.bash_history | tail -n 5 并没有被写入。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>history</tag>
        <tag>.bash_history</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 学习记录：grep 命令使用方法]]></title>
    <url>%2F2020%2F05%2F29%2FLinux-%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%9Agrep-%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[这篇文章总结下 grep 命令的常用方法。 1. 需求背景平时在使用 Linux 过程中，总会遇到需要搜索某个字符串的场景： 搜索某个字符串是否出现在指定文件中； 利用管道重定向，搜索 ls 、cat 等命令的输出中是否包含某个字符串； 递归搜索某个工程目录下的所有文件中，是否包含某个字符串。 grep 无疑是很好用且功能强大的工具之一。 2. 使用方法grep 的搜索是按行进行的，即：每次扫描一行，如果该行中包含指定的字符串，那么该行会被输出。 2.1 基本用法grep 最基本的用法如下： 1grep &lt;搜索字符串&gt; &lt;搜索的目标文件&gt; 举个例子：我们要搜索 hello_world.c 文件中是否包含 hello 这个字符串，可以这么做： 1grep 'hello' hello_world.txt 我们也可以利用输出重定向和管道完成这件事情： 1cat hello_world.c | grep 'hello' *注 1：grep 后面跟的 &lt;搜索字符串&gt; 可以加单引号 &#39;&#39;，也可以加双引号 &quot;&quot;，也可以什么都不加。当然还是建议使用引号将其包住，这样能方便命令的阅读。 *注 2：现在的 shell 基本都包含了 alias grep=&#39;grep --color=auto&#39; 可以通过 alias -p 查看。所以在使用 grep 搜索时，不需要带上 --color=auto 参数，搜索匹配到的关键字还是会用不同颜色显示出来。 *注 3：grep 命令对搜索字符串的大小写敏感。 2.2 -i 参数-i 参数里面的 i 表示英文单词 ignore，意思是搜索时忽略大小写，因为 grep 本身对大小写是敏感的。 1grep -i 'hello' hello_world.txt 那么，hello_world.txt 文件中所有的 hello 、HELLO、HeLLo 等均会被检索出来。 2.3 -v 参数-v 参数表示 反向搜索，意思是：将那些不包含 &lt;搜索字符串&gt; 的行输出： 1grep -v 'hello' hello_world.txt 那么，hello_world.txt 文件中不包含 hello 字符串的行将会被输出。 2.4 -n 参数-n 参数会将满足 &lt;搜索字符串&gt; 的行对应的行数也输出来： 1grep -n 'hello' hello_world.txt 输出： 123ubuntu@VM-0-14-ubuntu:~$ grep -n 'hello' hello_world.txt3:hello world from China12:hello, world. 从输出结果中我们可以看到，包含 hello 的行有第 3 行和 第 12 行。 2.5 -A 和 -B 参数-A 里面的 A 表示 After，-B 里面的 B 表示 Before。两个参数后面需要跟一个数字，表示我现在不仅要输出满足 &lt;搜索字符串&gt; 的那行，我还要将其前面和后面多少行也输出。 举个例子容易理解一些： 1grep -n -A 2 -B 3 'hello' hello_world.txt 上面的命令是说：我现在不仅输出包含 hello 的行，我还要将其前面（-B 参数）的 3 行、其后面的（-A 参数）的 3 行输出来。 2.6 -r 参数Linux 里面很多命令后面的 -r 参数大多表示递归的意思，grep 也是。 grep 后面加上 -r 参数，表示要搜索一个目录下面的所有文件（包括子目录里面的文件）是否包含 &lt;搜索字符串&gt; 。 看下面的例子： 1grep -r -i 'cnnmnist' . 输出： 123456789101112131415user@DataServer:/datapool/workspace/tanjuntao/FL-Influence$ grep -r -i 'cnnmnist' ../models/Nets.py:class CNNMnist(nn.Module):./models/Nets.py: super(CNNMnist, self).__init__()Binary file ./models/__pycache__/Nets.cpython-35.pyc matchesBinary file ./models/__pycache__/noisylabel.cpython-35.pyc matches./fedlearning.py:from models.Nets import CNNMnist, CNNCifar./fedlearning.py: net=CNNMnist().to(args.device)./fedlearning.py: net = CNNMnist().to(args.device)./print_model_arch.py:from models.Nets import CNNMnist, CNNCifar, ResNet18./fedinfluence_rka.py:from models.Nets import CNNMnist, CNNCifar./fedinfluence_rka.py: model = CNNMnist().to(args.device)./fedinfluence_rka.py: model = CNNMnist().to(args.device)./fedinfluence.py:from models.Nets import CNNMnist, CNNCifar./fedinfluence.py: model=CNNMnist().to(args.device)./fedinfluence.py: model=CNNMnist().to(args.device) 可以看到，当前目录（包含子目录里面的文件）中包含 cnnmnist 字符串的文件均被检索出来了。 2.7 &lt;搜索字符串&gt; 为正则表达式grep 最强大的功能还是配合正则表达式一起使用，这篇文章不准备详细介绍，后面会单独写一篇介绍 bash shell 里面正则表达式的使用方法，以及如何配合 grep 、awk 等文本处理程序一起使用。 3. References 12 Practical Examples of Linux grep Command 《鸟哥的 Linux 私房菜》10.6.1 节 &amp; 11.2.2 节 grep command in Unix/Linux 使用grep指令在当前目录下所有文件中搜索指定文本]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>搜索</tag>
        <tag>Linux</tag>
        <tag>grep</tag>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 学习记录：统计文件和目录的个数]]></title>
    <url>%2F2020%2F05%2F26%2FLinux-%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%9A%E7%BB%9F%E8%AE%A1%E6%96%87%E4%BB%B6%E5%92%8C%E7%9B%AE%E5%BD%95%E7%9A%84%E4%B8%AA%E6%95%B0%2F</url>
    <content type="text"><![CDATA[这篇文章总结下 Linux 下如何统计某个目录下面总共有多少个文件或文件夹。 1. 背景平时在 Linux 下工作，我们可能经常会碰到这种场景，即需要统计某个目录下总共有多少个文件，或者多少个子文件夹。这时候，熟练的使用一些命令将会有助于你开展工作。 注：下问的命令需要有足够权限执行，否则碰到有些文件不能读取（权限不足）的情况，将会造成统计出错。 2. 使用 ls 和 wc 命令ls 命令有一个参数是 -l ，表示列举出文件的详细信息： 12345ubuntu@VM-0-14-ubuntu:~$ ls -ltotal 1825244-rw-rw-r-- 1 ubuntu ubuntu 237412642 Mar 31 11:47 3-24.mp4-rw-rw-r-- 1 ubuntu ubuntu 242636679 Mar 24 11:41 3-31.mp4-rw-rw-r-- 1 ubuntu ubuntu 64569543 Apr 14 10:16 4-14-1.mp4 注意输出行的第一个字段，以 -rw-rw-r-- 为例。从左往右数第一位，表示 文件类型 ，后面每 3 位是一组，分别表示 属主、组用户、其它用户 的权限，可以参考之前的博文：Linux学习记录：SUID 和 SGID 等特殊权限 下面解释下第一位（表示文件类型）的含义： - : 表示普通文件； d : 表示目录； l : 表示链接文件 …… 解释完 ls -l 命令后，我们还需要知道 wc 命令的使用方法。 wc 命令用来统计信息，如统计一个文件有多少行、有多少个字符等。常用的一个参数是 -l，用来统计有多少行。 知道了这些以后，我们就能看懂下面的命令了。 ① 统计一个文件夹下有多少个文件，不包括子文件夹里的。 1ls -l | grep "^-" | wc -l grep &quot;^-&quot; 表示抓取行首以字符 - 开头的行，其中 ^ 表示从行首开始的第一个字符。 ② 统计一个文件夹下有多少个文件夹，不包括子文件夹。 1ls -l | grep "^d" | wc -l grep &quot;^d&quot; 表示抓取行首以字符 d 开头的行。 ③ 统计一个文件夹下有多少个文件，包括子文件夹里的。 1ls -lR | grep "^-" | wc -l 其中 ls 命令的 -R 参数表示递归，即：会遍历左右的子文件夹。 ④ 统计一个文件夹下有多少个文件夹，包括子文件夹 1ls -lR | grep "^d" | wc -l 3. 使用 find 命令和 wc 命令当然，要递归遍历所有的子文件夹的话，find 命令也是一个不错的选择，因为 find 在默认情况下（不加 -maxdepth 参数），是会遍历所有的子文件夹的。find 的使用方法可以参考之前的文章：Linux 学习记录: find 命令 下面来看使用方法。 1find . -type f | wc -l 上述命令会遍历当前文件夹及其子文件夹，统计所有文件类型为 f （普通文件）的个数。 同样道理，我们设置文件类型为 d，就能统计所有子文件及的个数： 1find . -type d | wc -l 当然，也可以只统计当前目录，不深入其子目录统计。 统计文件数： 1find . -maxdepth 1 -type f | wc -l 统计目录数： 1find . -maxdepth 1 -type d | wc -l 4. References How To Count Files in Directory on Linux linux命令看文件或文件夹个数]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>find</tag>
        <tag>grep</tag>
        <tag>wc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 学习记录：jq 处理 json]]></title>
    <url>%2F2020%2F05%2F25%2FLinux-%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%9Ajq-%E5%A4%84%E7%90%86-json%2F</url>
    <content type="text"><![CDATA[这篇文章总结下 Linux 下如何在 shell 中利用 jq 处理 json 数据。 1. 背景最近项目上需要利用政府数据做些分析，但是深圳市政府提供的 API，请求的数据都是分页的，通过 curl 每次只能请求一部分数据，但是我们的应用需要利用所有的数据，所以得将每次请求的 分片数据 整合起来。 又因为 API 返回的数据是 json 格式的，所以得需要在 shell 中处理 json。 于是想起了之前在 Github 上遇到的项目：the art of command line，里面介绍到了一个工具 jq，专门用来在 shell 里面处理 json 数据。于是专门学习了下其使用方法，在这篇文章中作个总结。 2. jq 的使用方法2.1 准备工作 这里我用深圳市政府提供的 个体工商户基本信息 来做演示。 请求这个 API 需要三个参数： page : 返回的数据在第几个分页； rows : 当前这个分页下面包含多少条数据记录 appKey : 你自己的 appKey ，这个需要注册用户后才能获取到。 我们首先看直接在 shell 中用 GET 方式请求数据会得到什么。 输入： 1ubuntu@VM-0-14-ubuntu:~$ curl https://opendata.sz.gov.cn/api/29200_01303570/1/service.xhtml?page=1\&amp;rows=3\&amp;appKey=&lt;填用户自己的 appKey&gt; 输出： 1&#123;"total":2464687,"data":[&#123;"OPLOC":"深圳市宝安区福永街道兴围社区兴业一路27号第一层101号铺","ENTTYPE":"9510","REGORG":"440306","REGSTATE_CN":"吊销，未注销","NAME":"汤福祥","REGORG_CN":"宝安局","OPSCOPE":"公话（在授权范围内经营公用电话业务）。","REGSTATE":"2","COMPFORM_CN":"个人经营","REGNO":"440306803535369","APPRDATE":"2007-11-26","COMPFORM":"1","ESTDATE":"2007-11-26","TRANAME":"深圳市宝安区福永福家公话亭"&#125;,&#123;"OPLOC":"深圳市福田区南园街道南园路埔尾新村28号103","ENTTYPE":"9510","REGORG":"440304","REGSTATE_CN":"存续（在营、开业、在册）","NAME":"周玉红","REGORG_CN":"福田局","OPSCOPE":"化妆品、日用品的批发与零售^","REGSTATE":"1","COMPFORM_CN":"个人经营","REGNO":"440304816709149","APPRDATE":"2016-09-20","COMPFORM":"1","ESTDATE":"2016-09-20","TRANAME":"深圳市福田区安颜化妆品商行"&#125;,&#123;"OPLOC":"深圳市坪山新区坑梓老坑社区西坑新大道12号101","ENTTYPE":"9510","REGORG":"440310","REGSTATE_CN":"吊销，未注销","NAME":"伍群红","REGORG_CN":"坪山局","OPSCOPE":"美容（不含医学美容）（《卫生许可证》有效期至2015年12月25日）。","REGSTATE":"2","COMPFORM_CN":"个人经营","REGNO":"440309807694281","APPRDATE":"2011-12-30","COMPFORM":"1","ESTDATE":"2011-12-30","TRANAME":"深圳市坪山新区爱妍坊美容中心"&#125;],"page":1,"rows":3&#125; 可以看到服务器上返回的 json 数据是很杂乱的，没有我们熟悉的 json 格式。这时候 jq 就可以派上用场了。 2.2 . 用法jq 最简单的用法使用 . 来美化输出。 还是之前的例子，我们在命令尾部通过管道 | 将 GET 获取到的 json 数据传递给 jq 。 输入： 1ubuntu@VM-0-14-ubuntu:~$ curl https://opendata.sz.gov.cn/api/29200_01303570/1/service.xhtml?page=1\&amp;rows=3\&amp;appKey=&lt;填用户自己的 appKey&gt; | jq '.' # 注意 jq 后面跟的表达式一定要用引号包裹住 输出： 123456789101112131415161718192021222324&#123; "total": 2464687, "data": [ &#123; "OPLOC": "深圳市宝安区福永街道兴围社区兴业一路27号第一层101号铺", "ENTTYPE": "9510", "REGORG": "440306", "REGSTATE_CN": "吊销，未注销", "NAME": "汤福祥", "REGORG_CN": "宝安局", "OPSCOPE": "公话（在授权范围内经营公用电话业务）。", "REGSTATE": "2", "COMPFORM_CN": "个人经营", "REGNO": "440306803535369", "APPRDATE": "2007-11-26", "COMPFORM": "1", "ESTDATE": "2007-11-26", "TRANAME": "深圳市宝安区福永福家公话亭" &#125;, # 省略...... ], "page": 1, "rows": 3&#125; 可以看到，输出的 json 已经是 well-formatted 的了，这是我们想要看到的。 2.2 获取数组中的元素上述返回的 json 数据中，&quot;data&quot; 字段是一个数组。 如果我们需要获取 &quot;data&quot; 字段第一个元素的值，可以这么做： 1ubuntu@VM-0-14-ubuntu:~$ curl https://opendata.sz.gov.cn/api/29200_01303570/1/service.xhtml?page=1\&amp;rows=3\&amp;appKey=&lt;填用户自己的 appKey&gt; | jq '.data[0]' 输出： 12345678910111213141516&#123; "OPLOC": "深圳市宝安区福永街道兴围社区兴业一路27号第一层101号铺", "ENTTYPE": "9510", "REGORG": "440306", "REGSTATE_CN": "吊销，未注销", "NAME": "汤福祥", "REGORG_CN": "宝安局", "OPSCOPE": "公话（在授权范围内经营公用电话业务）。", "REGSTATE": "2", "COMPFORM_CN": "个人经营", "REGNO": "440306803535369", "APPRDATE": "2007-11-26", "COMPFORM": "1", "ESTDATE": "2007-11-26", "TRANAME": "深圳市宝安区福永福家公话亭"&#125; 如果想要获取数组中所有元素怎们办呢？ 输入： 1ubuntu@VM-0-14-ubuntu:~$ curl https://opendata.sz.gov.cn/api/29200_01303570/1/service.xhtml?page=1\&amp;rows=3\&amp;appKey=&lt;填用户自己的 appKey&gt; | jq '.data[]' 输出： 1234567891011121314151617181920212223&#123; "OPLOC": "深圳市宝安区福永街道兴围社区兴业一路27号第一层101号铺", "ENTTYPE": "9510", "REGORG": "440306", "REGSTATE_CN": "吊销，未注销", "NAME": "汤福祥", "REGORG_CN": "宝安局", "OPSCOPE": "公话（在授权范围内经营公用电话业务）。", "REGSTATE": "2", "COMPFORM_CN": "个人经营", "REGNO": "440306803535369", "APPRDATE": "2007-11-26", "COMPFORM": "1", "ESTDATE": "2007-11-26", "TRANAME": "深圳市宝安区福永福家公话亭"&#125;&#123; "OPLOC": "深圳市福田区南园街道南园路埔尾新村28号103", "ENTTYPE": "9510", "REGORG": "440304", "REGSTATE_CN": "存续（在营、开业、在册）", "NAME": "周玉红",# 省略...... 需要注意的是，这种方式返回的数据是单独的 {} 组成的，是一条条独立的数据，并不是一个数组。 如果我们需要这些离散的数据组合成一个数组，可以这么做： 输入： 1ubuntu@VM-0-14-ubuntu:~$ curl https://opendata.sz.gov.cn/api/29200_01303570/1/service.xhtml?page=1\&amp;rows=3\&amp;appKey=&lt;填用户自己的 appKey&gt; | jq '[.data[]]' 输出： 12345678910111213141516171819[ &#123; "OPLOC": "深圳市宝安区福永街道兴围社区兴业一路27号第一层101号铺", "ENTTYPE": "9510", "REGORG": "440306", "REGSTATE_CN": "吊销，未注销", "NAME": "汤福祥", "REGORG_CN": "宝安局", "OPSCOPE": "公话（在授权范围内经营公用电话业务）。", "REGSTATE": "2", "COMPFORM_CN": "个人经营", "REGNO": "440306803535369", "APPRDATE": "2007-11-26", "COMPFORM": "1", "ESTDATE": "2007-11-26", "TRANAME": "深圳市宝安区福永福家公话亭" &#125;,# 省略....] 可以看到 {} 最外面已经包裹了一层 []，说明现在是一个数组。 2.3 获取某些特定的字段值有些应用场景下，我们可能只需要利用返回的 json 中某个或某几个特定的 key。 拿上面返回的 json 数据为例，我们只需要 &quot;data&quot; 属性下面的 &quot;NAME&quot; 和 &quot;REGNO&quot; 两个属性，我们可以这么做： 输入： 1ubuntu@VM-0-14-ubuntu:~$ curl https://opendata.sz.gov.cn/api/29200_01303570/1/service.xhtml?page=1\&amp;rows=3\&amp;appKey=&lt;填用户自己的 appKey&gt; | jq '.data[] | &#123;NAME: .NAME, REGNO: .REGNO&#125;' 输出： 123456789101112&#123; "NAME": "汤福祥", "REGNO": "440306803535369"&#125;&#123; "NAME": "周玉红", "REGNO": "440304816709149"&#125;&#123; "NAME": "伍群红", "REGNO": "440309807694281"&#125; 同样，如果想要返回的数据组装成一个数组，可以在最外层包裹一个 [] : 1ubuntu@VM-0-14-ubuntu:~$ curl https://opendata.sz.gov.cn/api/29200_01303570/1/service.xhtml?page=1\&amp;rows=3\&amp;appKey=&lt;填用户自己的 appKey&gt; | jq '[.data[] | &#123;NAME: .NAME, REGNO: .REGNO&#125;]' 同样，如果只需要 &quot;data&quot; 属性的第一个元素的 &quot;NAME&quot; 和 &quot;REGNO&quot; 属性，只需要将 data[] 换成 data[0] 即可： 1ubuntu@VM-0-14-ubuntu:~$ curl https://opendata.sz.gov.cn/api/29200_01303570/1/service.xhtml?page=1\&amp;rows=3\&amp;appKey=&lt;填用户自己的 appKey&gt; | jq '.data[0] | &#123;NAME: .NAME, REGNO: .REGNO&#125;' 2.4 返回的字段重命名我们也可以将自己想要获取的字段的名字进行重命名： 还是用上面的例子做示范：我们想要将原字段名 &quot;NAME&quot; 改成 &quot;my_name&quot; ，将 &quot;REGNO&quot; 改成 &quot;my_regno&quot;，可以这么做： 1ubuntu@VM-0-14-ubuntu:~$ curl https://opendata.sz.gov.cn/api/29200_01303570/1/service.xhtml?page=1\&amp;rows=3\&amp;appKey=&lt;填用户自己的 appKey&gt; | jq '.data[] | &#123;my_name: .NAME, myregno: .REGNO&#125;' 输出： 123456789101112&#123; "my_name": "汤福祥", "my_regno": "440306803535369"&#125;&#123; "my_name": "周玉红", "my_regno": "440304816709149"&#125;&#123; "my_name": "伍群红", "my_regno": "440309807694281"&#125; 可以看到，返回的字段名已经变成我们自定义的名称了。 3. References jq official tutorials 给力的linux命令–jq简易教程 the-art-of-command-line]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>curl</tag>
        <tag>jq</tag>
        <tag>json</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 学习记录：su 和 sudo]]></title>
    <url>%2F2020%2F05%2F23%2FLinux-%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%9Asu-%E5%92%8C-sudo%2F</url>
    <content type="text"><![CDATA[之前一直对 su 和 sudo 这两个命令犯迷糊，最近专门搜了这方面的资料，总算是把两者的关系以及用法搞清楚了，这篇文章来系统总结一下。 1. 准备工作因为本篇博客中涉及到用户切换，所以我需要提前准备好几个测试用户，方便后续切换。 Linux 中新建用户的命令是 useradd ，一般系统中这个命令对应的路径都在 PATH 环境变量里，如果直接输入 useradd 不管用的话，就用绝对路径名的方式：/usr/sbin/useradd 。 useradd 新建用户命令只有 root 用户才能执行，我们先从普通用户 ubuntu 切换到 root 用户（如何切换后文会介绍）： 12345ubuntu@VM-0-14-ubuntu:~$ su -Password: # 输入 root 用户登录密码root@VM-0-14-ubuntu:~# useradd -m test_user # 带上 -m 参数root@VM-0-14-ubuntu:~# ls /hometest_user ubuntu # 可以看到 /home 目录下面有两个用户了 因为还没有给新建的用户 test_user 设置登录密码，这就导致我们无法从普通用户 ubuntu 切换到 test_user，所以接下来，我们需要用 root 来设置 test_user 的登录密码。需要用到 passwd 命令： 12345root@VM-0-14-ubuntu:~# passwd test_userEnter new UNIX password: # 输出 test_user 的密码Retype new UNIX password: passwd: password updated successfullyroot@VM-0-14-ubuntu:~# 接着我们输入 exit 退出 root 用户到 普通用户 ubuntu： 123root@VM-0-14-ubuntu:~# exitlogoutubuntu@VM-0-14-ubuntu:~$ 可以看到，命令提示符前面已经由 root 变成 ubuntu，说明我们现在的身份是 ubuntu 用户。 2. su 命令介绍及主要用法首先需要解释下 su 代表什么意思。 之前一直以为 su 是 super user，查阅资料之后才知道原来表示 switch user。 知道 su 是由什么缩写来的之后，那么它提供的功能就显而易见了，就是切换用户。 2.1 - 参数su 的一般使用方法是： 1su &lt;user_name&gt; 或者 1su - &lt;user_name&gt; 两种方法只差了一个字符 -，会有比较大的差异： 如果加入了 - 参数，那么是一种 login-shell 的方式，意思是说切换到另一个用户 &lt;user_name&gt; 之后，当前的 shell 会加载 &lt;user_name&gt; 对应的环境变量和各种设置； 如果没有加入 - 参数，那么是一种 non-login-shell 的方式，意思是说我现在切换到了 &lt;user_name&gt;，但是当前的 shell 还是加载切换之前的那个用户的环境变量以及各种设置。 光解释会比较抽象，我们看一个例子就比较容易理解了。 我们首先从 ubuntu 用户以 non-login-shell 的方式切换到 root 用户，比较两种用户状态下环境变量中 PWD 的值（su 命令不跟任何 &lt;user_name&gt; ，默认切换到 root 用户）： 12345678910ubuntu@VM-0-14-ubuntu:~$ env | grep ubuntuUSER=ubuntuPWD=/home/ubuntu # 是 /home/ubuntuHOME=/home/ubuntu# 省略......ubuntu@VM-0-14-ubuntu:~$ su # non-login-shell 方式Password: # 输入 root 用户登录密码root@VM-0-14-ubuntu:/home/ubuntu# env | grep ubuntuPWD=/home/ubuntu # 可以发现还是 /home/ubunturoot@VM-0-14-ubuntu:/home/ubuntu# 我们的确是切换到 root 用户了，但是 shell 环境中的变量并没有改变，还是用之前 ubuntu 用户的环境变量。 接着我们从 ubuntu 用户以 login-shell 的方式切换到 root 用户，同样比较两种用户转台下环境变量中 PWD 的值： 1234567891011121314ubuntu@VM-0-14-ubuntu:~$ env | grep ubuntuUSER=ubuntuPWD=/home/ubuntu # 是 /home/ubuntuHOME=/home/ubuntu# 省略.......ubuntu@VM-0-14-ubuntu:~$ su - # 是 login-shell 方式Password:root@VM-0-14-ubuntu:~# env | grep rootUSER=rootPWD=/root # 已经变成 /root 了HOME=/rootMAIL=/var/mail/rootLOGNAME=rootroot@VM-0-14-ubuntu:~# 可以看到用 login-shell 的方式切换用户的话，shell 中的环境变量也跟着改变了。 总结：具体使用哪种方式切换用户看个人需求： 如果不想因为切换到另一个用户导致自己在当前用户下的设置不可用，那么用 non-login-shell 的方式； 如果切换用户后，需要用到该用户的各种环境变量（不同用户的环境变量设置一般是不同的），那么使用 login-shell 的方式。 2.2 切换到指定用户前面已经介绍了，如果 su 命令后面不跟任何 &lt;user_name&gt;，那么默认是切换到 root 用户： 123ubuntu@VM-0-14-ubuntu:~$ su -Password: # root 用户的密码root@VM-0-14-ubuntu:/home/ubuntu# 因为我们在 1. 准备工作 部分已经新建了一个 test_user 用户，并且我们也知道 test_user 用户的登录密码（root 用户设置的），我们就能从 ubuntu 用户切换到 test_user 用户： 123ubuntu@VM-0-14-ubuntu:~$ su - test_userPassword: # test_user 用户的密码$ 2.3 -c 参数前面的方法中，我们都是先切换到另一个用户（root 或者 test_user），在哪个用户的状态下执行命令，最后输入 exit 返回当前 ubuntu 用户。 还有一种方式是：不需要先切换用户再执行命令，可以直接在当前用户下，以另一个用户的方式执行命令，执行结束后就返回当前用户。这就得用到 -c 参数。 具体使用方法是： 1su - -c "指令串" # 以 root 的方式执行 "指令串" 我么看个例子： 12345678910ubuntu@VM-0-14-ubuntu:~$ cat /etc/shadowcat: /etc/shadow: Permission denied # ubuntu 用户不能直接查看 /etc/shadow 文件内容ubuntu@VM-0-14-ubuntu:~$ su - -c "tail -n 4 /etc/shadow"Password: # 输入 root 用户密码ubuntu:$1$fZKcWEDI$uwZ64uFvVbwpHTbCSgim0/:18352:0:99999:7:::ntp:*:17752:0:99999:7:::mysql:!:18376:0:99999:7:::test_user:$6$.ZY1lj4m$ii0x9CG8h.JHlh6zKbfBXRuolJmIDBHAd5eqhvW7lbUQXTRS//89jcuTzRilKqRkP8YbYW4VPxmTVHWRLYNGS/:18406:0:99999:7:::ubuntu@VM-0-14-ubuntu:~$ # 执行完马上返回 ubuntu 用户而不是 root 用户 这种执行方式和后面要介绍的 sudo 很像，都是临时申请一下 root 用户的权限。但还是有差异，我们接着往后看。 3. sudo 命令介绍及主要用法首先还是解释下 sudo 命令是什么意思。 sudo 的英文全称是 super user do，即以超级用户（root 用户）的方式执行命令。这里的 sudo 和之前 su 表示的 switch user 是不同的，这点需要注意，很容易搞混。 我们先介绍 sudo 命令能做什么事情，然后说明为何能做到这些，以及如何做到这些。 我们开始。 3.1 主要用法我们在 Linux 中经常会碰到 Permission denied 这种情况，比如以 ubuntu 用户的身份查看 /etc/shadow 的内容。因为这个文件的内容是只有 root 用户能查看的。 那如果我们想要查看怎么办呢？这时候就可以使用 sudo : 12345678ubuntu@VM-0-14-ubuntu:~$ tail -n 3 /etc/shadowtail: cannot open '/etc/shadow' for reading: Permission denied # 没有权限ubuntu@VM-0-14-ubuntu:~$ sudo !! # 跟两个惊叹号sudo tail -n 3 /etc/shadowntp:*:17752:0:99999:7:::mysql:!:18376:0:99999:7:::test_user:$6$.ZY1lj4m$ii0x9CG8h.JHlh6zKbfBXRuolJmIDBHAd5eqhvW7lbUQXTRS//89jcuTzRilKqRkP8YbYW4VPxmTVHWRLYNGS/:18406:0:99999:7:::ubuntu@VM-0-14-ubuntu:~$ 实例中，我们使用了 sudo !! 这个小技巧，表示重复上面输入的命令，只不过在命令最前面加上 sudo 。 因为我已经设置了 sudo 命令不需要输入密码，所以这里 sudo !! 就能直接输出内容。如果没有设置的话，需要输入当前这个用户的密码，例如本例中，我就应该输入 ubuntu 用户的登录密码。 两次相邻的 sudo 操作，如果间隔在 5min 之内，第二次输入 sudo 不需要重新输入密码；如果超过 5min，那么再输入 sudo 时，又需要输入密码。所以一个比较省事的方法是设置 sudo 操作不需要密码。后面介绍如何设置。 sudo 除了以 root 用户的权限执行命令外，还有其它几个用法，这里做简单介绍。 切换到 root 用户： 1sudo su - 这种方式也能以 login-shell 的方式切换到 root 用户，但是它和 su - 方法是由区别的： 前者输入 sudo su - 后，需要提供当前用户的登录密码，也就是 ubuntu 用户的密码； 后者输入 su - 后，需要提供 root 用户的登录密码。 还有一个命令： 1sudo -i 这个命令和 sudo su - 效果一致，也是切换到 root 用户，也是需要提供当前用户（ubuntu 用户）的登录密码。 我们现在切换到 test_user 用户，尝试显示 /etc/shadow 文件的内容： 123456ubuntu@VM-0-14-ubuntu:~$ su - test_userPassword: # test_user 的密码$ sudo cat /etc/shadow[sudo] password for test_user: # test_user 的密码test_user is not in the sudoers file. This incident will be reported.$ 我们会看到倒数第二行中的错误提示信息，我们无法查看 /etc/shadow 的内容，这是为什么？为什么 ubuntu 可以使用 sudo 但是 test_user 不行呢？ 这就涉及到 sudo 的工作原理了。 3.2 sudo 工作原理一个用户能否使用 sudo 命令，取决于 /etc/sudoers 文件的设置。 从 3.1 节中我们已经看到，ubuntu 用户可以正常使用 sudo ，但是 test_user 用户却无法使用，这是因为 /etc/sudoers 文件里没有配置 test_user。 /etc/sudoers 也是一个文本文件，但是因其有特定的语法，我们不要直接用 vim 或者 vi 来编辑它，需要用 visudo 这个命令。输入这个命令之后就能直接编辑 /etc/sudoers 这个文件了。 需要说明的是，只有 root 用户有权限使用 visudo 命令。 我们先来看下输入 visudo 命令后显示的内容。 输入（root 用户）： 1root@VM-0-14-ubuntu:~# visudo 输出： 12345678910111213# User privilege specificationroot ALL=(ALL:ALL) ALL# Members of the admin group may gain root privileges%admin ALL=(ALL) ALL# Allow members of group sudo to execute any command%sudo ALL=(ALL:ALL) ALL# See sudoers(5) for more information on "#include" directives:#includedir /etc/sudoers.dubuntu ALL=(ALL:ALL) NOPASSWD: ALL 解释下每一行的格式： 第一个表示用户名，如 root 、ubuntu 等； 接下来等号左边的 ALL 表示允许从任何主机登录当前的用户账户； 等号右边的 ALL 表示：这一行行首对一个的用户可以切换到系统中任何一个其它用户； 行尾的 ALL 表示：当前行首的用户，能以 root 用户的身份下达什么命令，ALL 表示可以下达任何命令。 我们还注意到 ubuntu 对应的那一行有个 NOPASSWD 关键字，这就是表明 ubuntu 这个用户在请求 sudo 时不需要输入密码，到这里就解释了前面的问题。 同时我们注意到，这个文件里并没有 test_user 对应的行，这也就解释了为什么 test_user 无法使用 sudo 命令。 接下来，我们尝试将 test_user 添加到 /etc/sudoers 文件中，使 test_user 也能使用 sudo 命令。我们在最后一行添加： 1test_user ALL=(ALL:ALL) ALL # test_user 使用 sudo 需要提供 test_user 的密码 接下来我们再在 test_user 账户下执行 sudo ： 123456789ubuntu@VM-0-14-ubuntu:~$ su - test_userPassword:$ tail -n 3 /etc/shadowtail: cannot open '/etc/shadow' for reading: Permission denied$ sudo tail -n 3 /etc/shadow # 加上 sudontp:*:17752:0:99999:7:::mysql:!:18376:0:99999:7:::test_user:$6$.ZY1lj4m$ii0x9CG8h.JHlh6zKbfBXRuolJmIDBHAd5eqhvW7lbUQXTRS//89jcuTzRilKqRkP8YbYW4VPxmTVHWRLYNGS/:18406:0:99999:7:::$ 可以看到，现在已经可以使用 sudo 了。 3.2 思考我们已经看到了，如果一个用户在 /etc/sudoers 文件中，那么它就具有 sudo 权限，就能通过 sudo su - 或者 sudo -i 等命令切换到 root 用户了，那这时这个用户就变成 root 用户了，那这不对系统造成很大的威胁吗？ 实际上的确是这样的。所以如果在编辑 /etc/sudoers 文件赋予某种用户 sudo 权限时，必须要确定该用户是可信任的，不会对系统造成恶意破坏，否则将所有 root 权限都赋予该用户将会有非常大的危险。 当然，root 用户也可以编辑 /etc/sudoers 使用户只具备一部分权限，即只能执行一小部分命令。有兴趣的读者可以参考 Reference 部分第二条，这篇文章不再赘述。 4. 二者的差异对比我们已经看到： 使用 su - ，提供 root 账户的密码，可以切换到 root 用户； 使用 sudo su - ，提供当前用户的密码，也可以切换到 root 用户 两种方式的差异也显而易见：如果我们的 Linux 系统有很多用户需要使用的话，前者要求所有用户都知道 root 用户的密码，这显然是非常危险的；后者是不需要暴露 root 账户密码的，用户只需要输入自己的账户密码就可以，而且哪些用户可以切换到 root，这完全是受 root 控制的（root 通过设置 /etc/sudoers 实现的），这样系统就安全很多了。 一般都是推荐使用 sudo 方式。 5. References The Difference Between su and sudo Commands In Linux 《鸟哥的 Linux 私房菜》13.4 节：使用者身份切换 用户与用户组、文件权限、文件系统层次结构 The Differences between Su, Sudo Su, Sudo -s and Sudo -i What’s the difference between “sudo -i” and “sudo su -” [closed] 命令前加sudo执行和用真正的root用户执行有什么区别？ Linux创建用户、设置密码、修改用户、删除用户命令]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>su</tag>
        <tag>sudo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 学习记录：tree 显示树形目录结构]]></title>
    <url>%2F2020%2F05%2F13%2FLinux-%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%9Atree-%E6%98%BE%E7%A4%BA%E6%A0%91%E5%BD%A2%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[这篇文章总结下 Linux 和 Windows 中 tree 命令的常用使用方法。 1. 背景有时候在进行工程项目展示时，需要提供目录结构，这时候最方便的一种展示形式是 树形结构 ，tree 命令就是帮助我们实现这个功能的。 2. 使用方法2.1 在 Windows 上使用Windows 上 tree 命令的参数很少，使用起来较为简单。 主要有两个参数（大小写不敏感），注意不是 -f / -a 的格式： /f : 显示当前目录下所有的文件，包括目录名和文件名； /a : 只显示当前目录下的所有目录名，即：当前目录的子目录、子目录的子目录…….，不会显示任何一个目录中的文件名。注：这里的 /a 参数不是 /all 的意思。 下面来看具体的例子。 首先利用 tree /f 查看当前目录下的所有文件，同时列出目录名： 123456789101112131415161718192021PS C:\Users\tanjuntao\Desktop\组会&gt; tree /f文件夹 PATH 列表卷序列号为 004F-9E97C:.│ api第二版本规划-2019.3.4.pptx│ API组科研讨论_1.28.pptx│ 0924.pptx│ 1119.pptx│ 恶意样本攻击和防御.pptx│ 2019-1-26.pptx│ 保存----保存.pptx│ 第二版本规划-2019.3.4.pptx│├─2018-4-17-论文组会│ juntaotan-poisoning attacks against regression learning- slides.pptx│ 讲稿.docx│└─算法 Aggarwal2009_Chapter_AdaptiveSamplingForK-MeansClus.pdf kechen.pdf prob_inequalities.pdf 如果我们使用 tree /a，那么将只会显示目录名： 123456PS C:\Users\tanjuntao\Desktop\组会&gt; tree /a文件夹 PATH 列表卷序列号为 004F-9E97C:.+---2018-4-17-论文组会\---算法 2.2 在 Linux 上使用Linux 上 tree 命令的参数比 Windows 上要多，使用起来稍微麻烦一点。 主要的参数有（区分大小写）： -f : 显示当前目录（包括各个子目录）的所有文件； -d : 显示当前目录（包括各个子目录）的所有目录名； -L : 后面需要跟一个数字 num，用来控制目录的深度。例如 num = 1，那么表示仅仅显示当前目录（不包括任何子目录）下的文件名额目录名。 前面两个参数的使用方法较为简单，不再赘述。下面演示下 -L 参数的具体使用。 输入： 1tree -L 1 #目录层数是 1，即当前目录下的所有文件名和目录名 输出： 12345678user@DataServer:/datapool/workspace/tanjuntao/data$ tree -L 1.├── cifar├── mnist├── processed└── raw4 directories, 0 files 将 num 的值改为 2： 输入： 1tree -L 2 #目录层数为 2 输出： 123456789101112131415161718user@DataServer:/datapool/workspace/tanjuntao/data$ tree -L 2.├── cifar│ └── MNIST├── mnist│ ├── MNIST│ ├── processed│ └── raw├── processed│ ├── test.pt│ └── training.pt└── raw ├── t10k-images-idx3-ubyte ├── t10k-labels-idx1-ubyte ├── train-images-idx3-ubyte └── train-labels-idx1-ubyte8 directories, 6 files References Microsoft Docs : tree Linux tree 命令]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>tree</tag>
        <tag>目录结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[version 大全]]></title>
    <url>%2F2020%2F05%2F07%2Fversion-%E5%A4%A7%E5%85%A8%2F</url>
    <content type="text"><![CDATA[这篇文章总结下一些常用的 系统 或者 软件 如何查看其版本号，方便后续查阅。 注：如果没有特殊说明，输入的命令在 Windows 下和 Linux 下都有效。 Windows 系统版本在 CMD 中输入： 1ver 我的笔记本是 win10 系统，输出为： 123C:\Users\tanjuntao&gt;verMicrosoft Windows [版本 10.0.18363.778] Python 版本在 windows 的 CMD 或者 Linux 的命令行中输入： 1python --version 输出： 12C:\Users\tanjuntao&gt;python --versionPython 3.6.3 MySQL 版本当我们登录到 MySQL 时，会有提示信息输出，可以找到 Server Version 关键字，后面就是我们 MySQL 的版本号。 还有另外一种查看版本号的方法。先登录 MySQL，然后输入： 1mysql&gt; select version(); 输出： 1234567mysql&gt; select version();+-----------+| version() |+-----------+| 8.0.11 |+-----------+1 row in set (0.00 sec) npm 版本前提是 npm 已经加入到系统环境变量中，我们只需要在命令行中输入： 1npm -v 输出： 12PS C:\Users\tanjuntao&gt; npm -v6.4.1 pip 版本输入： 1pip --version 输出： 12user@DataServer:~$ pip --versionpip 19.1.1 from /home/user/.local/lib/python3.5/site-packages/pip (python 3.5) conda 版本输入： 1conda --version 输出： 12user@DataServer:~$ conda --versionconda 4.5.11 node.js 版本输出 ： 1node -v 输出： 12PS C:\Users\tanjuntao&gt; node -v v10.13.0 ubuntu 版本输入： 1lsb_release -a 输出： 123456user@DataServer:~$ lsb_release -aNo LSB modules are available.Distributor ID: UbuntuDescription: Ubuntu 16.04.5 LTSRelease: 16.04Codename: xenial 从输出结果中可以看到，该服务器中安装的 ubuntu 版本是 16.04 LTS，发行版本代号是 xenial。 我们看另一个版本是 18.04 LTS 的 ubuntu： 123456tan@DESKTOP-OPQIR8V:/mnt/c/Users/tanjuntao$ lsb_release -aNo LSB modules are available.Distributor ID: UbuntuDescription: Ubuntu 18.04.4 LTSRelease: 18.04Codename: bionic 从输出结果中我们可以知道，该版本的发行代号是 bionic。 使用 apt 查看某个软件包版本输入： 1apt show &lt;包名&gt; 这里以查看 tmux 的版本为例； 1apt show tmux 输出： 123456789101112tan@DESKTOP-OPQIR8V:/mnt/c/Users/tanjuntao$ apt show tmuxPackage: tmuxVersion: 2.6-3ubuntu0.2Priority: optionalSection: adminOrigin: UbuntuMaintainer: Ubuntu Developers &lt;ubuntu-devel-discuss@lists.ubuntu.com&gt;Original-Maintainer: Romain Francoise &lt;rfrancoise@debian.org&gt;Bugs: https://bugs.launchpad.net/ubuntu/+filebugInstalled-Size: 647 kB......&lt;Content Omitted&gt; 从输出的结果的 Version 字段可以知道软件包的版本。 Linux 内核版本有两种方式可以查看内核版本号。 1cat /proc/version 输出： 12tan@tjt1024:/mnt/c/Users/tanjuntao$ cat /proc/versionLinux version 4.4.0-18362-Microsoft (Microsoft@Microsoft.com) (gcc version 5.4.0 (GCC) ) #836-Microsoft Mon May 05 16:04:00 PST 2020 1uname -a 输出： 1Linux tjt1024 4.4.0-18362-Microsoft #836-Microsoft Mon May 05 16:04:00 PST 2020 x86_64 x86_64 x86_64 GNU/Linux Google Chrome 版本1$ google-chrome --version Firefox 版本1$ firefox --version]]></content>
      <categories>
        <category>杂</category>
      </categories>
      <tags>
        <tag>version</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python print() 函数常见用法总结]]></title>
    <url>%2F2020%2F05%2F02%2FPython-print-%E5%87%BD%E6%95%B0%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[参考下面这篇文章，写的非常非常好，非常全面。 https://realpython.com/python-print/#toc]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>重定向</tag>
        <tag>python</tag>
        <tag>print</tag>
        <tag>f-string</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[P/NP/NP-Complete/NP-Hard 问题的理解]]></title>
    <url>%2F2020%2F04%2F30%2FP-NP-NP-Complete-NP-Hard-%E9%97%AE%E9%A2%98%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[几种问题的定义如下： 问题类型 多项式时间可解 多项式时间可验证解的正确性 P YES YES NP uncertain YES NP-Complete uncertain YES NP-Hard uncertain or NO YES or NO 咋一看觉得 NP 问题和 NP-Complete 问题是一样的，两者的共同点都是： 依据目前的研究进展，还无法断定这两类问题能否在多项式时间内可解； 都可以在多项式时间内验证一个解是否正确。 其实，从集合的角度看，NP-Complete 类问题是 NP 类问题的一个子集，即：所有 NP-Complete 的问题都是 NP 问题，但不是所有的 NP 问题都是 NP-Complete 的。 NP-Complete 问题的完整定义是：所有的 NP 类问题都可以在 多项式时间 内 归约 到这个问题，那么这个问题就是 NP-Complete 的。 ======待续====== 下面一张 韦恩图 表示了上面几种问题之间的关系： 注意 YouTube 视频下面一个搞笑的段子 一定需要说明清楚：NP 问题面对的都是 decision problems，但是 NP-Hard 问题面对的可能不是简单的 decision problems，所以说 NP-hard 问题一般会更难。 只要 NPC 中的任何一个问题被解决了，那么所有的 NP 问题都将在多项式时间内解决掉。 NP 问题是包含 P 类问题的。 NP 问题中还要一些问题，不知道它们能不能在多项式时间内得到 solution，但是 NP 问题一定能在多项式时间内去 verify 这个 solution 是不是正确的。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>P</tag>
        <tag>NP</tag>
        <tag>NPC</tag>
        <tag>NP-Hard</tag>
        <tag>computational complexity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google 高级搜索]]></title>
    <url>%2F2020%2F04%2F10%2FGoogle-%E9%AB%98%E7%BA%A7%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[这篇文章总结下 Google 一些高级搜索方法。 1. 背景需求因女朋友做翻译训练，有时候需要知道一些词组如何搭配，普通的搜索满足不了需求，需要用到 Google 高级搜索，于是向我求助。我也借这个机会学习了下如何使用 Google 的高级搜索，发现有些功能确实很实用也很方便使用。所以打算单独写一篇文章总结下，方便后续查阅。 2. 高级搜索技巧2.1 sitesite 命令后面跟某一个具体的网站地址，表示只单独查找这个网站的内容，使用很简单。 举个例子：我需要在 time.com 上查找 coronavirus 相关的报道，可以这样做： 1coronavirus site:time.com site 命令在搜索关键字 coronavirus 的前面还是后面不影响搜索结果。 也就是说： 1site:time.com coronavirus 的效果一样。 搜索结果： 从结果可以看到，所有的条目都是 time.com 的内容。 *注：site: 命令中的冒号是英文输入状态下的冒号，冒号后面不能有空格。 2.2 filetypefiletype 后面跟某种文件类型，如 pdf 、doc 、ppt 等，表示搜索特定类型的文件。但是亲测目前 Google 不能搜索 epub 类型的文件。下面通过一个例子来了解这个命令的使用。 比方说：我要搜索《The Little Prince》的 pdf 版本，可以这样做： 1the little prince filetype:pdf 同样，filetype 命令在关键字前面还是后面不影响搜索结果。 搜索结果如下： 从上面的结果可以看到，我们搜出来的都是 PDF 版本的《The Little Prince》。 2.3 intitle 和 allintitleintitle 后面跟一个搜索关键字，表示在网页的标题 中进行搜索。 比方说，我需要搜索在网页标题中出现 coronavirus 关键字的网页，可以这样做： 1intitle:coronavirus 搜索结果： 可以看到，搜索到的网页的标题中都包含 coronavirus。 当然，intitle 命令还能配合 site 和 filetype 命令一起使用。 举个例子，我们需要在一些政府网站（一般是 .gov 结尾）中搜索标题中包含 coronavirus 的 PDF 文件，我们可以这么做： 1site:.gov intitle:coronavirus filetype:pdf 搜索结果： intitle 后面只能跟一个搜索关键字，如果我们需要搜索多个关键字，可以使用 allinititle 命令，使用方法和 intile 一样。 举个例子，我们想要在 www.washingtonpost.com 中搜索网页标题中包含 global pandemic 的报到，可以这样做： 1allintitle:global pandemic site:www.washingtonpost.com 2.4 intext 和 allintextintext 和 allintext 的使用和前面介绍的 intitle &amp; allintitle 一样，只不过 intext 表示搜索网页的正文，后面只能跟一个关键字。如果需要在网页正文中搜索多个关键字，需要用 allintext。 举个例子：在 time.com 下面搜索网页正文中包含 quarantine 关键字的新闻报到： 1intext:quarantine site:time.com 搜索结果： 同理，如果需要搜索的关键字是 coronavirus disease，可以这么做： 1allintext:coronavirus disease site:time.com 从搜索结果中可以看到，第二条和第三天结果中并不精准包含 coronavirus disease 关键字，而只是出现了 disease。 如果需要精准搜索，我们需要用到双引号&quot;&quot;。 2.5 双引号 &quot;&quot; 的使用双引号 &quot;&quot; 表示精准搜索，即搜索结果一定要和双引号内的搜索关键字精准匹配。 同样是 2.4 节的例子，如果我们需要搜索结果中精确地、一字不差的包含 coronavirus disease 关键字的话，我们可以加上双引号： 1allintext:"coronavirus disease" site:time.com 这一次可以明显看到，搜索结果中都精确包含了 coronavirus disease 关键字。 2.6 通配符 * 和 . 的使用学过数据库、接触过 Linux 的朋友对通配符的概念应该比较熟悉，Google 高级搜索中也可以使用通配符。 . : 表示匹配某一个字符； * : 表示匹配任意长度的字符串。 举个很不恰当的例子。比方说我们这么搜： 1tu.k 在第 3 个字符的位置，我们使用 . 通配符，表示这个位置可以出现任意一种字符，那么可以预期的是，包含 tuck 、tu&amp;k 、tu3k etc. 的网页都会被检索出来。 搜索结果如下： 我们可以看到，TU-K 被检索出来。这里，. 就表示 -。 个人认为 . 单个字符通配功能一般，还是 * 更加强大。 举个英语翻译中实际碰到的例子：我们知道主语 agriculture 和宾语 hunger，我们想搭配一个谓语动词，但是不知道用什么词好，这时候就可以用上 * 通配符以及双引号 &quot;&quot; 来精确搜索了： 1"agriculture * hunger" 我们还可以在 intitle 或者 intext 中使用 * 通配符，不过涉及到空格问题，使用起来需要注意，具体来说分下面几种情况： intitle + &quot;&quot; + * : 通配符前后不能有空格 1intitle:"global*" site:time.com intitle + * : 通配符前后可以有空格 1intitle:global * site:time.com &quot;&quot; + * : 通配符前后可以有空格 1"global * " * : 通配符前后可以有空格 1global * 2.7 其它其它一些个人认为不是很常用的命令的使用方法，可以参考 Google高级搜索指令 和 Google高级搜索技巧之高级语法查询指令 。 3. References https://www.lifewire.com/search-using-wildcards-google-1616504 https://www.searchenginejournal.com/how-to-use-wildcard-search-with-various-google-services/28911/ http://www.googleguide.com/wildcard_operator.html http://blog.sina.com.cn/s/blog_72b6f3e20102vxnf.html https://zhuanlan.zhihu.com/p/23650199 https://www.bilibili.com/video/BV1Vs411T7jX?p=8]]></content>
      <categories>
        <category>杂</category>
      </categories>
      <tags>
        <tag>Google</tag>
        <tag>搜索</tag>
        <tag>高级搜索</tag>
        <tag>size</tag>
        <tag>intitle</tag>
        <tag>intext</tag>
        <tag>通配符</tag>
        <tag>搜索引擎</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[摩斯电码与布莱叶盲文]]></title>
    <url>%2F2020%2F04%2F06%2F%E6%91%A9%E6%96%AF%E7%94%B5%E7%A0%81%E4%B8%8E%E5%B8%83%E8%8E%B1%E5%8F%B6%E7%9B%B2%E6%96%87%2F</url>
    <content type="text"></content>
      <categories>
        <category>计算机</category>
      </categories>
      <tags>
        <tag>摩斯电码</tag>
        <tag>布莱叶盲文</tag>
        <tag>编码的奥秘</tag>
        <tag>编码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习记录:ps命令以及和grep配合使用]]></title>
    <url>%2F2020%2F03%2F29%2FLinux%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95-ps%E5%91%BD%E4%BB%A4%E4%BB%A5%E5%8F%8A%E5%92%8Cgrep%E9%85%8D%E5%90%88%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[这篇文章来总结下 Linux 中 ps 命令的常见使用方法，以及如何配合 grep 命令来查找特定的进程。 1. 需求背景在多人共用 GPU 的服务器上，我们经常使用 nvidia-smi 命令来查看当前 GPU 的占用情况，该命令会列出在哪块 GPU 上有进程在运行，以及该进程的 PID 。下面是一个例子： 123456+-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| 1 22244 C python3 7719MiB |+-----------------------------------------------------------------------------+ 光知道进程的 PID 还不够，有时候我们还想知道这个进程相关的 user 、start time 、CMD 等相关信息，这时候就需要用到 ps 命令和 grep 命令了。 2. ps 命令常见使用方法ps 命令的全称是 process status，就是进程状态的意思。 ps 命令是一个相当古老的命令，后面可以跟一大堆参数，这篇文章不打算罗列所有的参数，只考虑几个很常见的参数，以及这些参数的某种组合。 常见的参数主要有： -a : 不和当前登录终端相关的进程； -u : 有效的使用者相关的进程（不太懂）； -x : 通常与 -a 参数一块使用，做详细的输出； -f : 将所有和进程相关的信息都列出，做一个更加完整的输出； -e : 显示所有的进程，和 -A 参数等价； -A : 显示所有的进程，和 -e 参数等价。 一些常见的参数组合使用情形如下。 2.1 ps -aux命令： 1ps -aux 输出结果（截取其中一部分）： 1234567USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.0 119968 5212 ? Ss Jan12 27:04 /sbin/initroot 2 0.0 0.0 0 0 ? S Jan12 0:03 [kthreadd]root 3 0.0 0.0 0 0 ? S Jan12 1:21 [ksoftirqd/0]root 5 0.0 0.0 0 0 ? S&lt; Jan12 0:00 [kworker/0:0H]root 8 0.1 0.0 0 0 ? S Jan12 177:36 [rcu_sched]root 9 0.0 0.0 0 0 ? S Jan12 0:00 [rcu_bh] 解释下各个字段的含义。 USER : 很好理解，进程对应的用户名； PID : 很好理解，进程号（process ID）； %CPU : CPU 的使用率； %MEM : 内存的使用率； VSZ ：使用的虚拟内存大小（virtual size），不重要； TTY : 登录者的终端机位置，不重要； STAT : 进程的状态； START : 进程开始运行的时间； TIME : 此进程实际花费 CPU 的运行时间，不是系统时间； COMMAND : 触发该进程的指令。 该命令默认情况下是按照 PID 升序排列的，有时候我们可能希望其按照 CPU 或者按照内存降序排列，可以按照下面的方法做。 按照 CPU 降序排列： 1ps -aux --sort -pcpu 按照内存使用降序排列： 1ps -aux --sort -pmem 2.2 ps -ef命令： 1ps -ef 输出结果（截取其中一部分）： 1234567UID PID PPID C STIME TTY TIME CMDroot 1 0 0 Jan12 ? 00:27:04 /sbin/initroot 2 0 0 Jan12 ? 00:00:03 [kthreadd]root 3 2 0 Jan12 ? 00:01:21 [ksoftirqd/0]root 5 2 0 Jan12 ? 00:00:00 [kworker/0:0H]root 8 2 0 Jan12 ? 02:57:37 [rcu_sched]root 9 2 0 Jan12 ? 00:00:00 [rcu_bh] 这个命令的输出信息和 ps -aux 其实很像，都能列出当前系统中的所有进程。各个字段的含义如下： UID : 很好理解，进程对应的用户名； PID : 进程编号； PPID : 父进程编号（parent PID） C : 使用 CPU 的百分比； STIME : 进程开始运行的时间； TTY : 登录者的终端机位置，不重要； TIME : 此进程实际花费 CPU 的运行时间，不是系统时间； CMD : 触发该进程的指令。 2.3 查找某个特定用户的进程比方说我们系统中有 ubuntu 、root 等多个用户，我只想看 ubuntu 这个用户的进程，可以这样做： 1ps -u ubuntu 输出结果： 123456789 PID TTY TIME CMD PID TTY TIME CMD11193 ? 00:00:00 systemd11194 ? 00:00:00 (sd-pam)12676 ? 00:00:00 sshd12677 pts/0 00:00:00 bash18631 pts/0 00:00:00 ps18632 pts/0 00:00:00 less20491 ? 00:00:13 tmux: server32357 pts/1 00:00:00 bash 各个字段的意思和前面介绍的一致。 2.4 配合管道命令ps 命令的输出结果往往很长，我们可以通过管道，将输出结果导向 less 、more 命令，方便查看： 1ps -aux | less 1ps -ef | more 当然，如果需要将结果保存到文件，可以使用 输出重定向 功能： 1ps -aux &gt;&gt; process.txt # 将结果追加到 process.txt 文件中 最强大的还是配合 grep 命令一块使用。 举个例子，我们需要查找进程列表中和 ssh 相关的进程，那就可以这么做： 1ps -aux | grep ssh 输出结果（关键字 ssh 会被标红）： 123456789101112user@DataServer:~$ ps -ef | grep sshroot 344 2945 0 16:45 ? 00:00:00 sshd: user [priv]user 392 344 0 16:45 ? 00:00:00 sshd: user@nottyuser 393 392 0 16:45 ? 00:00:00 /usr/lib/openssh/sftp-serverroot 2945 1 0 Jan12 ? 00:00:53 /usr/sbin/sshd -Droot 5099 2945 0 17:45 ? 00:00:00 sshd: user [priv]user 5147 5099 0 17:45 ? 00:00:00 sshd: user@nottyuser 5148 5147 0 17:45 ? 00:00:00 /usr/lib/openssh/sftp-serverroot 5434 2945 0 17:52 ? 00:00:00 sshd: user [priv]user 5482 5434 0 17:52 ? 00:00:00 sshd: user@pts/20user 5506 5483 0 17:52 pts/20 00:00:00 ssh s4.linkeservers.mluser 5511 40663 0 17:52 pts/19 00:00:00 grep --color=auto ssh 另外一个例子，如需求背景部分介绍的，我们使用 nvidia-smi 命令知晓了进程的 PID，那么我们就可以去抓取这个进程更详细的信息： 1ps -aux | grep 22244 # 22244 是改进程的 PID 输出结果： 12345user@02-P100:~$ ps -aux | grep 22244root 22244 115 2.1 32012712 2755640 pts/0 Rl 17:44 14:32 python3 ./pytorch/train.py train --config_path=/TANet/pointpillars_with_TANet/second/configs/pointpillars/ped_cycle/xyres_16.proto --model_dir=/TANet/pointpillars_with_TANet/ped_calib_PSA_CBAM --fusion_weight 1 --image_weight 0 --lidar_weight 0user 22893 0.0 0.0 14680 936 pts/1 S+ 17:57 0:00 grep --color=auto 22244 3. References 《鸟哥的 Linux 私房菜》16.3 节《进程管理》； https://linux.cn/article-4743-1.html https://www.cnblogs.com/freinds/p/8074651.html https://www.cnblogs.com/peida/archive/2012/12/19/2824418.html https://www.jianshu.com/p/cba22cce2f97]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>grep</tag>
        <tag>ps</tag>
        <tag>进程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习记录：输入输出重定向与管道]]></title>
    <url>%2F2020%2F03%2F14%2FLinux%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%9A%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E9%87%8D%E5%AE%9A%E5%90%91%E4%B8%8E%E7%AE%A1%E9%81%93%2F</url>
    <content type="text"><![CDATA[这篇文章来总结下 Linux 中的输入输出重定向功能，以及如何使用管道命令。 1. 背景需求有时候我们使用find 命令查找文件或目录时，会碰到 Permission denied 这样的错误输出信息。但是这些错误信息并不是我们想要的，我们只需要那些符合查询条件的输出，这种情况下该怎么办呢？输入输出重定向可以帮我们将错误信息和正确信息区分开来。 2. 输入输出重定向Linux 中的输入输出分为下面 3 种： 标准输入（standard input）：简称为 stdin，用数字 0 来做标记，一般用 键盘 作为我们的标准输入； 标准输出（standard output）：简称为 stdout，用数字 1 来做标记，一般用 屏幕 作为我们的标准输出； 标准错误输出（standard error output）：简称为 stderr，用数字 2 来标记，一般也用 屏幕 作为标准输出。 举几个例子： 当我们使用 cat 命令创建一个新文件 testfile 时，如下所示，我们的键盘就作为标准输入，将键入的内容输入到文件 testfile 中； 12345$cat testfile&gt; hello&gt; world &gt; // 按下 ctrl+D 结束此次输入$ 当我们使用 cat 命令输出文件内容时，是一种标准输出，文件的内容都将输出到屏幕上； 当我们使用 ll hello_world.c 命令来查看文件信息，但这个文件并不存在时，就会报错，错误信息会输出到屏幕上，是一种标准错误输出。 输入输出重定向的意思，是将原本从键盘获取的输入，重定向由某个文件输入；将原本输出到屏幕的信息，重定向出处到某个文件。 这里需要介绍几种输入输出符号所表示的含义： &lt; : 重定向标准输入，后面跟某个文件名，表示从该文件中获取输入； &gt; : 以覆盖的方式重定向标准输出，后面跟某个文件名，表示将信息输出到该文件中，会覆盖掉文件原有的内容； &gt;&gt; : 以追加的方式重定向标准输出，后面跟某个文件名，表示将信息输出到该文件中，但不会覆盖原有内容，而是追加在文件尾部； 2&gt; : 以覆盖的方式重定向标准错误输出，后面跟某个文件名，表示将错误信息输出到该文件中，会覆盖掉文件原有的内容； 2&gt;&gt; : 以追加的方式重定向标准错误输出，后面跟某个文件名，表示将错误信息输出到该文件中，但不会覆盖原有内容，而是追加在文件尾部。 讲了那么多还是觉得有点抽象，下面来看几个具体的例子。 之前的文章 ssh config 配置文件 中我们介绍了如何添加 isa_key.pub 到 authorized_keys 中，就用到了标准输出重定向，而且是以追加到文件尾部的方式添加的： 1cat isa_key.pub &gt;&gt; .ssh/authorized_keys 另一个例子。 我们使用 find 命令在 /home 目录下查找后缀名是 .c 的文件，我们想将查询到的结果保存到文件 result.txt 中，可以这么做： 1find /home -name "*.c" &gt;&gt; result.txt 这种情况下，屏幕上还是会将 Permission denied 的错误信息打印出来。如果想将这一部分错误信息保存到文件 error.txt 中，可以这么做： 1find /home -name "*.c" &gt;&gt; result.txt 2&gt;&gt; error.txt 那如果进一步，我们想要将正确的和错误的信息都同意保存到文件 result.txt 中呢？可以这么做： 1find /home -name "*.c" &gt;&gt; result.txt 2&gt;&gt;&amp;1 这个特殊语法记住就可以了。 最后，如果我们不想将正确信息和错误信息保存到文件，但又不希望错误信息输出到屏幕中干扰我们阅读，那可以使用 /dev/null。 我们可以将错误的信息都输出到 /dev/null，这个就相当于一个黑洞，输出给它的信息都会被其 “吃掉”： 1find /home -name "*.c" 2&gt; /dev/null 3. 管道命令管道命令的思想和输出重定向很像，即：将原本要输出到屏幕的内容导向到另外一个地方，输出重定向是导向到文件，管道命令是导向到另外一个命令，将上一个命令的输出，做为下一个命令的输入。 管道命令用符号 | 表示（shift + \），常见的形式如下： 1command1 | command2 | command3 即 command1 的标准输出作为 command2 的标准输入，然后 command2 的标准输出作为 command3 的标准输入。 *注：管道命令只能处理标准输出 stdout，并不能处理标准错误输出 stderr；并且，下一个命令必须能接收标准输入 stdin，像 ls、mv、cp 这类命令就不能作为管道命令。 使用 cut 和 grep 命令来演示下如何使用。 3.1 cut 命令本来我们使用 echo ${PATH} 的输出是这样的（每个环境变量之间用 : 间隔）： 1/home/ubuntu/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin 如果我们需要取出 第 4 个 环境变量，可以使用 cut 命令： 1echo $&#123;PATH&#125; | cut -d ':' -f 4 结果： 12ubuntu@VM-0-14-ubuntu:~/.ssh$ echo $&#123;PATH&#125; | cut -d ':' -f 4/usr/sbin cut 命令是一行一行的方式进行扫描的，其有两个重要参数： -d : 分隔符。如上例中用 : 作为一行字符串的分隔符； -f : 表示分隔后的字符串数组中，取第几个。注意这里下标是从 1 开始的。 3.2 grep 命令grep 命令也和 cut 命令一样，一行一行的分析字符串。本质上是一个搜索命令，当找到目标字符子串时，输出一整行。 下面看例子。 输入 last 命令可以查看最近的登录信息，结果如下： 12345678910ubuntu pts/0 223.104.36.169 Wed Mar 25 12:08 still logged inubuntu pts/0 223.104.35.123 Tue Mar 24 20:01 - 22:51 (02:50)ubuntu pts/0 117.136.101.13 Sun Mar 22 08:57 - 09:00 (00:02)ubuntu pts/0 117.136.117.165 Sat Mar 21 21:37 - 23:49 (02:11)ubuntu pts/2 117.136.101.154 Sun Mar 15 21:20 - 23:37 (02:17)ubuntu pts/0 117.136.101.154 Sun Mar 15 20:33 - 22:46 (02:12)ubuntu pts/3 223.104.18.117 Sat Mar 14 14:44 - 14:45 (00:00)ubuntu pts/0 223.104.18.117 Sat Mar 14 11:01 - 13:15 (02:13)...... 我们想找到登录用户是 reboot 的那些行，可以这么做： 1last | grep 'reboot' 结果： 123ubuntu@VM-0-14-ubuntu:~/.ssh$ last | grep 'reboot'reboot system boot 4.15.0-54-generi Fri Mar 6 08:49 still runningreboot system boot 4.15.0-54-generi Fri Mar 6 08:40 - 08:48 (00:08) 如上所述，能找到 2 行记录。如果进一步的，我们只想要用户名的信息（当然只是为了说明，实际肯定没人这么干），可以继续使用 cut 命令： 123ubuntu@VM-0-14-ubuntu:~$ last | grep 'reboot' | cut -d ' ' -f 1rebootreboot 4. References 《鸟哥的 Linux 私房菜》10.5 和 10.6 小节 https://einverne.github.io/post/2017/03/shell-io-redirect.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>标准输入</tag>
        <tag>标准输出</tag>
        <tag>管道</tag>
        <tag>重定向</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习记录：SUID 和 SGID 等特殊权限]]></title>
    <url>%2F2020%2F03%2F08%2FLinux%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%9ASUID-%E5%92%8C-SGID-%E7%AD%89%E7%89%B9%E6%AE%8A%E6%9D%83%E9%99%90%2F</url>
    <content type="text"><![CDATA[1. 引言在上一篇文章 Linux学习记录：chmod 修改权限 中，我们已经介绍了 Linux 中是如何控制文件以及目录的权限的。 这篇文章中我们再来介绍几种特殊的权限，即 SUID 、SGID 和 SBIT。 2. 相关背景从前面的介绍我们已经知道，使用 ll 命令来查看文件或者目录权限时，第一列一共有 10 个字符长度，我们从右往左编号，即 9876543210，其中： 9 共 1 位表示标识位：标识当前的文件类型。如 d 表示目录，- 表示普通文件； 876 共 3 位表示 属主 的权限； 543 共 3 位表示 组用户 的权限； 210 共 3 位表示 其他用户 的权限。 如果一个文件 test.txt 的权限为 -rwxrwxrwx，用二进制表示为 111-111-111，用八进制表示则为 777，所以我们经常会看到这种： 1chmod 777 test.txt 意思就是 属主 、组用户、其他用户 都能 read/write/execute 该文件。 其实 Linux 还有 3 个比特位来表示 SUID 、SGID 和 SBIT 等特殊权限。 3. 具体介绍Linux 表示文件或者目录的权限，总共用了 12 个比特位。其中最低的 9 位，即前面相关背景部分介绍的，用来表示 3 类不同用户的 read/write/execute 权限的。最高的 3 位，从高往低数，分别表示 SUID 权限、SGID 权限和 SBIT 权限。 举个例子： 1211 10 9 8 7 6 5 4 3 2 1 01 0 0 1 1 1 1 0 0 1 0 0 第一行表示二进制串的位置编号，从 0 开始。第二行表示是否具有对应位的权限，0 代表不具有，1 代表具有。 上述例子的意思是：该文件 属主 能够 read/write/execute，组用户 和 其他用户 只能够 read。同时该文件还具有 SUID 权限。 下面就来具体解释下什么是 SUID 、SGID 和 SBIT 权限。 3.1 SUID 权限SUID 表示 Set User IDentification 。只能用来控制：是否让某种文件具有 SUID 权限，不能用来控制目录。 当一个文件 example 被设置了 SUID 权限时，那么除了 属主 之外的所有用户，在执行 example 时，都将获得 属主 的权限。 举个例子。example 一开始的权限是： 1-rwxr-xr-x 我们给 example 设置 SUID 权限后，变成： 1-rwsr-xr-x 注意 属主 权限的 execute 位现在是 s 而不是 x，这是因为设置了 SUID 的原因。 此时，组用户 和 其他用户 在执行 example 程序时，就会获得 write 权限。而在设置 SUID 之前，这两类用户是不具备 write 权限的。 那么我们该怎么来设置这个 SUID 权限呢？还记得我们前面介绍的 12 位二进制位吗？我们只需要将最高位设置为 1，用八进制表示即 4755 。 原来我们是这么设置 example 的权限的： 1chmod 755 example 现在我们这么设置： 1chmod 4755 example 3.2 SGID 权限SGID 的道理和 SUID 相似，只不过 SGID 不仅可以用来控制文件权限，还能控制目录的权限。 还是使用之前的例子。我们的 example 文件一开始的权限是： 1-rwxr-xr-x 当我们给 example 设置 SGID 权限后，变成： 1-rwxr-sr-x 注意 组用户 权限的 execute 位现在是 s 而不是 x，这是因为设置了 SGID 的原因。 此时，组用户(不包括 其他用户) 在执行 example 程序时，就会获得 write 权限。而在设置 SGID 之前，组用户 是不具备 write 权限的。 设置 SGID 权限也很简单，只需要将次高位的二进制设置为 1 即可。所以此时 example 文件的权限值用八进制表示为 2755 。 所以可以这么设置： 1chmod 2755 example 那如果我们想要同时设置 SUID 和 SGID 权限该怎么办呢？想必你也很清楚了，只需要这么做： 1chmod 6755 example 3.2 SBIT 权限SBIT 只能用来控制目录的权限，不能用来控制文件。 如果一个目录被设置了 SBIT 权限，表示任何一个在此目录下可以新建文件的用户，该文件只有该用户自身、或者是 root用户 可以删除，除此之外的用户都无权删除。 举个例子，我们看 /tmp 目录的权限： 1ls -ld /tmp 结果： 12tan@DESKTOP-OPQIR8V:/mnt/c/Users/tanjuntao$ ll -ld /tmpdrwxrwxrwt 1 root root 512 Mar 8 13:52 /tmp/ 注意 其它用户 权限的 execute 位现在是 t 而不是 x，这是因为设置了 SBIT 的原因。 即所有用户都能在 /tmp 目录下新建文件或目录，但只有该用户自身或者 root用户 能够删除这些文件。 4. References https://blog.csdn.net/xiaocainiaoshangxiao/article/details/17378611 https://my.oschina.net/junn/blog/169381 https://www.tecmint.com/how-to-find-files-with-suid-and-sgid-permissions-in-linux/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>SUID</tag>
        <tag>SGID</tag>
        <tag>权限</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习记录：chmod 修改权限]]></title>
    <url>%2F2020%2F03%2F07%2FLinux%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%9Achmod-%E4%BF%AE%E6%94%B9%E6%9D%83%E9%99%90%2F</url>
    <content type="text"><![CDATA[这篇文章记录下 Linux 中如何使用 chmod 命令修改文件以及目录的权限，也就是读/写/执行 的权限。 1. 背景需求有时候我们使用 scp 向服务器传输文件时，会提示说 permission denied 。这是因为我们对于需要上传到的目标目录没有 写 权限。此时需要修改目标目录的读写权限，使其对于所有用户都可写（当然这种做法不推荐），这样我们才能将文件传输上去。 另外一次经历，是之前需要将自己在 windows 中的 rsa keys 传到另外一台 Linux 机器上，这样能实现 rsa keys 的复用，也方便自己在另一台 Linux 机器上能正常向 github 提交代码。 但是，当我们使用 cp 命令将 windows 上的 id_rsa 和 id_rsa.pub 直接拷贝到 ~/.ssh 目录下，然后使用 ssh 时，会提示说 私钥的安全性不够，很容易被访问到。这时候就需要修改私钥文件和公钥文件的权限，以满足要求。 具体需要执行下面两条语句： 12chmod 600 id_rsachmod 644 id_rsa.pub 这两条命令有什么含义，继续往后看。 2. chmod 命令的使用Linux 中我们可以通过 ll 命令来查看文件和目录的权限。这里需要提一下，Linux 中并没有提供 ll 命令，这个命令是通过 alias 实现的。alias 的使用可以参考我上一篇博客：利用 alias 自定义一个删除备份命令 以我自己的服务器 home 目录为例，当我直接输入 ll 命令时，结果： 123456789101112131415161718ubuntu@VM-0-14-ubuntu:~$ lltotal 64drwxr-xr-x 7 ubuntu ubuntu 4096 Mar 7 10:30 ./drwxr-xr-x 3 root root 4096 Aug 8 2018 ../drwxrwxr-x 3 ubuntu ubuntu 4096 Mar 6 16:01 backup/-rw-rw-r-- 1 ubuntu ubuntu 233 Mar 6 16:01 .bash_aliases-rw------- 1 ubuntu ubuntu 3560 Mar 7 10:31 .bash_history-rw-r--r-- 1 ubuntu ubuntu 220 Aug 8 2018 .bash_logout-rw-r--r-- 1 ubuntu ubuntu 3773 May 29 2019 .bashrcdrwx------ 2 ubuntu ubuntu 4096 Aug 9 2018 .cache/drwx------ 3 ubuntu ubuntu 4096 Aug 9 2018 .gnupg/drwxr-xr-x 2 root root 4096 Mar 6 08:29 .pip/-rw-r--r-- 1 ubuntu ubuntu 807 Aug 8 2018 .profile-rw-r--r-- 1 root root 73 Mar 6 08:29 .pydistutils.cfgdrwx------ 2 ubuntu ubuntu 4096 Mar 6 10:39 .ssh/-rw-r--r-- 1 ubuntu ubuntu 0 Aug 9 2018 .sudo_as_admin_successful-rwxrwxrwx 1 ubuntu ubuntu 43 Mar 6 15:27 test_bash.sh*-rw------- 1 ubuntu ubuntu 8010 Mar 7 10:30 .viminfo 我们也可以单独查看某个文件或目录的权限： 1ll test_bash.sh 结果： 12ubuntu@VM-0-14-ubuntu:~$ ll test_bash.sh-rwxrwxrwx 1 ubuntu ubuntu 43 Mar 6 15:27 test_bash.sh* 输出的第一列 -rwxrwxrwx 就表示文件或目录的权限，下面详细解释。 第一位表示文件的类型： - ：表示普通文件； d ：表示目录，即 directory。 后面的 9 位按照 属主 / 组用户(group) / 其它用户 的顺序排列权限，每种用户的权限占据 3 位。 其中： r 表示可 read，用二进制表示为 100，用八进制表示为 4； w 表示可 write，用二进制表示为 010，用八进制表示为 2； x 表示可 execute，用二进制表示为 001，用八进制表示为 1。 如果某个位置为 - 表示该用户不具有此权限。以 drwxr-xr-x 为例： 第一个 rwx 表示 属主 具有 read/write/execute 权限； 第二个 r-x 表示 组用户 具有 read/execute 权限，但是没有 write 权限； 第三个 r-x 表示 其它用户 具有 read/execute 权限，但是没有 write 权限。 drwxr-xr-x 也可以用二进制来表示，即 111-101-101，转换为八进制即 755。755 和 drwxr-xr-x 表示同样的意思。 搞清楚了上面的解释，那么当我们需要将一个文件 test.txt 的权限设置成：属主可以 read/write，组用户和其他用户都只能 read 时（用二进制表示为 110-100-100），我们可以这么做： 1chmod 644 test.txt 最后，让我们再回过头来看背景需求部分介绍的 rsa keys 的权限问题： 12chmod 600 id_rsachmod 644 id_rsa.pub 第一行表示将私钥文件 id_rsa 设置为属主可 read/write，组用户和其他用户都不能 read/write/execute，这符合私钥文件的要求，即只有属主能访问到，属主之外的用户都无法访问； 第二行表示将公钥文件 id_rsa.pub 设置为属主可 read/write，组用户和其他用户都能 read。这也符合要求，即属主之外的用户可以利用属主的公钥来加密。 References https://blog.csdn.net/haydenwang8287/article/details/1753883 https://blog.csdn.net/pythonw/article/details/80263428]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>权限</tag>
        <tag>chmod</tag>
        <tag>777</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用 alias 自定义一个删除备份命令]]></title>
    <url>%2F2020%2F03%2F06%2F%E5%88%A9%E7%94%A8-alias-%E8%87%AA%E5%AE%9A%E4%B9%89%E4%B8%80%E4%B8%AA%E5%88%A0%E9%99%A4%E5%A4%87%E4%BB%BD%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[这篇文章来记录下如何在 linux 中使用 alias 自定义一条删除备份命令。 1. 需求背景平时我们基本上都使用 rm 命令来删除不需要的文件，但是这个命令还是挺危险的，如果因为不小心删除了一些非常重要的文件，那基本上是恢复不会来了，因为我自己前些天就因为按 tab 的时候没注意补全的信息，直接就回车了……删除了一个很重要的代码源文件。 后面开始查找如何在 linux 下面回复删除的文件。尝试了网上介绍的一些方法，发现仍然不行，遂放弃…… 于是就想：linux 中能不能像 windows 下面一样，如果不是强制删除文件，那么这些被删除的文件首先是存放在 回收站 的，除非人为清空回收站，否则这些被“删除”的文件没有被真正删除。 于是开始搜寻资料，最后发现可以使用 alias 来实现这个需求。 2. 具体实现2.1 .bashrc 相关背景先简单介绍下 .bashrc 文件的相关背景。 .bashrc 存放在 home 目录下面。. 开头的文件表示为了防止文件被意外修改，而默认不显示。使用 ls 命令是不显示这个文件的，只有使用 ll 才看得到。 .bashrc 相当于配置文件，bash shell 每次启动时都会首先读取 .bashrc 文件，完成配置更新。 2.2 alias 相关背景这里先解释下，alias 的功能是提供命令简写，举个例子，在 .bashrc 文件中有下面几行代码： 123alias ll='ls -alF'alias la='ls -A'alias l='ls -CF' 这里的 ll 就是命令 ls -alF 的一个别名，起到的效果是一样的。（注意等号 = 两边不能有空格） 如果要查看系统中共有多少条简写命令，可以使用： 1alias -p 结果如下，这是我系统中目前所有的 alias： 123456789ubuntu@VM-0-14-ubuntu:~$ alias -palias alert='notify-send --urgency=low -i "$([ $? = 0 ] &amp;&amp; echo terminal || echo error)" "$(history|tail -n1|sed -e '\''s/^\s*[0-9]\+\s*//;s/[;&amp;|]\s*alert$//'\'')"'alias egrep='egrep --color=auto'alias fgrep='fgrep --color=auto'alias grep='grep --color=auto'alias l='ls -CF'alias la='ls -A'alias ll='ls -alF'alias ls='ls --color=auto' 2.3 将所有的自定义命令放在 .bash_aliases 文件里这里需要解释下为什么文件名为 .bash_aliases。用 vim 打开 .bashrc ，里面有一段这样的代码： 123if [ -f ~/.bash_aliases ]; then # 表示如果在 home 目录下面存在 .bash_aliases 文件 . ~/.bash_aliases # 那么执行这个文件。. 表示执行fi 一个比较推荐的方法是：将所有的自定义命令统一存放在 .bash_aliases 里。当然，如果直接把 alias 语句放在 .bashrc 里也能实现相应功能，只不过这种做法不推荐。 默认情况下，home 目录下是没有 .bash_aliases 文件的，我们需要自己用 touch 新建一个。 我们首先添加一个 ws 命令，方便我们在任何目录下，只需要输入 ws 就能回到 workspace 目录： 1alias ws='cd ~/workspace' 保存退出后 ws 命令还不能马上生效，我们还需要执行（先切换到 home 目录下）： 1source .bashrc 现在我们就能在任意目录下使用 ws 这个命令了。 有了上面的实践之后，我们已经知道 alias 的原理了。接下来我们新建一个 del 命令，是一个 “假删除” 的命令：即并不真正删除文件，而是将文件移动到一个特定位置。 vim 打开 .bash_aliases 文件，在文件尾部添加如下代码（所有的等号两边不能有空格）： 123456789function delete_and_back_file() &#123; date=`date +%Y%m%d` path=`pwd $1` output_path=~/backup/$date mkdir -p $&#123;output_path&#125;/ mv $path/$1 $&#123;output_path&#125;/$1&#125;alias del='delete_and_back_file' 下面解释下上面的代码。 因为我们想要实现的 del 命令其实是一个 移动文件 的命令，这就涉及到 源目录 和 目标目录，我们的 1del ./xxx.txt 等价于： 1mv ./xxx.txt &lt;目标目录&gt; 上述命令中，我们需要给定一个参数，即 xxx.txt ，表示想要删除的文件名。因为每次需要删除的文件都不同，等价于一个变量，我们不好直接使用 alias 来简写，这时候就需要使用函数。 代码第四行：output_path 表示我们想要把文件移动到哪个 目标目录 下，可以自定义； 代码第五行：mkdir -p 表示递归新建目录。举个例子，加入我们需要在 home 目录基础上新建 ~/dir1/dir2 目录： 如果直接使用 mkdir ~/dir1/dir2 是执行不了的； 这时候就需要使用 -p 参数，表示递归建立目录； 即首先新建 dir1 目录，然后在 dir1 下面再新建 dir2 目录。 代码第九行：将这个函数名赋值给我们的自定义命令 del （如果我们将自定义命令命名为 rm，那么会覆盖掉系统中的 rm 命令，这时候如果需要调用系统命令，需要使用 \ 进行转义，即输入 \rm ）。 最后，我们在执行： 1source .bashrc 至此，我们就可以在任意目录下面调用 del 命令了。它不会真正的删除文件，只是将文件移动 .bash_aliases 中指定的目录下。 3. Drawbacks目前的主要缺陷是 del 后只能跟一个文件名，无法一次性删除多个文件，无法使用 -r 删除一个文件夹。 不过这些实现起来应该不难，只需要修改 del_and_back_file 函数即可。等 shell 更加熟练后，可以来实现这个需求。 4. References https://askubuntu.com/questions/1414/how-to-create-a-permanent-alias https://www.jianshu.com/p/16c3e2305bd8 https://blog.csdn.net/chy555chy/article/details/88549924 https://blog.csdn.net/gloriaied/article/details/80669390]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>alias</tag>
        <tag>.bashrc</tag>
        <tag>.bash_aliases</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh config 配置文件]]></title>
    <url>%2F2020%2F03%2F06%2Fssh-config-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[这篇文章讲一下 ssh 中的 config 配置文件。 config 文件一般在 ~/.ssh/ 或者 C://Users//&lt;username&gt;//.ssh 目录下面，没有后缀名。如果在上述目录中找不到 config 文件，那么可以新建一个。 背景需求我们使用 ssh 登录服务器时，一般做法是： 1ssh example@example.com -p xxxx 然后再输入登录密码。 如果需要登录到不同的服务器，并且每个服务器的登录名、地址、端口号、登录密码都不同的话，那么记忆起来就会显得繁琐，那有没有一种办法可以不用输入密码登录，直接输入服务器的一个别名就可以登录的呢？答案是有的。 第一种比较容易想到的方法是建立一些脚本文件，在脚本里面将 ssh 登录需要的各种参数命令都预先写好，然后将该脚本加入到环境变量中，这样方便在任何地方都能调用。具体的实现方法可以 参考这篇 上面方法有一个缺陷就是，每次登录还是需要输入密码。 还有一种更加简单的方式，那就是使用 config 配置文件。 具体实现加入我们需要登录的服务器是 example.com， 登录用户名是 example，登录端口是 2222（ssh 默认登录端口是 22），那么我们可以在 config 文件中加入这些： 1234Host my_server HostName example.com User example Port 2222 几个字段的解释如下： Host：就是你设置的服务器别名，可以是任意自己方便记忆的名字； HostName： 服务器的实际地址，可以是 IP 地址，也可以是域名地址； User ：ssh 登录名； Port：端口号。如果是默认端口号 22，那此字段可以省略。 现在，你可以直接在终端中： 1ssh my_server 来登录服务器了。但是每次登陆还是需要输入密码。 那怎么配置来实现免密登录呢？答案是将本机的公钥存放到服务器上。 将本机中的 id_rsa.pub 公钥文件（windows 系统在 C://Users//&lt;username&gt;//.ssh 目录下，本地终端先切换到此目录下）传到服务器上任意位置，这里传到服务器 home 目录下： 1scp id_rsa.pub my_server:~ # 这里就可以直接使用 my_server 了 接着将 id_rsa.pub 中的公钥复制到服务器上的 authorized_keys 文件中。同样，如果服务器 ~/.ssh 目录下没有此文件，那么新建一个。 可以利用 cat 命令以及 Linux 中的输出重定向 &gt;&gt; 来实现。服务器首先切换到 home 目录下。 1cat id_rsa.pub &gt;&gt; .ssh/authorized_keys 最后还需要将 ssh 服务重启一下： 1/etc/init.d/ssh restart 回车后需要输入服务器的密码完成认证。 至此，就大功告成。现在只需要在本地终端中输入： 1ssh my_server 就能够免密登录到自己的服务器了。 References https://www.cnblogs.com/zhonghuasong/p/7236989.html https://www.cnblogs.com/piperck/p/6188984.html https://blog.csdn.net/ky1in93/article/details/83093981]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>ssh</tag>
        <tag>配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何格式化 64G 以上的 U盘/SD卡为 FAT32 文件系统格式]]></title>
    <url>%2F2020%2F03%2F05%2F%E5%A6%82%E4%BD%95%E6%A0%BC%E5%BC%8F%E5%8C%96-64G-%E4%BB%A5%E4%B8%8A%E7%9A%84-U%E7%9B%98-SD%E5%8D%A1%E4%B8%BA-FAT32-%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[背景需求从网上购买的闪迪的 64G SD卡，准备插入小米摄像机里用来存储历史视频。但是插入之后，显示存储卡的格式不支持，SD卡是 NTFS 文件系统格式，但是小米摄像机需要 FAT32 格式。 遂准备 SD卡读卡器，插入 windows10 系统中，在资源管理器中通过鼠标右键选择“格式化”选项进行格式化。 但是 windows 系统中只能将 32G 以及 32G 容量以下的 U盘/SD卡 格式化为 FAT32 格式，但是对于 64G及以上的，无法完成格式化。于是只能寻求第三方软件。 解决方法发现一款很好的工具：DiskGenius 使用方法很简单，下载安装包，默认安装后，选择需要格式的盘符（选择插入的 SD卡），点击上方选项栏中的【格式化】按钮，选择 FAT32 格式就可以了。 Reference http://www.diskgenius.cn/download.php https://blog.csdn.net/b6_g9/article/details/52076285?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task]]></content>
      <categories>
        <category>杂</category>
      </categories>
      <tags>
        <tag>格式化</tag>
        <tag>diskgenius</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习记录:查看文件和文件夹大小]]></title>
    <url>%2F2020%2F03%2F05%2FLinux%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95-%E6%9F%A5%E7%9C%8B%E6%96%87%E4%BB%B6%E5%92%8C%E6%96%87%E4%BB%B6%E5%A4%B9%E5%A4%A7%E5%B0%8F%2F</url>
    <content type="text"><![CDATA[总结一下在 Linux 下面如何查看 文件 和 文件夹 的大小。为了方便理解和描述，我们的工作目录是 /datapool/workspace/tanjuntao/ ，所有的命令均在这个目录下面执行。 需求 1：如何查看 tanjuntao 下所有目录的大小，不显示子目录的大小命令： 1du -h --max-depth=1 . # . 就表示当前目录，-h 表示 -human-readable，以 M、G 等方式显示大小 结果： 123456789101112131415161718192021222324252627282930313233343536user@DataServer:/datapool/workspace/tanjuntao$ du -h --max-depth=1 .188K ./trash1.5G ./WaveMsNet449M ./FL-ESC105.7G ./FL-SVHN236M ./pytorch-playground1.7G ./ESC-50-master30M ./stat_learn_project648M ./pytorch-classification109M ./vanilla_machine_learning648M ./ml_fl172M ./Federated-Learning-PyTorch5.8G ./py3env211M ./data5.0M ./foolbox1.7G ./adversarial-robustness-toolbox43M ./archive2.4G ./WaveMsNet-master4.2G ./FL-Test66G ./FL-Cifar101.5G ./pytorch-svhn932M ./Environmental-Sound-Classification115M ./python254M ./CBIR384M ./pytorch-cifar1.1G ./FL-Influence189K ./federated-learning427G ./LearnML2.8G ./leaf4.0K ./.vscode8.0K ./test92K ./learn_git2.3G ./EvadeML-Zoo196M ./AdvBox114M ./Defensive-Distillation530G . 注意最后一行，表示当前 tanjuntao 目录的大小是 530G。 如果我们还需要进一步查看 ./FL-Cifar10 目录下那些目录比较大，可以继续使用： 1du -h --max-depth=1 ./FL-Cifar10 需求 2：如何只查看某个特定目录的大小比方说我们想要查看 tanjuntao 这个目录的总大小，我们可以使用需求1 中提到的命令，最后找输出中的最后一行，就可以得到 tanjuntao 目录的总大小。但我们还有更简单的方式。 命令： 1du -h --max-depth=0 . 结果： 12user@DataServer:/datapool/workspace/tanjuntao$ du -h --max-depth=0 .530G 也可以利用 -s 参数： 1du -sh . # -s 表示 -summarize 结果： 12user@DataServer:/datapool/workspace/tanjuntao$ du -sh .530G . 需求 3： 如何查看某个特定文件的大小如果我们需要查看 tanjuntao 目录下 master.zip 文件的大小，可以这么做。 命令： 1du -h ./master.zip 结果： 12user@DataServer:/datapool/workspace/tanjuntao$ du -h ./master.zip616M ./master.zip 除了使用 du 命令，还可以使用 ls 。 命令： 1ls -lh ./master.zip # -h 表示 -human-readable 结果： 12user@DataServer:/datapool/workspace/tanjuntao$ ls -lh ./master.zip-rw-rw-r-- 1 user user 616M Jan 31 20:31 ./master.zip 其中第五列就是文件大小。 需求 4： 如何查看当前目录下所有目录以及子目录的大小如果我们需要查看 tanjuntao 目录下面所有目录的大小，包括子目录，可以这么做。 命令： 1du -h . # 输出结果中只有目录，没有文件 需求 5： 如何查看某个目录下所有文件的大小假如我们需要查看 tanjuntao/data 目录下所有文件的大小（输出结果中没有目录，只有文件），可以这么做。 命令： 1du -ah ./data # 输出结果中只有文件，没有目录 结果： 12345678910111213141516171819user@DataServer:/datapool/workspace/tanjuntao$ du -ah ./data59K ./data/raw/train-labels-idx1-ubyte45M ./data/raw/train-images-idx3-ubyte9.5K ./data/raw/t10k-labels-idx1-ubyte7.5M ./data/raw/t10k-images-idx3-ubyte53M ./data/raw59K ./data/mnist/raw/train-labels-idx1-ubyte45M ./data/mnist/raw/train-images-idx3-ubyte9.5K ./data/mnist/raw/t10k-labels-idx1-ubyte7.5M ./data/mnist/raw/t10k-images-idx3-ubyte53M ./data/mnist/raw7.6M ./data/mnist/processed/test.pt46M ./data/mnist/processed/training.pt53M ./data/mnist/processed106M ./data/mnist7.6M ./data/processed/test.pt46M ./data/processed/training.pt53M ./data/processed211M ./data 需求 6：如何查看整个系统磁盘使用情况命令： 1df -h 结果： 1234567891011user@DataServer:/datapool/workspace/tanjuntao$ df -hFilesystem Size Used Avail Use% Mounted onudev 32G 0 32G 0% /devtmpfs 6.3G 562M 5.7G 9% /run/dev/sda2 438G 40G 376G 10% /tmpfs 32G 0 32G 0% /dev/shmtmpfs 5.0M 0 5.0M 0% /run/locktmpfs 32G 0 32G 0% /sys/fs/cgroup/dev/sda1 511M 3.4M 508M 1% /boot/efinone 1.1P 32T 1.1P 3% /datapooltmpfs 6.3G 0 6.3G 0% /run/user/2001 References https://blog.csdn.net/ouyang_peng/article/details/10414499 https://blog.csdn.net/xiongyangg/article/details/54809810]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>du</tag>
        <tag>df</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 学习记录： wget 和 curl]]></title>
    <url>%2F2020%2F02%2F28%2FLinux-%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%9A-wget-%E5%92%8C-curl%2F</url>
    <content type="text"><![CDATA[之前一直在用 wget 下载文件，最近又使用 curl 来提交 POST 请求，于是在这里将两者的主要用法以及区别总结下。 1. 二者比较 wget 主要用来下载文件，且速度较快，支持断点续传是其很大的优点； curl 也可以用来下载文件，但其功能不仅仅只有这些。curl 还可以用来向 web 提交表单信息。curl 可以理解成一个命令行版本的浏览器，只不过它对返回的 html 页面不做渲染。 2. 主要用法这里我们以下载 B 站的 一张壁纸 为例。 2.1 下载文件12345# wget 后直接跟文件地址wget https://i0.hdslb.com/bfs/album/323911edd18663c49ed1bdb7d3c02dfcac25c4ce.jpg# curl 需要使用参数，注意是大写字母 O，不是数字 0curl -O https://i0.hdslb.com/bfs/album/323911edd18663c49ed1bdb7d3c02dfcac25c4ce.jpg 2.2 下载文件然后重命名12345# wget 后需要跟参数，注意是大写字母 Owget -O wget_bizhi.jpg https://i0.hdslb.com/bfs/album/323911edd18663c49ed1bdb7d3c02dfcac25c4ce.jpg# curl 反过来了，跟的参数是小写字母 ocurl -o curl_bizhi.jpg https://i0.hdslb.com/bfs/album/323911edd18663c49ed1bdb7d3c02dfcac25c4ce.jpg 2.3 断点续传1234wget -c https://i0.hdslb.com/bfs/album/323911edd18663c49ed1bdb7d3c02dfcac25c4ce.jpg# 注意参数顺序是：-,大写字母C，空格，-，空格，-，大写字母Ocurl -C - -O https://i0.hdslb.com/bfs/album/323911edd18663c49ed1bdb7d3c02dfcac25c4ce.jpg 2.4 POST 提交请求curl 还能提交一个 POST 请求，下面是一个向第三方 API 提交请求的例子： 123456curl --location --request POST 'http://134.175.73.113:8080/member/login' \--header 'Content-Type: application/json' \--data-raw '&#123; "account": "15768092082", "password": "123456"&#125;' curl 命令还有很多其他的高级用法，具体可以参考下面这两篇博客： 11 cURL command usage with real-time example 15 Tips on how to use ‘curl’ command in Linux 3. References https://www.cnblogs.com/activecode/p/9446762.html https://blog.csdn.net/IT_hejinrong/article/details/79361095 https://www.zhihu.com/question/19598302 这篇文章中演示了 wget 断点续传]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>wget</tag>
        <tag>curl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo 中如何控制首页/归档页/tag页中显示的文章数]]></title>
    <url>%2F2020%2F02%2F28%2Fhexo-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%8E%A7%E5%88%B6%E9%A6%96%E9%A1%B5-%E5%BD%92%E6%A1%A3%E9%A1%B5-tag%E9%A1%B5%E4%B8%AD%E6%98%BE%E7%A4%BA%E7%9A%84%E6%96%87%E7%AB%A0%E6%95%B0%2F</url>
    <content type="text"><![CDATA[因为此刻是在回忆之前所做的一些配置，所以修改之前的配置，具体值是多少，现在已经记不起来了，但还能记得清做了哪些实际修改。 1. 需求背景hexo 默认情况下，在 archive页 、tag页 是有分页的，也就是一个页面下只能显示有限篇文章，如果需要继续浏览其他的文章，就得进入下一个分页查看，有时候这对我们来说非常不方便，因为我们可能并不需要做分页，只希望一个页面下就能展示所有的文章。但是 hexo 中还没有地方能够做这个配置，需要单独装 插件 才可以。 首页 在默认情况下，也是多少篇文章就会分页，具体是多少已经不记得了，但是 首页 显示的文章数是可以在配置文件中设置的。 下面具体来介绍下该怎么设置每种页面下，显示的文章数。 2. 具体实现2.1 配置文件hexo 中的配置文件分为两种（我们用 ./ 来表示博客系统的根目录）： 系统配置文件：在 ./_config.yml 下，是用来对整个 hexo 博客系统进行配置的，如 博客title 、博客author 等等； 主题配置文件：我装的主题是 NexT，相应的主题文件就在 ./themes/next/_config.yml 下面，是用来对该主题进行配置的，如 导航栏 、页脚样式 等等。 我们的配置全程只需要用到 系统配置文件。 2.2 开始配置我们打开 系统配置文件，搜索关键字 per_page，看到对应的 index_generator 了吗，这一块就是用来设置 首页 文章该怎么显示的。per_page 后面就是 首页 显示的文章数量，这里我设置为 10 （注意英文冒号 : 后面有一个空格）: 1234index_generator: path: '' per_page: 10 order_by: -date 要修改 archive页 和 tag页 显示的文章数量，我们需要安装两个插件，前提是电脑上已经提前装好了 npm，下面命令可以在任意目录下面执行： 12npm install hexo-generator-archive --savenpm install hexo-generator-tag --save 安装完之后，我们需要在刚才的 index_generator 下面添加两个字段，具体如下： 123456archive_generator: # 0表示不分页，全部显示 per_page: 0 tag_generator: per_page: 0 同样，per_page 用于控制页面显示多少篇文章。0 表示不分页，所有文章全部显示。 3. References Hexo博客修改Archive页面显示文章数量 Hexo程序archive页面数量设置]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>配置</tag>
        <tag>index_generator</tag>
        <tag>archive_generator</tag>
        <tag>tag_generator</tag>
        <tag>per_page</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何在 Windows Terminal 中使用指定软件打开文件]]></title>
    <url>%2F2020%2F02%2F27%2F%E5%A6%82%E4%BD%95%E5%9C%A8-Windows-Terminal-%E4%B8%AD%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AE%9A%E8%BD%AF%E4%BB%B6%E6%89%93%E5%BC%80%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[背景我们知道在 Linux 系统下可以使用 vim 或者 vi 等命令，直接打开一个文本文件。但是 windows 下面没有 vim 和 vi，那么如何在终端中直接打开一个 markdown 文本文件呢？因为有时候有些文件目录很深，如果去资源管理器中寻找文件再打开，将会显得很繁琐，如果能在命令行中直接打开，将会提高效率很多。 需求尝试在 windows terminal 中实现类似下面的功能: 1typora xxx.md 可以在命令行中直接打开一个 markdown 文件，但是因为默认安装的 Typora 没有提供这个命令，所以我尝试将 Typora 安装目录中的 typora.exe 文件添加到 Path 环境变量中。 添加完后重启 windows terminal，重新输入上述命令，但是不能打开 xxx.md 文件，只是新建了一个空白的 markdown 文件。 解决方案使用 explorer.exe 。 前提是需要在系统中设置：默认使用 typora 来打开 *.md 的文件。 此时输入： 1explorer.exe xxx.md 就能顺利使用 typora 来打开 xxx.md 文件。 start 命令也能实现相同的效果： 1start xxx.md # 将会使用系统默认程序打开 xxx.md 文件 扩展如果想在 windows terminal 中打开资源管理器，该怎么办呢？ 只需要执行： 1explorer.exe . # '.'表示当前目录 start 命令也能实现相同功能： 1start . # 打开当前文件的资源管理器]]></content>
      <categories>
        <category>杂</category>
      </categories>
      <tags>
        <tag>typora</tag>
        <tag>markdown</tag>
        <tag>terminal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 学习记录: find 命令]]></title>
    <url>%2F2020%2F02%2F27%2FLinux-%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95-find-%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[find 命令算是平时使用频率较高的一个命令了，因为经常需要搜索相关的文件，熟悉它的常见使用方法，对工作是很有帮助的。这篇文章来总结下如何用好 find 命令。 1. 根据文件名查找find 最常见的使用方法就是根据文件名来进行查找了。 查找当前目录 . 下文件名为 hello.txt 的文件： 1find . -name "hello.txt" 查找当前目录 . 下文件后缀名为 .py 的文件（使用通配符 *）： 1find . -name "*.py" 那如果加个否定，查找当前目录下文件后缀名不是 .py 的文件： 1find . ! -name "*.py" 忽略文件大小写，查找当前目录 . 下文件名中包含 test 的普通文件： 123find . -type f -iname "*test*" # 1. 忽略大小写的查找方法是使用参数 -iname，不是用 find -type f -i -name "*test*"# 2. -type f 意思是查找普通文件，后面会解释 2. 根据文件类型查找如果需要查找指定类型的文件，需要跟 -type 参数。 type 参数的主要取值有（不包括全部）： -f : 普通文件； -d : 目录； -l : 连接文件； -s : socket 类型文件。 查找当前目录下以 test 打头的目录： 1find . -type d -name "test*" 3. 根据文件大小查找如果需要利用文件大小来做筛选，可以跟 -size 参数。 -size 参数的单位主要有（不包括全部）： -c : 字节 Byte； -k : 千字节，KB； -M : 兆字节，MB; -G : GB。 此外，还有加号 + 和减号 - 有大小含义： +50M : 表示大于 50M 的文件； -50M : 表示小于 50M 的文件； 50M : 表示大小刚好是 50M 的文件。 查找当前目录下大于 100M 的普通文件： 1find . -type f -size +100M 4. 根据时间查找find 命令后面可跟的时间参数主要有 3 个，单位都是 “天”： -mtime : modified time，表示修改时间； -atime : accessed time，表示访问时间； -ctime : changed time，表示 修改时间。 同 -size 参数一样，时间参数同样可以配合加号 + 和 减号 - 一起使用： +n : 表示 n 天之前的，不包括 n 天本身； -n : 表示 n 天以内的，包含 n 天本身； n : 表示 n 天当天。 举个例子，我们需要找出当前目录下超过 10 天的没有被访问过的普通文件： 1find . -type f -atime +10 同样，找出当前目录下 5 天之内被修改的普通文件： 1find . -type f -mtime -5 5. 根据文件所有者和权限查找需要使用 -user 和 -perm 参数。 例如，找出 / 目录下文件拥有者是 ubuntu 的所有普通文件： 1find / -type f -user ubuntu 找出当前目录下所有权限为 655 的普通文件： 1find . -type f -perm 655 6. 限制查找深度可以使用 -maxdepth 和 -mindepth 两个参数来控制查找的深度，因为默认情况下，find 命令会查找其后所跟的目录及其所有子目录。 如果只查找当前目录本身，不深入其子目录查找，可以设置 -maxdepth 1 : 1find . -maxdepth 1 -name "*.py" 如果需要查找当前目录的子目录中所包含的 .py 文件，可以加上 -mindepth 2 参数： 1find . -mindepth 2 -maxdepth 2 -name "*.py" *注：这两个参数不要同 -type 参数一同使用（尝试后发现不行） 7. 查找完执行额外的动作默认情况下 find 命令查找结束后，会将结果输出到屏幕上。如果我们想要对所有查找到的结果做一些处理，可以使用 -exec 参数。 拿一个例子说明。 1find /usr/bin /usr/sbin -perm 644 -exec ls -l &#123;&#125; \; 有 3 个需要注意的地方： -exec : 跟在 find 命令的最后，表示执行额外的动作； {} : 表示 find命令找到的所有内容； \; : 表示额外动作的结束。 另一个例子：将当前目录下超过 10 天未被访问的文件移动到 ./expired 目录下： 1find . -type f -atime +10 -exec mv &#123;&#125; ./expired \; 最后一个例子，批量修改文件权限：将当前目录下权限为 644 的普通文件，权限修改为 655 : 1find . -type f -perm 644 -exec chmod &#123;&#125; 655 \; References 《鸟哥的 Linux 私房菜》6.5 节《指令与文件的搜寻》 https://www.cnblogs.com/chyingp/p/linux-command-find.html http://einverne.github.io/post/2018/02/find-command.html https://blog.csdn.net/yanlaifan/article/details/52797155 https://blog.csdn.net/wzzfeitian/article/details/40985549]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>find</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matlab里面利用classify函数做多类别线性判别分析]]></title>
    <url>%2F2019%2F12%2F11%2Fmatlab%E9%87%8C%E9%9D%A2%E5%88%A9%E7%94%A8classify%E5%87%BD%E6%95%B0%E5%81%9A%E5%A4%9A%E7%B1%BB%E5%88%AB%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[论文阅读笔记：A survey on Adversarial Machine Learning]]></title>
    <url>%2F2019%2F11%2F24%2F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9AA-survey-on-Adversarial-Machine-Learning%2F</url>
    <content type="text"><![CDATA[这篇简短的综述论文主要介绍了什么是Adversarial Machine Learning，列出了AML的几种常见分类。 IntroductionDefinition of adversarial machine learning machine learning in the presence of an adversary. All kinds of attacks can be categorized in three main types: poisoning attacks: trying to poison the training of test dat detection attacks: with the objective of evading detection model extraction attacks: with the objective of replicating the model 当然，这三种分类方法只是作何自己做的一个分类，实际上还可以有不同的分类方法，需要做的就是理解每一种分分类的划分依据是什么就可以了，不需要刻意去记住有那几种分类。 当然，还有一种分类是将攻击者和防御者建模成一个博弈问题。 It is worth to mention another much different aspect of research in adversarial machine learning which models the model-attacker worlds as a game and defines different moves for each side of the game.By such definition, a min-max algorithm can be used to determine best moves for each side and fide possible equilibriums of the game. Taxonomy这一小节列举出了目前在AML领域中几种常见的攻击方法。 A. Model Extraction Attacks 这种攻击方法一般在machine learning as a service的场景中。即云服务商只会给用户提供一个查询的API接口，但是每次查询是需要收费的。 这时候攻击者就希望能不能找到一个和目标模型表现近似的模型，从而避免向云服务商付费。 这种攻击可以大致分为下面3类： eqution-solving model extraction attacks在逻辑回归等简单模型中适用，原理主要是解方程组。 path finding attacks一般用在决策树模型中。 membership queries attacks判定一个给定的数据点事会否在原始数据集中。 B. Adversarial Examples 这个不需要过多解释了，现在的文章实在是太多了。 需要介绍的是在论文[1]中，作者提出了既能够欺骗人眼也能够欺骗机器的对抗样本。 对抗样本目前最需要解决的问题是防御问题，尤其时针对迁移性攻击的防御。 C. Data Poisoning Attacks 这种攻击适用于攻击者不能直接接触到训练数据，但训练过程是online training or adaptive training，即训练过程中需要不断加入新的训练数据。 这种攻击在SVM中很容易成功，因为svm做分类最主要就是support vectors在起作用。 D. Model Evasion Attacks 这种攻击和对抗样本攻击很像，是在模型做inference阶段的攻击。但是因为做spam detection or malware detection在机器学习中很重要，所以这种攻击就被单独拎出来了。 Conclusion 这篇文章中没有介绍道data inversion attack，即能够推导出原始训练数据近似的攻击。 AML的防御还是一个需要不断深入研究的领域。 参考[1] Elsayed, G.F., Shankar, S., Cheung, B.Papernot, N., Kurakin, A., Goodfellow, I. and Sohl-Dickstein, J., 2018. Adversarial Examples that Fool both Human and Computer Vision. arXiv preprint arXiv:1802.08195]]></content>
  </entry>
  <entry>
    <title><![CDATA[论文阅读笔记：Federated Learning:Challenges, Methods and Future Directions]]></title>
    <url>%2F2019%2F11%2F23%2F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9AFederated-Learning-Challenges-Methods-and-Future-Directions%2F</url>
    <content type="text"><![CDATA[这篇博客简单总结一下这篇2019年8月份的综述文章。 这篇综述论文最主要的贡献在于阐述了当前研究情况下 federated learning 面临的主要问题，以及为了一下可以值得期望的研究方向。 可是就现阶段的研究而言，真的还有很多很多问题需要解决，联邦学习现在就是愿景非常美好，能够利用到不同device上面的额数据做一些训练任务，但是想真正的实现一个物理系统还有很多很多的工作需要做。 目前存在的4个主要问题如下： 通信开销太大 不同的device系统是异质的 不同device上面的数据也是异质的 联邦学习中的安全和隐私问题]]></content>
  </entry>
  <entry>
    <title><![CDATA[下载网络视频]]></title>
    <url>%2F2019%2F11%2F17%2F%E4%B8%8B%E8%BD%BD%E7%BD%91%E7%BB%9C%E8%A7%86%E9%A2%91%2F</url>
    <content type="text"><![CDATA[这篇文章介绍几种下载网页视频的方法。 方法1：第三方下载工具you-getyou-get是一个第三方的Python包，用户只需要安装这个包，然后就可以直接在命令行中下载视频，非常便捷。 step1没有安装Python3的需要先在电脑上安装Python3。接着使用pip3命令安装you-get。 1pip3 install --upgrade you-get # 确保安装最新版本 step2打开命令行，切换到想要保存视频文件的目录中，在浏览器地址栏中拷贝视频的播放地址，接着输入命令 1you-get &lt;视频地址&gt; 这种方式下载默认的视频格式文件，如果想要下载不同格式、不同清晰度的视频文件，可以加上-i参数： 1you-get -i &lt;视频地址&gt; 观察控制台的输出，然后选择自己想要下载的格式文件对应额下载命令，再重新执行一遍you-get命令，此时不用再带上-i命令： 1you-get &lt;每种格式文件对应的下载参数&gt; &lt;视频地址&gt; you-get目前支持很多网站的下载，如youtube bilibili 优酷 腾讯视频 等等。 方法2：直接在网页中找到视频文件原地址这种方式能成功的前提是网站的开发者并没有对视频文件做很好的保护，因为一般的大厂为了防止视频被下载，都会对视频文件的地址做保护。 下面介绍这种简单的下载方式。 在视频对应网页中右键检查或者直接F12打开网页调试页面。 点击调试面板最左上角的箭头，然后将鼠标移到网页中，点击视频文件所在的区域，然后就能在element中找到对应的video标签，src属性中地址就是视频文件的原地址了，一般是.mp4格式的。 在新标签中打开地址，在新标签中直接将视频另存为就能下载了。 方法3：使用插件+下载工具下载M3u8格式视频文件前面说了，一般的网站都会对视频源文件做保护，不能直接找到文件的原地址。常见的做法就是修改video标签的src属性为blob:https://xxxxx，这时候如果在新标签中打开这个地址，什么东西都找不到。 那么这种情况下怎么下载到原视频文件呢？ 这篇文章 中详细分析了blog:http的原理：网页中的视频文件被切分为了一个个小的ts格式的视频片段，这么做的原因，一方面是方便无缝切换视频清晰，另一方面是为了防止视频被下载。 但是尽管找到了ts格式的视频片段，但是也不能一个个的下载下来啊，太繁琐了。 于是有人做了Meu8这种受保护的视频格式文件的下载工具，具体可以follow这篇文章 参考 https://blog.csdn.net/Bumphy/article/details/82865889 https://blog.csdn.net/xingyun89114/article/details/80699527 https://blog.csdn.net/Angry_Mills/article/details/82705595]]></content>
  </entry>
  <entry>
    <title><![CDATA[信息熵]]></title>
    <url>%2F2019%2F11%2F11%2F%E4%BF%A1%E6%81%AF%E7%86%B5%2F</url>
    <content type="text"><![CDATA[这篇文章简单地总结一下信息熵。 概率和编码长度首先假设我们有离散型随机变量 X，X 总共有n个取值，每种取值的概率为p(x)。 假如X有4种可能的取值，每种取值的概率都相同，那么我们需要用2个二进制位来编码每种可能的取值。相应的，如果X有16个取值，那么需要用4个二进制位来编码。 但是如果每种取值发生的概率并不是相同的呢？例如，其中某个取值出现的概率明显要高于其他取值，如果这时候还是用相同的编码长度来编码每种可能的取值，就会造成一种浪费。一个直觉的方法是将概率大的取值分配较短的编码，概率小的取值分配稍微长的编码长度，这样使得整体的编码长度达到最短。 香农提出公式：如果概率为p(x)，那么所需要的编码长度为 那么对于整个概率分布，平均编码长度（每种概率对应的编码长度加权求和）为 上面的公式就是信息熵 。 信息熵的含义从上面的讨论中我们知道了：信息熵其实就等于平均编码长度。 如果熵越大，那么平均编码长度越大，能表示的可能性就越多，不确定性就越大，信息量就越大。（想想，如果平均编码长度为1，那么只能表示两种可能，这时候所能包含的信息就很少。） 对应到随机变量的概率分布上，如果概率分布越分散（极端情况下，均匀分布），那么表明这个随机变量的不确定性越高，平均编码长度越长，信息熵越大，包含的信息量越大。 同物理学中的熵做类比：物理学中，熵越大，表明越无序，对应到信息学中，表明可能性越多，包含的信息量越多。 what to take away 信息熵等于平均编码长度 信息熵越大，包含的信息量越大 均匀分布概率最分散，不确定性最大，信息熵最大。 参考 http://www.ruanyifeng.com/blog/2019/08/information-theory.html 信息熵、交叉熵、相对熵（KL散度）下面来叙述这3种常用熵的概念以及他们之间的联系。 从上面的讨论中我们知道：信息熵（也称为香农熵）对应着一种最优的编码方式，即找不到更短的平均编码长度了。 信息熵的公式实际上是在求1/log(p(x))的数学期望。 假如p(x)对应真实的概率分布，但在实际应用场景中，我们往往不知道真实分布，我们只能得到一个估计的概率分布q(x)，这时候，我们用q(x)来编码p(x)，即将原来每个p(x)对应的编码长度1/log(p(x))替换成1/log(q(x))，然后我们再计算这种编码方式对应的平均编码长度： 这就是交叉熵公式。 重点：当使用q(x)去编码p(x)时，得到的平均编码长度只会大于等于p(x)的香农熵（p(x)的最短编码长度）。等号成立的条件是q(x)完全等于p(x)，如果q(x)和p(x)之间的差别越大，那么得到的编码长度差别也就越大。 从上面的讨论可以知道，使用q(x)编码p(x)，得到的编码长度总是会比最短的编码长度大，那么多出来的这部分，我们定义为相对熵，也就是KL散度。 这里换一种公式来表达信息熵、交叉熵和KL散度……(因为hexo中不支持latex，我只能自己到处找别人写的公式….) 香农熵 交叉熵 相对熵 用一张图来表示这3者之间的关系如下： 上图中，第一行表示香农熵，第二行表示交叉熵，第三行表示KL散度。所谓的KL散度，其实就是交叉熵和香农熵之间的差值，并且永远取非负值。 当交叉熵和香农熵完全相同时（即估计得到的q(x)完全等于p(x))，这时计算得到的相对熵为0. 相对熵有下面几条性质： 如果两个函数f(x)和g(x)完全相同，那么计算两者之间的KL散度为0。两个函数之间的差异越大，那么KL散度越大。（这里f(x)和g(x)分别对应p(x)和q(x)) 相对熵可以用来度量两个概率分部之间的相似度，如果相对熵越小，那么说明两个概率分布越接近。 相对熵公式不对称，不具备交换律。即D(p, q)≠D(q, p)。 参考 https://www.zhihu.com/question/41252833/answer/140950659 https://www.zhihu.com/question/41252833/answer/141598211 （这篇文章后面关于KL散度的公式有错误，前面部分解释的还是很清楚的）https://blog.csdn.net/haolexiao/article/details/70142571]]></content>
  </entry>
  <entry>
    <title><![CDATA[关于贝叶斯公式的几种理解]]></title>
    <url>%2F2019%2F10%2F07%2F%E5%85%B3%E4%BA%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F%E7%9A%84%E5%87%A0%E7%A7%8D%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[论文阅读笔记：Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks]]></title>
    <url>%2F2019%2F07%2F30%2F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9ADistillation-as-a-Defense-to-Adversarial-Perturbations-against-Deep-Neural-Networks%2F</url>
    <content type="text"><![CDATA[总览这篇文章发表在 S&amp;P 2016 上，是对抗样本领域中较早的一种系统性的防御方法，即defensive distillation(防御蒸馏)。因为先前的防御方法大多是经验性的，如对抗训练等等，都是从结果上证明了其防御有效，但没有从原理层次详细的分析防御之所以成功的原因。在本文中，作者观察在两个简单的识别MNIST和CIFAR10的模型上的实验结果，证实了defensive distillation方法的有效性，但这个方法在后来的文章中，也就是S&amp;P 2017 CW attack，被证实是无效的防御。]]></content>
  </entry>
  <entry>
    <title><![CDATA[论文阅读笔记：DeepFool: a simple and accurate method to fool neural networks]]></title>
    <url>%2F2019%2F07%2F27%2F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9ADeepFool-a-simple-and-accurate-method-to-fool-neural-networks%2F</url>
    <content type="text"><![CDATA[寻找对抗样本的过程就是最优化下面的目标函数： 其中k表示分类器，r表示添加的扰动，delta文中表示的含义是分类器在x这个样本点处的robustness，其实就是扰动的L-2 norm。 分类器在整个数据集上的robustness计算公式如下： 从公式中可以看出，如果添加的扰动越小，计算到的ro值就越小，相应的根据文中的定义，分类器的robustness就越好。 本文中的主要贡献： 提出了一种新的计算对抗样本的方法DeepFool 从实验中验证了：新的攻击算法所添加的扰动更小，同时计算对抗样本所消耗的时间更少 实验中说明了：如果使用不恰当的算法（FGSM算法）来验证分类器的鲁棒性，那么很可能会过分的评估分类器的robustness。 攻击算法细节 二元分类 and 线性分类器 这是最简单的一种情况。我们有一个线性分类器，那么可以得到一个超平面将两个类别却分开来。 现在X属于其中一个类别，也就是在直线（或者超平面）的一侧，想要添加扰动使分类器将其分类为另一个类别，那么只需要将X更新到直线（或者超平面）另外一侧就可以了。 现在优化算法的约束条件是最小扰动，那么就只需要找到X到直线（超平面）的最短距离，使X加上这个距离后转换到直线另外一侧，那么就算是找到了对抗样本，同时也就找到了最小的扰动r(投影距离)。 这时候处理起来就很简单了，只需要使用一次距离公式即可： 分类器： 投影距离： 二元分类 and 非线性分类器 现在考虑一种更为普遍的setting。分类器的决策边界是非线性的。 这种情况等价于：寻找X到曲线的最短距离，然后将X加上这个最短距离，即可导致X转移到决策边界的另外一侧，从而被错误分类。 但是直接找这个投影点是不好找的。我们需要用到一种迭代的算法。下面简单描叙一下如何寻找一个点到一个曲线（或者是高维空间中的曲面）的最短距离。 12345678假设当前是第 i 次迭代我们在Xi这点对分类器使用一阶泰勒展开，我们将会得到一条直线（或者一个超平面）将Xi投影到这条直线上，我们找到了下一次迭代的X(i+1)，同时我们将这次迭代添加的扰动向量记为ri接着我们再在X(i+1)这点使用同样的方法，寻找下一次迭代的点X(i+2)，同时计算r(i+1)我们的算法一直迭代，知道找到的X落在分类器的决策边界上，算法终止。输出：将每一次迭代过程中产生的ri（是一个向量）全部累计起来（向量加法），得到最终的扰动r。 注：最后找到的扰动只能保证我们能将X改变到决策边界上，所以我们还需要在r的基础上乘上一个微小的系数，保证能越过决策边界。 多元分类 and 线性分类器 现在考虑多元分类情况下如何产生对抗样本，首先还是从简单的线性分类器开始考虑。 多元分类情形下，W是一个矩阵，每一行对应着其中一个类别的权重，通过W * X + b，我们最后得到的是一个score value，是一个向量，其中每一维代表每一个类别的评分。最后分类的标准就是找评分值最大的那个类别。所以我们寻找对抗样本，只需要找到一个类别k，使这个类别的评分值比X原来类别的评分值高，那么就可以达到对抗样本攻击的目的。 形式化的描述： 现在我们定义另外一个函数 因为每一个F_k(x)都是线性的，所以这个函数也是线性的，所以存在一个超平面。超平面的一侧函数值大于0，另外一侧小于0. 所以我们只需要添加扰动使这个函数值符号改变，那么我们就找到了一种产生对抗样本的方法。 因为假如现在的样本点X0（正确类别是4，这样说明了上面公式被减掉的那一项是F4(x))，带到函数中值小于0，表面当前类别4的评分值大于第k类的评分值。但是如果现在上述函数的函数值变为正号，说明第k类的评分值大于第4类，所以分类器这时候已经错误分类为第k类了，那么添加的扰动就是X0到这个函数的超平面的投影距离。 但是这个距离可能不是最小的，所以我们需要遍历所有类别k，找到最小的那个投影距离作为最后的扰动距离。 寻找这个类别k的公式 最后的最小扰动公式 多元分类 and 非线性分类器 同前面二元分类问题相似，我们使用的仍然是一阶泰勒展开的思路。 前面的公式是F_k(x) - F_4(x) = 0，现在这两个函数都是非线性函数，我们在Xi这点分别使用一阶泰勒展开，可以得到下面公式： 这个公式表示的是被分类器分类为正确类别的空间。我们找投影点的思路和前面二元分类相似，也是一个不断迭代的过程。 123我们在迭代的每一步都会使用一阶泰勒展开，将非线性的函数转为线性函数然后将Xi投影到这个线性函数所决定的超平面上，得到X(i+1)，同时计算投影距离r(i)然后不断这样一个泰勒展开-&gt;投影-&gt;泰勒展开-&gt;投影的过程，知道最后上述公式的函数值符号发生变化 当然，为了找到最小的扰动，我们需要针对所有的类别，都运用上面的算法，最后取扰动最小的那个。 实验结果 从图中可以看出 DeepFool所添加的扰动较[4][FGSM]和[18][L-BFGS]都要小（ro值越小，表明添加的扰动越小，参见ro的定义函数） DeepFool产生对抗样本的时间很短 上图是模型多迭代训练5个epoch，模型的鲁棒性的变化情况，可以看出： 如果使用原来的干净样本，模型鲁棒性基本没有变化 如果使用DeepFool产生的对抗样本（后5个epoch都是在这对抗样本上训练的），那么模型在1个epoch后的鲁棒性就大大提升 使用FGSM产生的对抗样本做训练，模型的鲁棒性反而会下降！！！ 然后作者们对这个奇怪的现象又做了另外一个实验。 他们将Deepfool产生的扰动放大，然后再使模型在这个放大的样本上训练，看训练后模型的鲁棒性怎么变化，结果如下图 如果只是deepfool产生的对抗样本，那么可以增加模型的鲁棒性；但是如果这个扰动太大了，反而会使模型鲁棒性降低，这就说明了FGSM算法产生的扰动很大，不是最小的扰动 然后作者们还做了一个实验，说明使用正确的方法（扰动更小的攻击方法）更能说明模型的鲁棒性，而使用那些扰动大的方法来评估模型的鲁棒性，往往会过分评估 作者首先是使用FGSM产生对抗样本，然后在这个样本上多训练了5个epoch。接着使用Deepfool在新的模型（更加的鲁棒）上产生对抗样本，计算ro的值，如蓝线所示；同时使用FGSM再重新生成对抗样本，同样也计算ro值，如红色点线所示。可以看到，后者明显过分估计了模型的鲁棒性，实际上行模型并没有其估计的那么鲁棒。]]></content>
  </entry>
  <entry>
    <title><![CDATA[论文阅读笔记：Explaining and Harnessing Adversarial Examples]]></title>
    <url>%2F2019%2F07%2F15%2F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9AExplaining-and-Harnessing-Adversarial-Examples%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[论文阅读笔记：Intriguing Properties of Neural Network]]></title>
    <url>%2F2019%2F07%2F10%2F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9AIntriguing-Properties-of-Neural-Network%2F</url>
    <content type="text"><![CDATA[对抗样本（adversarial examples）的开山之作。 文中主要提到了神经网络两个比较奇怪的性质： 神经网络中携带语义信息的不是某单个神经元，而是整个网络所表示的空间。 给样本添加一些轻微的扰动，会导致神经网络模型错误分类，这些样本就称为对抗样本。 这里对第一个性质做简单的说明。 首先，作者们通过观察某个隐层中某个特定神经元对什么样的输入图片会产生最大的激活值，以此来确定这个神经元对什么样的特征是最敏感的，从而确定这个神经元包含什么样的语义信息。这个想法是可行的并且前面（2014年之前）已经有这方面的研究了。实验结果就发现：单个神经元确实能够表示某种语义信息。接着作者们又做了另外一个实验：选择隐层所表示的空间中的一个随机方向（就是基向量的随机线性组合，是隐层中单个神经元所无法表示的方向），然后用同样的方法使其激活值最大。实验结果发现，同单个神经元一样，这个随机的方向也能表示某种确切的语义信息，从而说明了这个随机方向也可以表示某种特征。 所以最后得到的结论就是：神经网络某个隐藏层中携带语义信息的并不是单个神经元，而是这个隐层所表示的整个空间。 文中提出来的几个假设： it is assumend that is possible for the output unit to assign non-significant(and, presumaby, non-epsilon)probabilities to regions of the input space that contain no training examples in their vicinity. the adversarial examples represent low-probability(high-dimensional)”pockets” in the manifold, which are hard to efficiently find by simply randomly sampling the input around a given example. 那么作者们是如何找到对抗样本的呢？ 寻找对抗样本的过程，其实是一个不断优化的过程：一方面我们希望添加的扰动尽可能小，因为如果是较大的扰动，任何state-of-the-art的模型都会错误分类，就算是人眼也可能识别不了，这和对抗样本问题的研究初衷不一样，我们想知道的是为什么神经网络在面对这些人眼不可分辨的图片时，会给出巨大的错误结果。另一方面，我们希望对抗样本能被错误分类到另一个其它类别，从损失函数的角度看，就是想让其越大越好，于是作者们给出了下面的优化目标函数： 其中x是原始图片，r是添加的扰动，f是分类器，l是目标类别（同x正确的类别不同）。我们希望r越小越好，同时使对抗样本x + r被错误分类到一个指定类别l下，同时还需要生成的x + r的值在[0,1]之间（保证是一张合法的图片）。 直接解这个问题不容易，因此作者转换了一种思路，从损失函数的角度找最优的r: 简单看看损失函数：一方面我们需要r的某种范式越小越好，另一方面我们希望loss(x+r, l)越小越好，因为这样表明x+r分类成类别l的概率越大。所以我们的目标是最小化上述公式，用到的方法是box-constrained L-BFGS. 最后，作者们通过实验发现了几个重要的现象： 对于论文中提到的所有网络结构（non-convolutional network, AlexNet, QuocNet），都能用上述方法生成对抗样本； 对抗样本具有跨模型的泛化能力：在A模型上产生的对抗样本，有很大一部分在B模型（和A模型结构相同，超参数不同）上也有效（也能是B模型错误分类） 对抗样本具有跨数据集的泛化能力：在D1数据集训练得到的模型上产生的对抗样本，在D2数据集训练得到的模型上也有效，D1和D2属于不同的子集，两个模型是结构完全不同的模型。]]></content>
  </entry>
  <entry>
    <title><![CDATA[统计学习方法：感知机]]></title>
    <url>%2F2019%2F07%2F09%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%9A%E6%84%9F%E7%9F%A5%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[注：代码和运行结果都是从jupyter notebook中导出的 导入所需要的第三方库12345import pandas as pdimport numpy as npfrom sklearn.datasets import load_irisimport matplotlib.pyplot as plt%matplotlib inline 导入数据（iris数据集是sklearn中自带的数据集）123456# load datairis = load_iris()df = pd.DataFrame(iris.data, columns=iris.feature_names)df['label'] = iris.targetdf.columns = ['sepal length', 'sepal width', 'petal length', 'petal width', 'label']df.label.value_counts() 2 50 1 50 0 50 Name: label, dtype: int64 选取两个特征sepal length和sepal width，绘制散点图12345plt.scatter(df[:50]['sepal length'], df[:50]['sepal width'], label='0')plt.scatter(df[50:100]['sepal length'], df[50:100]['sepal width'], label='1')plt.xlabel('sepal length')plt.ylabel('sepal width')plt.legend() &lt;matplotlib.legend.Legend at 0x1b17e936160&gt; 数据处理：将pandas类型数据转换为np.array123data = np.array(df.iloc[:100, [0, 1, -1]])X, y = data[:, :-1], data[:, -1]y = np.array([1 if i == 1. else -1 for i in y]) 定义模型12345678910111213141516171819class Model(): def __init__(self): self.w = np.ones(len(data[0])-1, dtype=np.float32) self.b = 0 self.l_rate = 0.1 def linear_func(self, w, b, x): return np.dot(x, w) + b def fit(self, X_train, y_train): finished = False while not finished: wrong_count = 0 for d in range(len(X_train)): if y_train[d] * self.linear_func(self.w, self.b, X_train[d]) &lt;= 0: self.w = self.w + self.l_rate * y_train[d] * X_train[d] self.b = self.b + self.l_rate * y_train[d] wrong_count += 1 if wrong_count == 0: finished = True return "perceptron model" 1234perceptron = Model()perceptron.fit(X, y)print(perceptron.w)print(perceptron.b) [ 7.8 -10. ] -12.099999999999973 使用自己训练的模型做二分类1234567x_points = np.linspace(4, 7, 10)y_ = -(perceptron.w[0] * x_points + perceptron.b) / perceptron.w[1]plt.plot(x_points, y_)plt.plot(data[:50, 0], data[:50, 1], 'bo', color='blue', label='0')plt.plot(data[50:100, 0], data[50:100, 1], 'bo', color='orange', label='1')plt.legend() &lt;matplotlib.legend.Legend at 0x1b17ecb4d68&gt; 使用sklearn中自带的感知机模型123456from sklearn.linear_model import Perceptronclf = Perceptron(fit_intercept=False, max_iter=1000, shuffle=False)clf.fit(X, y)print(clf.coef_)print(clf.intercept_) [[ 74.6 -127.2]] [0.] c:\python36\lib\site-packages\sklearn\linear_model\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning. FutureWarning) 绘制sklearn训练得到的感知机模型1234567x_points = np.arange(4, 8)y_ = -(clf.coef_[0][0] * x_points + clf.intercept_) / clf.coef_[0][1]plt.plot(x_points, y_)plt.plot(data[:50, 0], data[:50, 1], 'bo', color='blue', label='0')plt.plot(data[50:100, 0], data[50:100, 1], 'bo', color='orange', label='1')plt.legend() &lt;matplotlib.legend.Legend at 0x1b17f046278&gt; C:\Python36\Lib\site-packages\sklearn\datasets\data]]></content>
  </entry>
  <entry>
    <title><![CDATA[python中如何批量修改文件后缀名]]></title>
    <url>%2F2019%2F07%2F05%2Fpython%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%B9%E9%87%8F%E4%BF%AE%E6%94%B9%E6%96%87%E4%BB%B6%E5%90%8E%E7%BC%80%E5%90%8D%2F</url>
    <content type="text"><![CDATA[需求背景win10中的锁屏壁纸着实好看，想着能不能在本机中找到壁纸存放的目录，于是网上搜了一下，还真有。 壁纸文件存放的目录是1C:\Users\18856\AppData\Local\Packages\MicrosoftWindowsContentDeliveryManager_cw5n1h2txyewy\LocalState\Assets 上面18856是我电脑上的账户名，只需要修改为你自己的账户名即可，其它的所有分级目录都相同。 切换到噶目录下后，发现所有文件都没有后缀名，不能直接用图片查看器打开，需要一个个修改后缀名为.jpg格式，但是手工是在是太麻烦，因此想着用Python批量修改。 实现用os.path里面几个常用的API可以实现需求。 代码123456789101112131415161718192021222324import osimport syspath_input = sys.argv[1]ext_input = sys.argv[2]components = path_input.split('\\')path_new = ''# 将原始单斜杠'\'转为双斜杠'\\'for idx, item in enumerate(components): if idx != (len(components) - 1): path_new += item + '\\\\' else: path_new += item# print(path_new)files = os.listdir(path_new)for file in files: # 分隔文件名和后缀名 fullname = os.path.splitext(file) newname = fullname[0] + ext_input os.rename(path_new + '\\\\' + file, path_new + '\\\\' + newname) 运行从命令行中运行，需要附带两个参数： 原文件所在的目录，将windows中文件资源管理器上的路径拷贝过来； 目标后缀名，如.jpg 运行示例12python batch_rename.py C:\Users\18856\AppData\Local\Packages\Microsoft.Windows.ContentDeliveryManager_cw5n1h2txyewy\LocalState\Assets .jpg 参考文章 https://www.runoob.com/python/att-string-split.html https://www.cnblogs.com/wuxie1989/p/5623435.html https://blog.csdn.net/weixin_40449300/article/details/83184928]]></content>
  </entry>
  <entry>
    <title><![CDATA[安装jupyter notebook和修改字体]]></title>
    <url>%2F2019%2F07%2F02%2F%E5%AE%89%E8%A3%85jupyter-notebook%E5%92%8C%E4%BF%AE%E6%94%B9%E5%AD%97%E4%BD%93%2F</url>
    <content type="text"><![CDATA[安装 jupyter notebookjupyter notebook 有两种方式安装：一种是通过 Anaconda，Anaconda里面已经集成了jupyter notebook；另一种是通过pip install安装，本文中我采用的是第二种安装方法。 我Windows系统中只安装了Python3，因此控制台中输入python就指向Python3，如果系统中Python2和Python3共存，那么在控制台中可能需要输入python3，因为官方建议在Python3下面安装jupyter notebook。 在控制台中输入下面命令安装jupyter notebook:12python -m pip install --upgrade pippython -m pip install jupyter 安装需要花一定时间，等成功之后，在控制台输入:1jupyter notebook 启动jupyter notebook。然后选择一个目录，点击new就可以新建Python文件了。 配置 jupyter notebook 主题和字体这里首先需要安装一个第三方包jupyter-themes:1pip install jupyterthemes github仓库中有每个参数（包括主题、字体、字体大小等）的详细介绍和如何配置，如果有需要可以耐心研究研究。 下面这个配置可以比较好的满足需求。1jt -t oceans16 -f fira -fs 13 -cellw 90% -ofs 11 -dfs 11 -T 直接在控制台中输入上述命令，接着重新启动jupyter notebook就可以更新设置了。 最后的效果是这样的： 参考文章 https://blog.csdn.net/qq_30565883/article/details/79444750 https://jupyter.org/install.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[如何删除github仓库中某个文件或者文件夹]]></title>
    <url>%2F2019%2F06%2F04%2F%E5%A6%82%E4%BD%95%E5%88%A0%E9%99%A4github%E4%BB%93%E5%BA%93%E4%B8%AD%E6%9F%90%E4%B8%AA%E6%96%87%E4%BB%B6%E6%88%96%E8%80%85%E6%96%87%E4%BB%B6%E5%A4%B9%2F</url>
    <content type="text"><![CDATA[从远处clone仓库到本地在本地任意找一个文件夹，鼠标右键打开git bash，将远程仓库clone到此文件夹下面。1git clone &lt;仓库的git地址&gt; 进入到仓库目录中1cd &lt;仓库名&gt; 删除文件夹如果文件夹名称是单独的英文单词，不带有空格，那么输入：1git rm -r &lt;文件夹名&gt; 如果文件夹名称是中文名或者带有多个英文单词但是使用空格分隔，那么输入1git rm -r &apos;&lt;文件夹名&gt;&apos; # 同前面的区别是多了单引号，可以使用tab补全文件夹名称 删除文件如果只是删除某个特定文件的话：1git rm &lt;文件名全程&gt; # 包含文件名+后缀 如果需要批量删除某类后缀名的文件1git rm *.&lt;后缀名&gt; # e.g. git rm *.md 将修改提交到远程仓库123git add *git commit -m &quot;输入提示信息&quot;git push origin master 搞定！]]></content>
  </entry>
  <entry>
    <title><![CDATA[HelloWorld of Keras]]></title>
    <url>%2F2019%2F05%2F17%2FHelloWorld-of-Keras%2F</url>
    <content type="text"><![CDATA[mnist_cnn1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# -*- coding: utf8 -*-# "Hello World" of Keras in image classification taskfrom __future__ import print_function # 在Python2的代码中可以使用Python3的print函数，即需要加上括号import kerasfrom keras.datasets import mnistfrom keras.models import Sequential # 序列模型，可以封装各种网络层: 卷积层、池化层、全连接层、dropout等from keras.layers import Dense, Dropout, Flattenfrom keras.layers import Conv2D, MaxPooling2Dfrom keras import backend as K # Keras是一个高层API，后端使用TensorFlow或者Theanobatch_size = 128num_classes = 10epochs = 12 # 12个epoch足够训练好模型img_rows, img_cols = 28, 28 # mnist dataset中image的大小# 如果第一次跑，会通过url方式从amazon下载mnist.npz；如果已经下载过了，那么直接加载本地mnist.npz(x_train, y_train), (x_test, y_test) = mnist.load_data()# 根据不同的后端，将图片转换为不同的格式，TensorFlow是channel_lastif K.image_data_format() == 'channels_first': x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols) # 通道为1 x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols) input_shape = (1, img_rows, img_cols)else: x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1) x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1) input_shape = (img_rows, img_cols, 1)# 将每个pixel的类型从integer转为float32，使除法计算更加精确x_train = x_train.astype('float32')x_test = x_test.astype('float32')# 将0-255范围的pixel值转化到0-1x_train /= 255x_test /= 255# 如果后端是TensorFlowprint('x_train shape:', x_train.shape) # 输出(60000, 28, 28, 1)print(x_train.shape[0], 'train samples') # 输出60000print(x_test.shape[0], 'test samples') # 输出10000# 每个y_train或者y_test本来是一个整数表示每一类，现转换为one-hot encodingy_train = keras.utils.to_categorical(y_train, num_classes)y_test = keras.utils.to_categorical(y_test, num_classes)# 实例化一个序列模型model = Sequential()# Conv1model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))# Conv2model.add(Conv2D(64, (3, 3), activation='relu'))# Pool1model.add(MaxPooling2D(pool_size=(2, 2)))model.add(Dropout(0.5))# 将maxpooling后的64个activation map 全部拉伸为一个向量model.add(Flatten())model.add(Dense(128, activation='relu'))model.add(Dropout(0.5))# 用softmax函数将logits转为每个类别的概率model.add(Dense(10, activation='softmax'))# 设置损失函数，优化器，度量标准model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])# 训练模型# verbose=1 展示详细的训练过程# verbose=0 只显示最后结果，不展示过程# verbose=2 每个epoch输出一条信息，不展示详细的训练过程# validation_data会在每个每个epoch结束后计算当前模型的准确率，validation_data不参与训练model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2, validation_data=(x_test, y_test))# 在测试集上测试模型效果score = model.evaluate(x_test, y_test, verbose=0)print('Test loss:', score[0])print('Test Accuracy:', score[1]) 训练过程123456789101112131415161718192021222324252627282930313233x_train shape: (60000, 28, 28, 1)60000 train samples10000 test samplesTrain on 60000 samples, validate on 10000 samplesEpoch 1/12 - 248s - loss: 0.2778 - acc: 0.9140 - val_loss: 0.0612 - val_acc: 0.9800Epoch 2/12 - 219s - loss: 0.0982 - acc: 0.9708 - val_loss: 0.0433 - val_acc: 0.9857Epoch 3/12 - 212s - loss: 0.0766 - acc: 0.9776 - val_loss: 0.0354 - val_acc: 0.9876Epoch 4/12 - 226s - loss: 0.0653 - acc: 0.9806 - val_loss: 0.0342 - val_acc: 0.9879Epoch 5/12 - 221s - loss: 0.0573 - acc: 0.9828 - val_loss: 0.0307 - val_acc: 0.9896Epoch 6/12 - 230s - loss: 0.0539 - acc: 0.9834 - val_loss: 0.0286 - val_acc: 0.9907Epoch 7/12 - 224s - loss: 0.0490 - acc: 0.9850 - val_loss: 0.0280 - val_acc: 0.9901Epoch 8/12 - 229s - loss: 0.0475 - acc: 0.9856 - val_loss: 0.0300 - val_acc: 0.9904Epoch 9/12 - 249s - loss: 0.0465 - acc: 0.9862 - val_loss: 0.0286 - val_acc: 0.9906Epoch 10/12 - 257s - loss: 0.0423 - acc: 0.9866 - val_loss: 0.0261 - val_acc: 0.9910Epoch 11/12 - 243s - loss: 0.0393 - acc: 0.9883 - val_loss: 0.0313 - val_acc: 0.9903Epoch 12/12 - 277s - loss: 0.0413 - acc: 0.9877 - val_loss: 0.0266 - val_acc: 0.9907Test loss: 0.026646837043244158Test Accuracy: 0.9907Process finished with exit code 0 如果是第一次运行，会从 https://s3.amazonaws.com/img-datasets/mnist.npz 下载数据集，Windows会存放在c://Users//&lt;User name&gt;//.keras/datasets下面；Linux会存放在home/.keras/datasets下面。 cifar10_cnn.py12 训练过程 参考 Keras Model API: https://keras.io/models/model/ mnist_cnn.py: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py cifar10_cnn.py: https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py https://www.cnblogs.com/lfri/p/10485597.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[李学龙老师在2017届本科生毕业典礼上的讲话]]></title>
    <url>%2F2019%2F05%2F04%2F%E6%9D%8E%E5%AD%A6%E9%BE%99%E8%80%81%E5%B8%88%E5%9C%A82017%E5%B1%8A%E6%9C%AC%E7%A7%91%E7%94%9F%E6%AF%95%E4%B8%9A%E5%85%B8%E7%A4%BC%E4%B8%8A%E7%9A%84%E8%AE%B2%E8%AF%9D%2F</url>
    <content type="text"><![CDATA[演讲原文尊敬的各位老师、各位家长，各位师弟、师妹们： 大家早上好！ 我想汇报和分享的很多故事其实都是“曾经”，但是我今天更想用“这一年”来说。曾经有一首歌叫《同桌的你》，曾经有一部电影叫《大话西游》，曾经有一本书叫《未来之路》，曾经在这一年我到了合肥。大约坐了将近40个小时的火车，其实完全可以加上一个注释：怀抱着光荣、梦想、期望、憧憬，但其实对我来说，当时旅途就是旅途本身，至多是一种坚持，至少当时我是这么认为的。在这一年，在北方长大的我，第一次见到了竹子，它顽强地在并不富饶的土地上茁壮成长；在这一年很多来自南方的同学，他们第一次见到了水在极端情况下的迸发和绽放。1994年的漫天大雪里，我现在还能记得很多穿着单衣的同学在欢呼雀跃。 这一年，我18岁之前建立的所有自信心都被打碎了，荡然无存，如樯橹灰飞烟灭，因为在我曾经自诩很擅长的每一个方面都有N多的人比我强。当我跟他们一样穿着拖鞋、端着饭盆去食堂的时候，总会有人说“看！这是国奥金牌，这也是国奥金牌，这是某个省的状元，这又是某个省的状元。”其实给人压力最大的并不是你知道有人比你优秀，而是你每天都要和他们在一起；也不是说你觉得比你优秀的人比你更努力，而是你发现其实他们不太努力的时候还是比你优秀。在这一年，很幸运的，我跟他们一样有了一个让我终生引以为傲、倍加珍惜的名字——科大人！ 科大人很拼，江湖传言“穷清华，富北大，不要命的上科大”。但科大就是科大，科大有自己的秉持、自己的传统、自己的文化、自己的理念，我们其实没有必要去跟别人去比。科大人很萌，像“天使路”、“勤奋路”这样的名字，如果不是情商和智商的终极极品爆发大概是想不到的。科大人很执着，1995年毕建忠（9304/9315校友）和我分别任校学生会东西区体育部长的时候，我们（重新）启动了“巾帼杯”女子足球赛，听说一直踢了二十几年，到现在还有。科大人很团结，这个不需要多说，等大家到新的学习和工作岗位上，大家自然会知道，我保证。其实科大人还有很多共同的禀赋和标签，刚才蒋书记和校长也都提到了，像“红专并进，理实交融”其实已经融进了科大人的血脉，已经刻入了科大人立德树人的骨髓。 我是早上三点半的航班到的合肥，我当时在想跟大家讲点什么呢？其实我想了讲“知道，不知道，知不道，道不知”的故事，或者给大家讲“知足，不知足，知不足”的故事。后来想了想，还是不讲这些了。因为我的资历和经历也比较有限，很多故事也是听别人讲的。所以在毕业典礼的时候，我想给大家讲两个我会对毕业生说的我的担心吧。 首先，我担心大家会以为社会上和大学里是一样的。其实我们看一下大学，从中国古代的太学到西方古典模式的大学到中西方现代模式的大学，大学始终是这么一个存在，它鼓励我们去弘扬我们的品德、砥砺我们的品格、开拓我们的视野、激发我们的潜力，让我们不断地完善和更新，达到一种更好的自我，趋于完美和完善的境地。虽然大学的入门门槛在不断地调整，但大学始终是一个象牙塔，它是社会少数精英阶层的消费。但是当我们迈向社会的时候，我们会发现，盖人生历程，大抵逆境居十之六七，顺境亦居十之三四，而顺逆两境又常相间，我们会碰到很多的困难。为什么呢？因为社会对年轻人的宽容和忍让、理解和支持同大学是不一样的。“物可瞬间无主，人须时时有心”，我们能保证不变初心，在我们的追求和志向不被外物所改变的情况下，我建议大家学会适应。达尔文在《物种起源》里说，得以生存的不见得是最强大的，也不见得是最智慧的，但一定是最适合环境的。唯一不变的事情就是变化本身，所以我送给大家的第一个词是“适应”，并请大家记住，适应绝不是随波逐流。 第二个担心是，我担心大家认为课本里学到的知识可以让大家出去生活，至少是生存了，因为生活和生存是两个概念。其实课本里的知识如果是完备的话，我们没有必要去建立一所实体大学。事实上课本里的知识、大家学过的知识很可能是不完备的，不齐整的，甚至可能是不正确的。以光学为例，我有两个透镜，有一个光轴，这边有个蜡烛，这边有个屏，蜡烛成什么样的像，这个我们可以在课本中学到，但是你在做一个实际系统的时候，这两个透镜能严格地同轴吗？或许不能！这个时候该怎么解决，课本里似乎不会告诉你。那么学电的话，我们学电子管、晶体管，并不表示我们要去用它，我们是要知道历史上发生过什么样的事情，我们应该怎么样去学习，我们要学习“学习的本领”，我们去归纳总结，去提炼升华。其实，科学研究本身至少有两个目的。第一个目的是实际的应用，去解决实际的问题。我们理工科的学生，尤其应该如此，绝对不能纸上谈兵。那么另一个目的就是满足人类对世界的好奇心。我们知道在电口（电学）上很著名的故事。当法拉第发现电磁感应定律的时候，他并不知道能做什么；麦克斯韦预言电磁波存在的时候，法拉第刚刚去世；当赫兹证明电磁波存在的时候，麦克斯韦又刚刚去世；当马可尼开始进行无线通讯的时候，赫兹也去世了。但是我们不能忘记科学史上这一步一步好奇心的驱使。其实能够鼓励创新的年代并不是特别的多，我们常说“千里马常有，而伯乐不常有”。我们看一看在欧洲，从后希腊到罗马到文艺复兴，其实创新沉睡了一千年。在中国我们有卷帙浩繁的经史子集，我们几乎可以从中间找到现在的任何一个发现或发明的原始的雏形，但是我们必须要承认，我们的技术是多于科学的，我们没有能够把炼丹术发展成化学，没有能够把观星术发展成天文学。所以在今天，在全国、全社会鼓励创新驱动发展的时候，我想大家还是非常幸运的。所以我想送给大家第二个词是“好奇”，而且好奇绝不是随心所欲。 我个人并不是特别推崇“人情练达即文章”，但是我坚信“世事洞明皆学问”。我想在大学的五年里（现在是四年制），大家至少学会了一个人的时候怎么样去平静、去思考，两个人的时候怎么样去合作、去争论，三个人的时候如何去协调、去平衡。其实我们担心的事情还很多，我们担心大家有成绩的时候妄自尊大，担心大家碰到困难的时候妄自菲薄。正因为大家很优秀，所以我们的担心才很多。 15年前的毕业典礼，我是博士生的毕业代表，到现在我还记得当时我的发言，我说“春来几度，苍翠依旧，于无声处，却已见茁壮。而今我们将成为科大的历史，但母校永远是我们心中的一座丰碑，上面镌刻着老师的关爱、同学的友情，和我们的点点滴滴……”很快大家就要离开这个校园，可能还会离开这个城市，或许很多人可能还会短期或长期离开我们的祖国，但是我深信大家或早或晚一定会回来！我是毕业6年之后回到合肥，我没有去会场，我直接打车到了黄山路的一个小饭店，进去之后，老板一句不经意的问候“很久没见了”，让我“泪飞顿作倾盆雨”，刚才在我们的校旗传过来的过程中，我心里就是这样的感觉。 我想大家一定会记得这一年，虽然这一年以后也会成为曾经，但是大家一定会记得在这一年大家收获了母校、老师、师兄师姐们的一些不可辜负的爱和期望。如果每一位科大人都是一颗闪亮的星，那么我们希望以一颗颗星的光芒去引领整个星河璀璨、瀚海星云的梦想。谢谢大家！ 演讲视频 李学龙(946)校友在2017届本科生毕业典礼上的讲话(节选)]]></content>
      <tags>
        <tag>李学龙</tag>
        <tag>演讲</tag>
        <tag>科大</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[佛祖.py]]></title>
    <url>%2F2019%2F05%2F04%2F%E4%BD%9B%E7%A5%96-py%2F</url>
    <content type="text"><![CDATA[皮一下哈哈… 代码123456789101112131415161718192021222324252627282930print(" _ooOoo_ ")print(" o8888888o ")print(" 88 . 88 ")print(" (| -_- |) ")print(" O\\ = /O ")print(" ____/`---'\\____ ")print(" . ' \\| |// `. ")print(" / \\||| : |||// \\ ")print(" / _||||| -:- |||||- \\ ")print(" | | \\\\\\ - /// | | ")print(" | \\_| ''\\---/'' | | ")print(" \\ .-\\__ `-` ___/-. / ")print(" ___`. .' /--.--\\ `. . __ ")print(" ."" '&lt; `.___\\_&lt;|&gt;_/___.' &gt;'"". ")print(" | | : `- \\`.;`\\ _ /`;.`/ - ` : | | ")print(" \\ \\ `-. \\_ __\\ /__ _/ .-` / / ")print(" ======`-.____`-.___\\_____/___.-`____.-'====== ")print(" `=---=' ")print(" ")print(" ............................................. ")print(" 佛祖镇楼 BUG辟易 ")print(" 佛曰: ")print(" 写字楼里写字间，写字间里程序员； ")print(" 程序人员写程序，又拿程序换酒钱。 ")print(" 酒醒只在网上坐，酒醉还来网下眠； ")print(" 酒醉酒醒日复日，网上网下年复年。 ")print(" 但愿老死电脑间，不愿鞠躬老板前； ")print(" 奔驰宝马贵者趣，公交自行程序员。 ")print(" 别人笑我忒疯癫，我笑自己命太贱； ")print(" 不见满街漂亮妹，哪个归得程序员？") 打印效果：]]></content>
  </entry>
  <entry>
    <title><![CDATA[写出pythonic的代码]]></title>
    <url>%2F2019%2F05%2F04%2F%E5%86%99%E5%87%BApythonic%E7%9A%84%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[列表遍历Pythonic1234names = ['juntao', 'xiaojun', 'mingming', 'haohao']# 查找名字列表中带有字母'm'的名字m_name = [name for name in names if 'm' in name]print(m_name) 不要用循环写！12345m_name = []for name in names: if 'm' in name: m_name.append(name)print(m_name) 字典遍历Pythonic1234exam_detail = &#123;'juntao': 44, 'xiaojun': 88, 'mingming': 99, 'haohao': 22&#125;# 找出及格的学生及其分数score_greater_than_60 = &#123;k: v for k, v in exam_detail.items() if v &gt;= 60&#125;print(score_greater_than_60) 不要这样写！12345score_greater_than_60 = &#123;&#125;for k, v in exam_detail.items(): if v &gt;= 60: score_greater_than_60[k] = vprint(score_greater_than_60) 参考文章 一些 pythonic 的代码实例：https://github.com/mikeckennedy/write-pythonic-code-demos https://blog.csdn.net/g8433373/article/details/80709116]]></content>
  </entry>
  <entry>
    <title><![CDATA[一些Python语法]]></title>
    <url>%2F2019%2F05%2F01%2F%E4%B8%80%E4%BA%9BPython%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[关键字 globalglobal关键字主要目的是为了在函数中使用全局变量。1234567891011121314count = 10def modify_count(): temp_count = 20 print('local variable: ', temp_count) global count print('global variable: ', count) count = 100 print('after modified: ', count) returnprint('before modify_count(): ', count)modify_count()print('after modify_count(): ', count) 输入结果：1234567before modify_count(): 10local variable: 20global variable: 10after modified: 100after modify_count(): 100Process finished with exit code 0 从代码中可以看到，，如果要在函数modify_count()中使用全局变量count，那么需要使用global关键字修饰。同时，函数内部对count的修改，将影响到函数外面count的值。 if __name__ == &#39;__main__&#39; 说明因为在 Python 中，一个.py文件就是一个模块，模块中一般写了很多函数供其他模块使用。但我们需要测试模块中函数的正确性，这时候就需要用到 if __name__ == &#39;__main__&#39;. __name__是每个模块的一个属性名，如果我们需要在当前模块中编写测试代码，一般情况下，我们是将测试代码放在 if __name__ == &#39;__main__&#39;这条判断语句下面，因为如果是直接运行当前模块，那么__name__的值就是__main__，条件判断下面的测试代码将会被执行。 如果有另外一个模块module_2.py引用了当前模块module_1.py,那么当我们运行module_2.py时，module_1.py中的__name__的值将变成module_2，即引用了module_1这个模块的模块名。那这时module_1中的if判断将不满足，也就不会执行测试代码，module_2也就只是导入了module_1中的函数，符合我们的期望。 np.random.seed()说明seed(int) 接收一个整数做为生成随机数的算法的参数。如果接收到的整数值相同，那么产生的随机数也相同。 代码123456import numpy as npnum = 0while num &lt; 5: np.random.seed(5) print(np.random.random()) num += 1 输出结果123450.221993171089739480.221993171089739480.221993171089739480.221993171089739480.22199317108973948 但是设置的seed()的值只生效一次，下次使用random()函数如果不是上次的seed()值，那么将会产生不同的随机数。 代码123456import numpy as npnum = 0np.random.seed(5)while num &lt; 5: print(np.random.random()) num += 1 输出结果123450.221993171089739480.87073230617737640.206719155339426420.91861090793792160.48841118879482914 从结果可以看到，第一次输出的random()值，由于和上面代码一样设置了seed(5)，所以第一行结果相同；但是后面4次循环由于没有继续设置seed()，因而产生的随机数不同。]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pycharm在需要输入命令行参数时如何调试]]></title>
    <url>%2F2019%2F04%2F30%2Fpycharm%E5%9C%A8%E9%9C%80%E8%A6%81%E8%BE%93%E5%85%A5%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0%E6%97%B6%E5%A6%82%E4%BD%95%E8%B0%83%E8%AF%95%2F</url>
    <content type="text"><![CDATA[1. 前言在 pycharm 中进行调试，一般的做法是：先设置断点，然后开始调试程序。调试过程中选择step over, step into, step out, resume program等操作。 但是当程序需要从命令行中读取参数时，该怎么调试呢？ 如果和之前一样，设置断点，然后开始调试，但在这种做法下，程序没有从命令行中获取到参数；如果在 pycharm 的 teminal中输入 python xxx.py -x xx -y yy -z zz，这种做法会直接运行完程序，无法在断点处停下来。 2. 解决方法在 pycharm 中选择 Run -&gt; edit configurations，然后在parameters中填入需要设置的命令行参数，这时候不需要输入python xxx.py。点击apply然后OK。 然后在代码中就可以直接 debug 了，会在断点处停下来。 参考文章 https://blog.csdn.net/wishchin/article/details/78560725]]></content>
  </entry>
  <entry>
    <title><![CDATA[pycharm调试]]></title>
    <url>%2F2019%2F04%2F29%2Fpycharm%E8%B0%83%E8%AF%95%2F</url>
    <content type="text"><![CDATA[pycharm中调试功能按钮说明列表 竖排 Resume Program(F9): 运行到下一个断点。 横排 step into(F7)和step into my code(Shift+Alt+F7之间的区别：前者会进入到一些系统函数内容，后者只会进入自己定义的函数内部。 参考文章 pycharm调试功能]]></content>
  </entry>
  <entry>
    <title><![CDATA[ubuntu下使用virtualenv新建一个python3虚拟环境]]></title>
    <url>%2F2019%2F04%2F28%2Fubuntu%E4%B8%8B%E4%BD%BF%E7%94%A8virtualenv%E6%96%B0%E5%BB%BA%E4%B8%80%E4%B8%AApython3%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[1. 虚拟环境的意义因为不同的应用需要的环境可能是不同的，比方说A和B程序都需要用到第三方库C，但是A只能在C(version=1.0)下面运行，B只能在C(version=2.0)下面运行，这时要想同时运行A和B程序就会存在问题。 虚拟环境提供的好处就是，可以将不同应用程序所需要的环境隔离开来，每个程序有一套属于自己专属的环境，程序之间不会相互干扰。 2. 步骤 首先需要安装 virtualenv 1sudo apt-get install python-virtualenv 接着创建 python3 的虚拟环境(ubuntu中默认安装了python2和python3) 1234cd ~/1997tanjuntao/ # 切换到一个目录下面virtualenv -p /usr/bin/python3 py3env # 在当前目录下面创建新目录py3env(目录名可以自定义)，py3env中就是虚拟环境# 如果是新建一个python2环境，可以这样做# virtualenv -p /usr/bin/python2 py2env 接着激活环境 切换到py3env所在目录1cd ~/1997tanjuntao/ source命令激活虚拟环境1source py3env/bin/activate 这时候会发现命令行前面多了一个括号 (py3env) 这个新建的虚拟环境中使用的是python3，默认情况下，没有包含系统python3中安装的包，很干净。 这时候使用pip install安装的第三方包，只存在于这个虚拟环境中，不会影响系统中的python3 退出虚拟环境1deactivate # 一句话 参考文章 https://blog.csdn.net/qingche456/article/details/65465760]]></content>
  </entry>
  <entry>
    <title><![CDATA[解决hexo中发布文章后图片无法显示的问题]]></title>
    <url>%2F2019%2F04%2F28%2F%E8%A7%A3%E5%86%B3hexo%E4%B8%AD%E5%8F%91%E5%B8%83%E6%96%87%E7%AB%A0%E5%90%8E%E5%9B%BE%E7%89%87%E6%97%A0%E6%B3%95%E6%98%BE%E7%A4%BA%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1. 问题描述默认情况下，在 hexo 中新建一篇markdown博文，在文章中插入本地图片，再部署到 github 上面，是无法显示这些图片的。 2. 解决方法首先找到博客根目录下面的_config.yml文件，修改post_assrt_folder的值为true. 接着 git bash 切换到博客根目录下面，安装第三方插件hexo-assrt-image1npm install hexo-asset-image --save 接着hexo new &quot;文章名&quot; 新建一篇文章，会发现source/_posts/目录下面会生成一个和 markdown 文件同名的文件夹。再将当前这篇文章中需要插入的图片放到这个文件夹中，再到 markdown 中引用该文件夹中的图片，最后hexo g -&gt; hexo d，就可以正常的查看这些图片了。 参考文章 https://blog.csdn.net/qq_38148394/article/details/79997971 https://www.jianshu.com/p/3db6a61d3782]]></content>
  </entry>
  <entry>
    <title><![CDATA[解决pycharm在同目录下import，pycharm会报错，但实际可以正常运行的错误]]></title>
    <url>%2F2019%2F04%2F28%2F%E8%A7%A3%E5%86%B3pycharm%E5%9C%A8%E5%90%8C%E7%9B%AE%E5%BD%95%E4%B8%8Bimport%EF%BC%8Cpycharm%E4%BC%9A%E6%8A%A5%E9%94%99%EF%BC%8C%E4%BD%86%E5%AE%9E%E9%99%85%E5%8F%AF%E4%BB%A5%E6%AD%A3%E5%B8%B8%E8%BF%90%E8%A1%8C%E7%9A%84%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[1. 问题描述假如我们的目录结构是这样： 项目名称是HelloWorld，在项目主目录下面新建一个文件夹test_dir，接着在HelloWorld/test_dir目录下面新建两个文件: HelloWorld/test_dir/hello1.py和HelloWorld/test_dir/hello2.py。 hello1.py中插入如下代码：1234import hello2 as h2print(h2.hello_world())print(h2.nihao()) hello2.py中插入如下代码：12345def nihao(): return 'nihao'def hello_world(): return 'hello world' 此时，会发现在hello1.py中一直红线提示No module named hello2： 但是如果我们运行hello.py发现是能够正常输出结果的。 2. 解决方法在左侧工程目录树中，选中test_dir，右键，然后选择 Mark directory as，再选择source root，然后就会发现先前的红线错误提示已经没有了。 3. More…如果我们是在当前工程主目录下面新建两个文件HelloWorld/hello1.py和HelloWorld/hello2.py，两个.py文件的代码内容和之前一样，这时候却又不会出现No module named hello2这种错误了。 参考文章 https://www.zhihu.com/question/52880389/answer/134369870]]></content>
  </entry>
  <entry>
    <title><![CDATA[论论文笔记: Manipulating Machine Learning: Poisoning Attacks and Countermeasures for Regression Learning]]></title>
    <url>%2F2019%2F04%2F18%2F%E8%AE%BA%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Manipulating-Machine-Learning-Poisoning-Attacks-and-Countermeasures-for-Regression-Learning%2F</url>
    <content type="text"><![CDATA[这篇文章发表于 2018 年，是第一篇针对线性回归模型提出投毒攻击方法的论文，实验证明：这种gradient-based的攻击方法效果很好；同时这篇文章里面也提出了一种防御方法：TRIM，实验结果也证明其防御效果很好。 主要贡献 第一篇针对linear regression进行poisoning attack的论文，之前的论文都是针对classification的模型 改造了一个原本针对分类问题进行投毒攻击的模型，将其应用在回归问题中，将攻击结果作为一个baseline 提出了一种基于数据集统计特征的攻击方法，只需要掌握有限的信息即可完成攻击 提出了防御算法TRIM,能够抵御众多的攻击，效果比传统的robust statistic好 方法Linear Regression Model传统的线性回归模型：$$\mathcal{L}\left(\mathcal{D}{\mathrm{tr}}, \boldsymbol{\theta}\right)=\underbrace{\frac{1}{n} \sum{i=1}^{n}\left(f\left(\boldsymbol{x}{i}, \boldsymbol{\theta}\right)-y{i}\right)^{2}}{\operatorname{MSE}\left(\mathcal{D}{\text { tr }}, \boldsymbol{\theta}\right)}+\lambda \Omega(\boldsymbol{w})$$ 根据$\Omega(w)$的不同，linear regression 可以分为下面4种 类型 $\Omega(w)$ Ordinary Least Square(OLS) $\Omega(w) = 0$ Ridge $\Omega(\boldsymbol{w})=\frac{1}{2}\ \boldsymbol{w}\ _{2}^{2}$ LASSO $\Omega(\boldsymbol{w}) = \ {w}_1\ $ Elastic-Net Regression $\Omega(w) = \rho\ w_1\ + (1-\rho)\frac{1}{2}\ w_2^2\ $ Adversarial Model这里定义敌手模型。 敌手模型主要是用来对潜在用户进行分析建模，例如：假设攻击者掌握了多少知识、攻击者的目标是什么、攻击者的攻击策略是什么等。具体的可以从下面4个维度来对潜在的攻击者进行建模： 维度 描述 Adversary Goal 一般包含两种:availability attack &amp; integrity attack.前者主要目的是使整个模型的可用性降低，后者主要是使模型在针对特定样本做 inference 时，准确率降低 Adversary Knowledge 一般包含两种：white-box &amp; black-box. 对于前者，假设攻击者知晓：traning data &amp; learning algorithm &amp; loss function &amp; model parameters；对于后者，假设攻击者只知晓：learning algorithm &amp; loss function。但是这篇文章中，作者通过产生替代数据，同样可以得到 model parameters. Adversary Capability 攻击者的能力，一般定义为其能往数据集中插入多少 poisoning samples. $n:original \ samples$, $p :poisoning \ samples$, $N = n + p:\ total \ smaples$, 定义poisoning rate $\alpha = p/n$ Adversary Strategy $\begin{array}{rl}{\arg \max {\mathcal{D}{p}}} &amp; {\mathcal{W}\left(\mathcal{D}^{\prime}, \boldsymbol{\theta}{p}^{\star}\right)} \ {\text { s.t. }} &amp; {\boldsymbol{\theta}{p}^{\star} \in \arg \min {\boldsymbol{\theta}} \mathcal{L}\left(\mathcal{D}{\mathrm{tr}} \cup \mathcal{D}_{p}, \boldsymbol{\theta}\right)}\end{array}$ 攻击者的目标主要是优化上面的目标函数：即如何插入 poisoning samples 来使得在 $D’$上的 loss 值最大. 攻击方法攻击者的目标函数：$$\begin{array}{rl}{\arg \max {\mathcal{D}{p}}} &amp; {\mathcal{W}\left(\mathcal{D}^{\prime}, \boldsymbol{\theta}{p}^{\star}\right)} \ {\text { s.t. }} &amp; {\boldsymbol{\theta}{p}^{\star} \in \arg \min {\boldsymbol{\theta}} \mathcal{L}\left(\mathcal{D}{\mathrm{tr}} \cup \mathcal{D}_{p}, \boldsymbol{\theta}\right)}\end{array}$$ 注： 目标函数 $W$一般用 validation set 上面的 loss 来表示。loss 越大，表名攻击效果越好。对于 linear regression, loss function = Mean Square Error(MSE) 那么该插入什么样的 poisoning sample 才能使目标函数 $W$ 的值最大呢？ 答案是：考虑梯度，考虑$W$对$D_p$的梯度。有了这个梯度，我们就能使用gradient ascent方法来更新$D_p$，直到最后$W$收敛，我们可以得到最佳的$D_p$. 但是这个优化问题是一个Bi-level问题，即约束条件本身也是一个优化问题，解决这种问题通常情况下是NP-hard.因为$x_c$和$W$并不是显示相关，而是通过$\theta$间接相关，因此作者通过chain rule来计算所需要的梯度 $\nabla_{\boldsymbol{x}_{c}} \mathcal{W}$: $$\nabla_{\boldsymbol{x}{c}} \mathcal{W}=\nabla{\boldsymbol{x}{c}} \boldsymbol{\theta}\left(\boldsymbol{x}{c}\right)^{\top} \cdot \nabla_{\boldsymbol{\theta}} \mathcal{W}$$ 注：$x_c$表示 poisoning sample 的 features 1. 首先求解公式的第二项 $\nabla_{\theta} \mathcal{W}$ 第二项比较好计算，因为在线性回归问题中，目标函数表示的是在 validation set 上计算到的 loss： $$\mathcal{W}{\mathrm{val}}\left(\mathcal{D}{\mathrm{val}}, \boldsymbol{\theta}\right)=\frac{1}{m} \sum_{j=1}^{m}\left(f\left(\boldsymbol{x}{j}^{\prime}, \boldsymbol{\theta}\right)-y{j}^{\prime}\right)^{2}$$ 对这个函数求对$\theta$的偏导：$$\nabla_{\boldsymbol{\theta}} \mathcal{W}{\mathrm{val}}=\left[ \begin{array}{c}{\nabla{\boldsymbol{w}} \mathcal{W}{\mathrm{val}}} \ {\nabla{b} \mathcal{W}{\mathrm{val}}}\end{array}\right]=\left[ \begin{array}{c}{\frac{2}{m} \sum{j=1}^{m}\left(f\left(\boldsymbol{x}{j}\right)-y{j}\right) \boldsymbol{x}{j}} \ {\frac{2}{m} \sum{j=1}^{m}\left(f\left(\boldsymbol{x}{j}\right)-y{j}\right)}\end{array}\right]$$ 至此，公式第二项偏导数计算完毕，下面计算第一项。 2. 求解公式第二项 $\nabla_{\boldsymbol{x}{c}} \boldsymbol{\theta}\left(\boldsymbol{x}{c}\right)^{\top}$ 防御方法实验结果]]></content>
  </entry>
  <entry>
    <title><![CDATA[SSH公钥方式登录原理以及对称加密和非对称加密知识点总结]]></title>
    <url>%2F2019%2F04%2F10%2FSSH%E5%85%AC%E9%92%A5%E6%96%B9%E5%BC%8F%E7%99%BB%E5%BD%95%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86%E5%92%8C%E9%9D%9E%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[先占坑，后面需要学习这块的内容，用博客记录下来，加深理解！ 参考文章 https://blog.csdn.net/csm201314/article/details/78453579]]></content>
  </entry>
  <entry>
    <title><![CDATA[Linux 系统中常见目录的含义及作用]]></title>
    <url>%2F2019%2F04%2F02%2FLinux-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%B8%B8%E8%A7%81%E7%9B%AE%E5%BD%95%E7%9A%84%E5%90%AB%E4%B9%89%E5%8F%8A%E4%BD%9C%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言这篇博客将随着我对 Linux 系统的不断使用和了解而不定期更新。 常用目录的含义 目录 含义 /bin 存放 linux 中常用命令，如cd pwd ls等 /lib /etc /var 参考文章 http://blog.sina.com.cn/s/blog_684d52a90102uw30.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[安装固态硬盘]]></title>
    <url>%2F2019%2F04%2F02%2F%E5%AE%89%E8%A3%85%E5%9B%BA%E6%80%81%E7%A1%AC%E7%9B%98%2F</url>
    <content type="text"><![CDATA[0.前言以下全是个人的装机经验，没有很认真的写，只是打算做个记录，这样下次如果还需要干这类活时有个参考。不建议follow！！仅供一点参考。 笔记本老早就出现各种问题了，觉得应该是系统的问题而不是硬件的问题，于是寻思着重装一下系统。但是心里对重装系统有些抵触，因为会导致很多之前辛苦安装好配置好的软件无法使用。拖了好久，终于买了一块固态硬盘来重装系统…笔记本简直焕发第二春啊！ 1. 数据备份 为了避免发生意外情况，建议还是首先把原先硬盘中重要的文件备份一下。所谓备份，就是把一些重要的文件拷贝到另外一块存储介质中，如移动硬盘等，这样重装系统能够放心点。 其实需要备份的文件感觉不是很多，个人觉得下面这些需要着重备份： C盘：桌面，图片，视频。偷懒一点，就把user文件夹全部备份一下，因为这个差不多是系统盘中最重要的文件了。 其它：如果是数据盘，那不用考虑，所有的文件都得备份。如果是软件盘，那么之前一些工程文件目录，如eclipse，webstorm，pycharm等，做好备份。另外Tim，微信聊天记录也最好备份一下，这样重装之后可以直接导入这些记录。 2. 制作启动U盘因为打算把新系统装到 SSD 中，所以需要事先制作一个启动 U 盘，用来安装新系统。 下载系统iso镜像 可以使用微软官方的渠道下载，直接搜索windows10镜像下载 微软，然后下载一个工具，让其将Windows操作提供写入到U盘中(U盘会被格式化，事先备份U盘) 也可以使用第三方的镜像，比较推荐的是 MSDN我告诉你。找到打算下载的镜像，将完整的ed2k地址复制到迅雷等下载工具中，启动镜像下载。 制作启动U盘 可以使用UltraISO rufus等启动U盘制作工具，导入之前下载的iso镜像，就可以轻松完成U盘制作。 3. 拆机拆机过程中不用太担心，小心操作，一般不会带来什么问题的。拆机之前记得先把笔记本电源取下来，防止意外发生。 将笔记本后盖中所有看得见的螺丝全部拧下来，然后将笔记本后盖和主板分离。这个过程中需要留意一点，因为主板上面有可能有线和后盖通过一个卡口连在一块，所以在取下后盖之前，需要将这个卡口解开，不能直接就分离后盖。 找到硬盘位。拧下所有的螺丝，将原来的 HDD 取下，装上新的 SSD，按照相反的步骤再装回原位，这样新的固态硬盘就轻松的装好了。 找到光驱位。拧下所有的螺丝，将原来的 HDD 装入光驱位硬盘托架(需花钱额外购买)，记得一定要将 HDD 装到位，也就是一定要保证接线口是完全缝合的，否则可能在装好新系统后，无法读取到这块 HDD 硬盘。 将原来光驱的前挡板取下来，装到这块硬盘托架上，这样能够保证托架装上笔记本后，不会明显的凹下去，因为如果使用硬盘托架厂商送的通用挡板，会导致笔记本装好后没有一体性，因为光驱位那里凹下去了，影响美观。 将后盖装回去(有线的地方先把线接好)，再将所有的螺丝全部拧回去，这样就完成了拆机安装 SSD 的过程。 4. 重装系统到 SSD目标是将新系统装到 SSD 中，原来的 HDD 作为普通的硬盘使用。 具体的，笔记本开机后，不断按f2 f8 delete(不同笔记本不同)进入BIOS。找到boot，选择从U盘启动。 这时候会识别到刚才新装的固态硬盘，会提示是否需要进行分区，因为SSD一般是128G或者256G，感觉没有必要再分区了，整块盘作为一个分区就可以了。然后接下来就是愉悦的安装过程了，这个比较傻瓜，没什么好说的。 5. 可能的问题 新系统中无法使用触控板。可以卸载当前的触控板驱动，然后去笔记本厂商官网，找到当前这款型号笔记本对应的触控板驱动程序，重新安装就可以了。 读取不到光驱位硬盘。可以搜索一下这个问题，一般会提示让下载一个amd xxx controller，然后打开设备管理器，将驱动程序手动更新一下就可以了。 原先的光驱挡板不好拆卸。实在不行就暴力拆卸，装到托架上时，用502胶水粘一下。]]></content>
  </entry>
  <entry>
    <title><![CDATA[常用的正则匹配]]></title>
    <url>%2F2019%2F04%2F02%2F%E5%B8%B8%E7%94%A8%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%B9%E9%85%8D%2F</url>
    <content type="text"><![CDATA[正则表达式由两个部分组成：1/正则表达式主体/修饰符(可选) 其中修饰符主要用到两种： i: ignore 的意思。表示忽略大小写进行匹配。 g: global 的意思。表示匹配整个字符串，否则第一次匹配成功后，不会继续匹配后面的字符串。 常用的特殊字符 特殊字符 含义 * 0个或者 N 个 + 1个或者 N 个 ? 0个或者1个 .等价[^\n\r] 匹配任意字符(除了换行和回车) \ 可以做转义字符也可以和\b等一起使用表示特殊含义 ^ 匹配输入的开始如/^A/匹配一个以字符A开头的字符串 $ 匹配输入的结束如/t$/匹配一个以字符t结尾的字符串 [0-9] 匹配一个数字 \d 同上匹配一个数字 \s 空白字符(空格、制表符、换行符等) \D等价[^0-9] 匹配一个非数字字符 [a-zA-Z] 匹配任何一个大小写字母 \w 匹配一个单字字符(字符、数字、下划线) \W 同上匹配一个非单字字符 [^xyz] 匹配不包含xyz字符的第一个字符 使用正则表达式的常用方法 方法 描述 replace() 查找匹配的字符串，然后用新字符串替代 split() 将原字符串按照正则表达式拆分，将结果存入到一个数组中 使用括号的子字符串匹配 这种情况下，会将匹配的结果存入到一个数组中，也就是会 记忆 匹配得到的结果。 几个例子 eg1 1234var re = /\w+\s/g;var str = "fee fi fo fum";var myarr = str.match(re);console.log(myarr); 输出结果1["fee+空格", "fi+空格", "fo+空格"] eg2(综合性实例) 1234567891011121314151617181920212223// names中不同名字之间有多个空白字符、制表符等不可见字符var names = "Harry Trump ;Fred Barney; Helen Rigby ; Bill Abel ; Chris Hand ";console.log("===========original string==============\n" + names);var pattern = /\s*;\s*/;var namelist = names.split(pattern);// console.log(namelist);// 将匹配到的结果存起来作为$1和$2var newpattern = /(\w+)\s+(\w+)/;newnamelist = [];for (var i = 0, len = namelist.length; i &lt; len; i++) &#123; // 名字颠倒 newnamelist[i] = namelist[i].replace(newpattern, "$2, $1");&#125;console.log(newnamelist);// 按照字母表排序newnamelist.sort();console.log(newnamelist); 需要注意的就是函数split()和replace()的使用。 如果是带有括号的，那么匹配的结果将存放在$1 $2 … $n中]]></content>
  </entry>
  <entry>
    <title><![CDATA[Office 2016 激活方法]]></title>
    <url>%2F2019%2F04%2F02%2FOffice-2016-%E6%BF%80%E6%B4%BB%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[激活方法 下载安装激活工具：链接: https://pan.baidu.com/s/1C3KqB4m7dnm9t2uKoDARHw 提取码: 9r4h Windows 10 中可能会提示说下载的文件中包含病毒，导致下载不成功。这时候只需要在Windows defender中找到刚才那条阻止记录，并将其设置为允许，重新下载文件，就可以下载成功了。 接下来执行压缩包里面的.exe文件，安装到最后，Windows defender又提示说检测到病毒xxx不能执行，安装失败之类的，这时候需要在windows安全中心，选择管理设置，然后关闭实时保护，然后以管理员方式运行压缩包里面的.bat文件，再重新安装刚才那个.exe文件，这时候就可以成功安装了，安装完之后再开启实时保护。 启动软件，点击红色按钮，在打开 office 中的账户，发现已经激活成功了，简直神器！ 参考文章 https://blog.csdn.net/weixin_40941966/article/details/80872533]]></content>
  </entry>
  <entry>
    <title><![CDATA[Python3: ModuleNotFoundError: No module named 'PIL']]></title>
    <url>%2F2019%2F04%2F02%2FPython3-ModuleNotFoundError-No-module-named-PIL%2F</url>
    <content type="text"><![CDATA[问题使用matplotlib.pyplot模块调用绘图函数时，出现以上错误。 解决方法在控制台中输入如下命令1pip install pillow 即可解决问题]]></content>
  </entry>
  <entry>
    <title><![CDATA[使用 Github Pages + Hexo 搭建一个个人博客]]></title>
    <url>%2F2019%2F04%2F02%2F%E4%BD%BF%E7%94%A8-Github-Pages-Hexo-%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[前言其实在很早之前，曾使用 WordPress 搭建过一个个人博客。当时也是满满的激情，想模仿大佬们学着写技术博客。但是后来由于忙着追求GPA，加上自身懒惰自控力不行，博客就基本放弃更新了。 但是现在已经过上了研究生生活，每天有大把的时间可以由自己来支配，同时接触的东西也越来越多：做项目需要实时的学习一些技术和技巧，需要不断总结下来；各种学术报告会，听完之后需要回去再认真揣摩消化，变成自己的东西，这也需要总结下来。虽然之前也零零散散的做了一些笔记，但是组织松散不方便查找，遂考虑重新开通个人博客，将平常生活中的所学所思都记录下来，做技术和科研的积淀，争取能够厚积薄发！ 开始搭建 1. 前期准备 安装node.js：因为hexo是一个基于node.js开发的静态博客框架，其运行必须要有node.js环境支持 安装git：要往 github 提交代码，就必须先在本地安装好 git 客户端 在 github 上面新建一个仓库，名称设为你的github账户名.github.io 上面软件的安装应该没有大问题，去官网下载一路 next 基本就可以了。需要注意的是，如果之前没有用过 git 提交代码到 github，那么第一次安装 git 之后还需要做些 SSH Key 的设置，这里不打算赘述这个步骤。 2. 安装hexo打开 git bash，直接输入下面这条命令：1npm install -g hexo-cli 安装可能需要几分钟，耐心等待，不出意外的话，能够成功安装 hexo 3. 构建本地目录在本地文件系统中，任意找一个目录作为网站文件的存放目录，比如在 D盘 新建一个目录blog，作为网站所有文件存放的根目录。 在 git bash 中进入这个目录12cd d:cd blog/ 下载安装 hexo 博客系统所需要的各种文件1npm install 安装过程可能比较慢，且没有进度条提示。安装完之后，在blog目录下面会产生很多新的目录，简单解释一下几个常用目录和文件的含义及作用 themes：存放所有的博客主题，hexo 默认安装的主题是 landscape source: 存放所有的 markdown 源文件。所有新建的博文(post) 都存放在source/_posts/ 这个目录下面。如果后面新建了其它页面，比如关于，归档，标签等，那么在source目录下面将会新产生about, achieve, tag等目录。 public：这个目录中存放所有的网页文件，html,css, javascirpt等等。简单说，hexo 会将我们编写的 markdown 文件解析成 html 文件，同时配套生成一些 css 样式文件，进而保护 markdown 源文件不被其他人获取。 _config.yml：全局的配置文件，可以配置网站 标题, 子标题 等 4. 启动前面安装步骤顺利完成之后，就可以在本地启动 hexo 了。 1npm server # 或者是 npm s, 效果等价 这时会提示说在localhost:4000端口可以访问刚刚建立的博客，正常情况下，如果4000端口没有被占用的话，那么就可以正常访问到博客，会有一篇默认的Hello World的博文。如果4000端口被占用了，具体的表现就是网站打不开，那么可以尝试更换一个启动端口，具体命令可以自行搜索。 5. 部署到 Github Page在部署到 github page之前，需要在本地修改一些配置。 首先打开_config.yml，搜索deploy字段，修改为如下格式：123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:tanjuntao/tanjuntao.github.io.git branch: master 其中第二个repo字段的值，需要改为你在 github 上面那个网站项目对应的 SSH 值，也不打算截图介绍这个 SSH 值是怎么获取到的了。需要注意的是每个:后面需要紧跟一个空格，否则会出错！ 接着，在 git bash 中执行下面的命令：1npm install hexo-deployer-git --save 执行这条命令可能会出错，解决办法是 npm + flush刷新一下缓存, 具体的命令记不清楚。 最后就可以 deploy 到远程的 github 上面了。 1hexo deploy # 或者 hexo d, 效果是一样的 这时候，就可以看到 github 仓库里面生成了很多文件，但是找不到 source 文件夹，这就是前面介绍到的，保护了源文件。 要通过github账户名.github.io方式访问网站，还需要在项目仓库中的setting中设置中找到github page，设置一下后，才能够访问到。 6. 使用后面当需要新建一篇博文时，只需要做下面三个步骤，非常方便！1234hexo new &quot;博文标题&quot;hexo generate # hexo ghexo server # hexo s, 启动本地服务，可在本地中预览效果，这步可选hexo deploy # hexo d 7. 修改主题hexo 中默认使用的主题是landscape,但最为流行的一个主题叫next，打算安装next主题的可以接着往下看。 首先切换到博客更目录下面，也就是前面的blog/目录，执行下面命令：1git clone https://github.com/iissnan/hexo-theme-next themes/next 可能需要一段时间才能完成。 接着打开跟目录下面的_config.yml文件，修改theme字段的值为next就可以了。 如果后期打算更换其它主题，只需要修改这个theme的值就可以了，同样非常方便！ next主题默认的样式还是没有符合我的要求：左边有一个导航栏，且有关于 分类 标签 这些选项，我们还需要进一步对主题进行设置。 在/theme/next/目录中找到_config.yml文件，同样是一个设置文件，只不过仅仅是next这个主题的设置文件。 搜索Scheme，修改其值为Pisces或者Gemini，因为只有这两个 scheme 能显示左右侧边栏。 12345# Schemes# scheme: Muse# scheme: Mist# scheme: Piscesscheme: Gemini 接着搜索sidebar，参考下列注释进行设置：12345678910111213141516sidebar:# Sidebar Position - 侧栏位置（只对Pisces | Gemini两种风格有效） position: left //靠左放置 #position: right //靠右放置# Sidebar Display - 侧栏显示时机（只对Muse | Mist两种风格有效） #display: post //默认行为，在文章页面（拥有目录列表）时显示 display: always //在所有页面中都显示 #display: hide //在所有页面中都隐藏（可以手动展开） #display: remove //完全移除 offset: 12 //文章间距（只对Pisces | Gemini两种风格有效） b2t: false //返回顶部按钮（只对Pisces | Gemini两种风格有效） scrollpercent: true //返回顶部按钮的百分比 其它一些设置，包括头像，新页面，评论系统等，可以参见Hexo的Next主题详细配置 这篇文章，非常详细！ 8.其它关于使用 hexo deploy时，如何附带像git commit那样的message，只需要如下命令即可：1hexo deploy --message &quot;提交的信息内容&quot; 非常方便！ 9.注意事项 不要修改使用hexo new命令生成的文章标题 在编辑hexo new生成的 markdown 文件时，不要动手修改title的值，否则会导致这篇post无法显示在网站上！ 10. 如何配置 NexT 主题可以参考下面链接： NexT 官方提供的主题配置 hexo 的 NexT 主题详细配置 这篇文章就先到这里啦~ 主要参考 https://segmentfault.com/q/1010000007139908 https://www.jianshu.com/p/3a05351a37dc https://www.jianshu.com/p/40e4349a0cc7]]></content>
  </entry>
  <entry>
    <title><![CDATA[测试博文]]></title>
    <url>%2F2019%2F04%2F01%2F%E6%B5%8B%E8%AF%95%E5%8D%9A%E6%96%87%2F</url>
    <content type="text"><![CDATA[二级标题：测试markdown语法的支持情况 有序列表 有序列表 无序列表 无序列表 这里是引用内容 123import tensorflow as tfimport numpy as npprint('hello world') 这是行内代码，这是加粗，这是斜体]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F03%2F31%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
